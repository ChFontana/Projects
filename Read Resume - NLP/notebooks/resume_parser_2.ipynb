{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import docx2txt\n",
    "from PyPDF2 import PdfReader, PdfFileWriter, PdfFileMerger\n",
    "\n",
    "import spacy\n",
    "import en_core_web_sm\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "import pickle\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from spacy import displacy\n",
    "\n",
    "\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\cfont\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<spacy.lang.en.English at 0x1a23c5a16d0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "spacy.load('en_core_web_sm')\n",
    "spacy.load('en_core_web_lg')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extracting text from DOCX\n",
    "def doctotext(m):\n",
    "    temp = docx2txt.process(m)\n",
    "    resume_text = [line.replace('\\t', ' ') for line in temp.split('\\n') if line]\n",
    "    text = ' '.join(resume_text)\n",
    "    return (text)\n",
    "    \n",
    "#Extracting text from PDF\n",
    "def pdftotext(m):\n",
    "    # pdf file object\n",
    "    # you can find find the pdf file with complete code in below\n",
    "    pdfFileObj = open(m, 'rb')\n",
    "\n",
    "    # pdf reader object\n",
    "    pdfFileReader = PdfReader(pdfFileObj)\n",
    "\n",
    "    # number of pages in pdf\n",
    "    num_pages = len(pdfFileReader.pages)\n",
    "\n",
    "    currentPageNumber = 0\n",
    "    text = ''\n",
    "\n",
    "    # Loop in all the pdf pages.\n",
    "    while(currentPageNumber < num_pages ):\n",
    "\n",
    "        # Get the specified pdf page object.\n",
    "        pdfPage = pdfFileReader.pages[currentPageNumber]\n",
    "\n",
    "        # Get pdf page text.\n",
    "        text = text + pdfPage.extract_text()\n",
    "\n",
    "        # Process next page.\n",
    "        currentPageNumber += 1\n",
    "\n",
    "    return (text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = pdftotext(\"C:/Users/cfont/Downloads/Cristian Fontana - CV-Resume.pdf\")\n",
    "\n",
    "# load pre-trained model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "# nlp_big = spacy.load('en_core_web_lg')\n",
    "\n",
    "# initialize matcher with a vocab\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "# displacy.render(nlp(text),style=\"ent\",jupyter=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The time of execution of above program is : 7.17\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "from subprocess import list2cmdline\n",
    "from pdfminer.high_level import extract_text\n",
    "import docx2txt\n",
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "import time\n",
    "start = time.time()\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "model_checkpoint = \"xlm-roberta-large-finetuned-conll03-english\"\n",
    "token_classifier = pipeline(\n",
    "    \"token-classification\", model=model_checkpoint, aggregation_strategy=\"simple\"\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "def text_extraction(file):\n",
    "    \"\"\"\"\n",
    "    To extract texts from both pdf and word\n",
    "    \"\"\"\n",
    "    if file.endswith(\".pdf\"):\n",
    "        return extract_text(file)\n",
    "    else:\n",
    "        resume_text = docx2txt.process(file)\n",
    "    if resume_text:\n",
    "        return resume_text.replace('\\t', ' ')\n",
    "    return None\n",
    "\n",
    "\n",
    "\n",
    "# Organisation names extraction\n",
    "def org_name(file):\n",
    "    # Extract the complete text in the resume\n",
    "    extracted_text = text_extraction(file)\n",
    "    classifier = token_classifier(extracted_text)\n",
    "    # Get the list of dictionary with key value pair \"entity\":'ORG'\n",
    "    values = [item for item in classifier if item[\"entity_group\"] == \"PER\"]\n",
    "    # Get the list of dictionary with key value pair \"entity\":'ORG'\n",
    "    res = [sub['word'] for sub in values]\n",
    "    final1 = list(set(res))  # Remove duplicates\n",
    "    final = list(filter(None, final1)) # Remove empty strings\n",
    "    print('Name:', final)\n",
    "\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(\"The time of execution of above program is :\", round((end - start), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Cristian Fontana\\n\\nD a t a   S c i e n t i s t   -   I t a l i a n   C i t i z e n\\nEconomist with a strong background in Data Science and Business Intelligence with more than 5 years of\\nexperience,  building  quantitative  solutions,  doing  forecasts,  regression,  time  series,  machine  learning,  and\\ncreating dashboards. \\nAdvanced  understanding  of  statistical,  algebraic  and  other  analytical  techniques.  Highly  organized,\\nmotivated and diligent. I enjoy finding problems and working on projects to resolve them holistically.\\n\\nExperience\\n\\nDATA SCIENTIST\\nLEANTK (CONTRACT)\\n0 1 / 2 0 2 3   -   0 6 / 2 0 2 3   ( 6   M O N T H S )\\n\\nSaved more than 20.000 USD annually to small startups building a financial app with AI that take inputs\\nfrom multiple models and make decisions in real time. \\nDevelop  input  and  assumptions  based  on  preexisting  models  to  estimate  the  costs  and  savings\\nopportunities associated with varying levels of network growth and operations.\\nToolbox: AWS - GCP, Python, Git, Numpy, Pandas, Sickit Learn, Matplotlib, Pipenv, Statsmodels, Plotly,\\nSQL,  No-SQL,  Boto3,  Jupyternotebook,  Dash,  Lambda,  S3,  DynamoDB,  Serverless,  Deployment,\\nDocker, Notion, Figma, Slack, Github, Spark, RedShift, Athena, Azure, Scipy, PostgreSQL.\\n\\nDATA SCIENTIST\\nMUNDI (CONTRACT)\\n06/2022 - 12/2022 (6 MONTHS)\\n\\nReduced the churn rate by 15% in 3 months by finding the importance of the clients' two first days of\\noperating.\\nIn  charge  of  the  dashboards  and  analytic  presentations  about  the  status  of  the  company,  clients  and\\ncommercial status.\\nToolbox:  Mode,  Snowflake,  Python,  Git,  Excel,  Jira,  Github,  SQLAlchemy,  Jupyternotebook,  Numpy,\\nPandas, Scikit Learn, XGBoost, Scipy, Maplotlib, Pipenv, Seaborn, PostgreSQL, dbt, Carta, Slack, Looker,\\nPower BI, Docker, Mode, Agile methodologies, Scipy, PostgreSQL.\\n\\nDATA SCIENTIST/ANALYTICS\\nFREELANCE\\n11/2021 - 05/2022 (6 MONTHS)\\n\\nSaved  20  to  40  hours  monthly  to  small  businesses  with  the  deployment  of  different  dashboards  to\\nfollow up KPIs. \\nFinding valuable insights, and trends from the data.\\nDeployed a recommendation system that recommends other menu items based on past order history,\\nincreasing average spend by 10%.\\nToolbox:  Python,  SQL,  Excel,  Macros  VBA,  Tableau,  Power  BI,  Pandas,  Numpy,  Keras,  Statsmodel,\\nTensorflow, Looker, Sagemarker, AutoML, Azure, dbt, Slack, PostgreSQL.\\n\\nDATA SCIENTIST\\nPWC\\n01/2021 - 11/2021 (10 MONTHS)\\n\\nDevelopment  of  analytics  and  machine  learning-based  solutions  for  distinct  initiatives.  Handling,\\nforecasting and visualizing data for macroeconomic, microeconomic and other financial projects. \\nRecognize internal and external stakeholder requirements. \\nToolbox: Python, SQL, Excel, Macros VBA, Tableau, Power BI, Keras, Statsmodel, Tensorflow, GitHub,\\ndbt, Trello, Slack, Powerpoint,  PyTorch, LightGBM, XGBoost, Scipy, PostgreSQL.\\n\\nDATA ANALYTICS\\nFOBEN \\n09/2017 - 12/2020 (4 YEARS)\\n\\nReduced cost by 20% by implementing a forecast of the needed inventory and avoiding unnecessary\\ncosts.\\nBuilt  sales  forecast  using  parameters,  trend  lines  and  reference  lines,  saving  more  than  30  monthly\\nhours of manual work.\\nImplemented a long-term pricing system that improve the customer lifetime value by 20%.\\nBuild out the data and reporting infrastructure from the ground up using Tableau and SQL to provide\\nreal-time insights into the product, marketing funnels and business KPIs.\\nDeveloped and maintained reports with more than 95% on-time delivery.\\nToolbox: Excel, PowerBI, Tableau, SQL, Python, R, Powerpoint, Pandas, Numpy, PostgreSQL, MySQL.\\n\\nC O N T A C T\\n\\n+393444465965\\n\\nc.fontana95@gmail.com\\n\\nhttps://www.linkedin.com/in/cristianfontana/\\n\\nE D U C A T I O N\\n\\n2014-2020\\n\\nBachelor's degree in Economics.\\n\\nUniversidad de Buenos Aires, Argentina.\\n\\nL A N G U A G E S\\n\\nSpanish (Native)\\n\\nEnglish (Advanced).\\n\\nPortuguese (Intermediate).\\n\\nS K I L L S\\nPYTHON\\nNumpy,  Pandas,  Scikit  Learn,  PyTorch,  XGBoost,\\n\\nLightGBM,  SciPy,  Matplotlib,  Keras,  Pipenv,\\n\\nTensorflow, \\n\\nStatsmodels, \\n\\nPlotly, \\n\\nSeaborn,\\n\\nSQLAlchemy, \\n\\nBoto3, \\n\\nBokeh, \\n\\nDash,\\n\\nJupyternotebook, PySpark.\\n\\nCLOUD & DATABASES\\nAWS,  GCP,  Azure,  SQL,  No-SQL,  Lambda,\\n\\nSagemaker,  S3,  Athena,  MongoDB,  RedShift,\\n\\nDynamoDB,  Containers, \\n\\nStorage, \\n\\nServerless,\\n\\nDeployment, PostgreSQL, SQL, SQL Server, MySQL,\\n\\nDocker, Git, Github, Bash, AutoML.\\n\\nVISUALIZATION\\nPower BI, Tableau, Looker, Mode, Dash, Plotly.\\n\\nOTHERS\\nAgile methodologies, CRISP, Notion, Jira, Trello,\\n\\nExcel, dbt, R, Powerpoint, VBA, Macros, Figma,\\n\\nCarta, Slack, Clickup, Pendo, and Productboard.\\n\\nC E R T I F I C A T E S\\n\\nData Analyst - DataCamp.\\nData Scientist - DataCamp.\\n\\nGoogle Data Analytics Specialization - Coursera.\\nPower BI - Crehana.\\n\\nIBM Data Science - Specialization - Coursera.\\n\\nMacroeconomic Diagnostics - FMI - Edx.\\nFinancial Market Analysis - FMI - Edx.\\nEssentials  of  Corporate  Finance  -  Specialization\\n- University of Melbourne.\\n\\n\\x0c\""
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_extraction(\"C:/Users/cfont/Downloads/Cristian Fontana - CV-Resume.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)lve/main/config.json: 100%|██████████| 650/650 [00:00<00:00, 649kB/s]\n",
      "Downloading pytorch_model.bin: 100%|██████████| 440M/440M [01:59<00:00, 3.69MB/s] \n",
      "Some weights of the model checkpoint at dccuchile/bert-base-spanish-wwm-uncased were not used when initializing BertForTokenClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Downloading (…)okenizer_config.json: 100%|██████████| 310/310 [00:00<00:00, 312kB/s]\n",
      "Downloading (…)solve/main/vocab.txt: 100%|██████████| 248k/248k [00:00<00:00, 980kB/s]\n",
      "Downloading (…)/main/tokenizer.json: 100%|██████████| 486k/486k [00:00<00:00, 3.72MB/s]\n",
      "Downloading (…)cial_tokens_map.json: 100%|██████████| 134/134 [00:00<00:00, 88.9kB/s]\n"
     ]
    }
   ],
   "source": [
    "ner_pipeline = pipeline(\"ner\", model=\"dccuchile/bert-base-spanish-wwm-uncased\", tokenizer=\"dccuchile/bert-base-spanish-wwm-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'entity': 'B-PER', 'score': 0.9973539, 'index': 1, 'word': 'C', 'start': 0, 'end': 1}, {'entity': 'B-PER', 'score': 0.61249125, 'index': 2, 'word': '##rist', 'start': 1, 'end': 5}, {'entity': 'I-PER', 'score': 0.7767754, 'index': 3, 'word': '##ian', 'start': 5, 'end': 8}, {'entity': 'I-PER', 'score': 0.9985752, 'index': 4, 'word': 'F', 'start': 9, 'end': 10}, {'entity': 'I-PER', 'score': 0.99541783, 'index': 5, 'word': '##ont', 'start': 10, 'end': 13}, {'entity': 'I-PER', 'score': 0.9936428, 'index': 6, 'word': '##ana', 'start': 13, 'end': 16}, {'entity': 'B-MISC', 'score': 0.6400754, 'index': 44, 'word': 'Data', 'start': 120, 'end': 124}, {'entity': 'I-MISC', 'score': 0.72663593, 'index': 48, 'word': 'Intelligence', 'start': 146, 'end': 158}, {'entity': 'B-MISC', 'score': 0.7665526, 'index': 223, 'word': 'A', 'start': 995, 'end': 996}, {'entity': 'B-MISC', 'score': 0.6003907, 'index': 224, 'word': '##WS', 'start': 996, 'end': 998}, {'entity': 'B-MISC', 'score': 0.8929192, 'index': 226, 'word': 'G', 'start': 1001, 'end': 1002}, {'entity': 'I-MISC', 'score': 0.8367763, 'index': 227, 'word': '##CP', 'start': 1002, 'end': 1004}, {'entity': 'B-MISC', 'score': 0.95448285, 'index': 229, 'word': 'Python', 'start': 1006, 'end': 1012}, {'entity': 'B-MISC', 'score': 0.87074137, 'index': 231, 'word': 'G', 'start': 1014, 'end': 1015}, {'entity': 'I-MISC', 'score': 0.9069965, 'index': 232, 'word': '##it', 'start': 1015, 'end': 1017}, {'entity': 'B-MISC', 'score': 0.90110844, 'index': 234, 'word': 'N', 'start': 1019, 'end': 1020}, {'entity': 'I-MISC', 'score': 0.90143305, 'index': 235, 'word': '##ump', 'start': 1020, 'end': 1023}, {'entity': 'I-MISC', 'score': 0.86130655, 'index': 236, 'word': '##y', 'start': 1023, 'end': 1024}, {'entity': 'B-MISC', 'score': 0.87174124, 'index': 238, 'word': 'Pan', 'start': 1026, 'end': 1029}, {'entity': 'I-MISC', 'score': 0.94818896, 'index': 239, 'word': '##das', 'start': 1029, 'end': 1032}, {'entity': 'B-MISC', 'score': 0.6232384, 'index': 241, 'word': 'Sick', 'start': 1034, 'end': 1038}, {'entity': 'I-MISC', 'score': 0.6936084, 'index': 242, 'word': '##it', 'start': 1038, 'end': 1040}, {'entity': 'I-MISC', 'score': 0.7220197, 'index': 243, 'word': 'Lea', 'start': 1041, 'end': 1044}, {'entity': 'I-MISC', 'score': 0.7826858, 'index': 244, 'word': '##rn', 'start': 1044, 'end': 1046}, {'entity': 'B-MISC', 'score': 0.87277985, 'index': 246, 'word': 'Mat', 'start': 1048, 'end': 1051}, {'entity': 'I-MISC', 'score': 0.724152, 'index': 247, 'word': '##p', 'start': 1051, 'end': 1052}, {'entity': 'I-MISC', 'score': 0.57228535, 'index': 248, 'word': '##lot', 'start': 1052, 'end': 1055}, {'entity': 'I-MISC', 'score': 0.6279386, 'index': 250, 'word': '##b', 'start': 1057, 'end': 1058}, {'entity': 'B-MISC', 'score': 0.8302841, 'index': 252, 'word': 'Pi', 'start': 1060, 'end': 1062}, {'entity': 'I-MISC', 'score': 0.87224364, 'index': 253, 'word': '##pen', 'start': 1062, 'end': 1065}, {'entity': 'I-MISC', 'score': 0.76484853, 'index': 254, 'word': '##v', 'start': 1065, 'end': 1066}, {'entity': 'B-MISC', 'score': 0.7932105, 'index': 256, 'word': 'St', 'start': 1068, 'end': 1070}, {'entity': 'I-MISC', 'score': 0.8277077, 'index': 259, 'word': '##del', 'start': 1075, 'end': 1078}, {'entity': 'I-MISC', 'score': 0.6400315, 'index': 260, 'word': '##s', 'start': 1078, 'end': 1079}, {'entity': 'B-MISC', 'score': 0.70351315, 'index': 262, 'word': 'P', 'start': 1081, 'end': 1082}, {'entity': 'I-MISC', 'score': 0.7303198, 'index': 264, 'word': '##ly', 'start': 1085, 'end': 1087}, {'entity': 'B-MISC', 'score': 0.9483574, 'index': 266, 'word': 'S', 'start': 1089, 'end': 1090}, {'entity': 'I-MISC', 'score': 0.7728949, 'index': 267, 'word': '##QL', 'start': 1090, 'end': 1092}, {'entity': 'B-MISC', 'score': 0.96207094, 'index': 269, 'word': 'No', 'start': 1095, 'end': 1097}, {'entity': 'I-MISC', 'score': 0.8352377, 'index': 270, 'word': '-', 'start': 1097, 'end': 1098}, {'entity': 'I-MISC', 'score': 0.96069664, 'index': 271, 'word': 'S', 'start': 1098, 'end': 1099}, {'entity': 'I-MISC', 'score': 0.96394664, 'index': 272, 'word': '##QL', 'start': 1099, 'end': 1101}, {'entity': 'B-MISC', 'score': 0.92864054, 'index': 274, 'word': 'Bo', 'start': 1104, 'end': 1106}, {'entity': 'I-MISC', 'score': 0.6044729, 'index': 275, 'word': '##to', 'start': 1106, 'end': 1108}, {'entity': 'I-MISC', 'score': 0.94793105, 'index': 276, 'word': '##3', 'start': 1108, 'end': 1109}, {'entity': 'B-MISC', 'score': 0.8684431, 'index': 278, 'word': 'Ju', 'start': 1112, 'end': 1114}, {'entity': 'I-MISC', 'score': 0.78859407, 'index': 279, 'word': '##py', 'start': 1114, 'end': 1116}, {'entity': 'I-MISC', 'score': 0.56973124, 'index': 281, 'word': '##ote', 'start': 1120, 'end': 1123}, {'entity': 'B-MISC', 'score': 0.9443978, 'index': 284, 'word': 'Dash', 'start': 1130, 'end': 1134}, {'entity': 'B-MISC', 'score': 0.87848246, 'index': 286, 'word': 'Lamb', 'start': 1137, 'end': 1141}, {'entity': 'I-MISC', 'score': 0.9603932, 'index': 287, 'word': '##da', 'start': 1141, 'end': 1143}, {'entity': 'B-MISC', 'score': 0.88808984, 'index': 289, 'word': 'S', 'start': 1146, 'end': 1147}, {'entity': 'I-MISC', 'score': 0.91564626, 'index': 290, 'word': '##3', 'start': 1147, 'end': 1148}, {'entity': 'B-MISC', 'score': 0.84132135, 'index': 292, 'word': 'Dynamo', 'start': 1151, 'end': 1157}, {'entity': 'I-MISC', 'score': 0.784019, 'index': 293, 'word': '##D', 'start': 1157, 'end': 1158}, {'entity': 'I-MISC', 'score': 0.8442521, 'index': 294, 'word': '##B', 'start': 1158, 'end': 1159}, {'entity': 'B-MISC', 'score': 0.8629924, 'index': 296, 'word': 'Server', 'start': 1162, 'end': 1168}, {'entity': 'I-MISC', 'score': 0.89945614, 'index': 297, 'word': '##less', 'start': 1168, 'end': 1172}, {'entity': 'B-MISC', 'score': 0.8228229, 'index': 299, 'word': 'De', 'start': 1175, 'end': 1177}, {'entity': 'B-MISC', 'score': 0.8016846, 'index': 304, 'word': 'Dock', 'start': 1187, 'end': 1191}, {'entity': 'I-MISC', 'score': 0.9471208, 'index': 305, 'word': '##er', 'start': 1191, 'end': 1193}, {'entity': 'B-MISC', 'score': 0.81268466, 'index': 307, 'word': 'Not', 'start': 1195, 'end': 1198}, {'entity': 'B-MISC', 'score': 0.89676565, 'index': 310, 'word': 'Fi', 'start': 1203, 'end': 1205}, {'entity': 'I-MISC', 'score': 0.5872541, 'index': 311, 'word': '##gma', 'start': 1205, 'end': 1208}, {'entity': 'B-MISC', 'score': 0.8993991, 'index': 313, 'word': 'S', 'start': 1210, 'end': 1211}, {'entity': 'I-MISC', 'score': 0.869886, 'index': 314, 'word': '##la', 'start': 1211, 'end': 1213}, {'entity': 'I-MISC', 'score': 0.8976517, 'index': 315, 'word': '##ck', 'start': 1213, 'end': 1215}, {'entity': 'B-MISC', 'score': 0.94409275, 'index': 317, 'word': 'G', 'start': 1217, 'end': 1218}, {'entity': 'I-MISC', 'score': 0.79098195, 'index': 318, 'word': '##ith', 'start': 1218, 'end': 1221}, {'entity': 'I-MISC', 'score': 0.9226176, 'index': 319, 'word': '##ub', 'start': 1221, 'end': 1223}, {'entity': 'B-MISC', 'score': 0.9212618, 'index': 321, 'word': 'Spa', 'start': 1225, 'end': 1228}, {'entity': 'I-MISC', 'score': 0.7512233, 'index': 322, 'word': '##rk', 'start': 1228, 'end': 1230}, {'entity': 'B-MISC', 'score': 0.9588814, 'index': 324, 'word': 'Red', 'start': 1232, 'end': 1235}, {'entity': 'I-MISC', 'score': 0.90120816, 'index': 325, 'word': '##S', 'start': 1235, 'end': 1236}, {'entity': 'I-MISC', 'score': 0.83109754, 'index': 326, 'word': '##hi', 'start': 1236, 'end': 1238}, {'entity': 'I-MISC', 'score': 0.9276653, 'index': 327, 'word': '##ft', 'start': 1238, 'end': 1240}, {'entity': 'B-MISC', 'score': 0.9041004, 'index': 329, 'word': 'Athena', 'start': 1242, 'end': 1248}, {'entity': 'B-MISC', 'score': 0.9325871, 'index': 331, 'word': 'A', 'start': 1250, 'end': 1251}, {'entity': 'I-MISC', 'score': 0.85616386, 'index': 332, 'word': '##zure', 'start': 1251, 'end': 1255}, {'entity': 'B-MISC', 'score': 0.83755606, 'index': 334, 'word': 'Sc', 'start': 1257, 'end': 1259}, {'entity': 'I-MISC', 'score': 0.72945625, 'index': 335, 'word': '##ip', 'start': 1259, 'end': 1261}, {'entity': 'I-MISC', 'score': 0.83048034, 'index': 336, 'word': '##y', 'start': 1261, 'end': 1262}, {'entity': 'B-MISC', 'score': 0.9889818, 'index': 338, 'word': 'Post', 'start': 1264, 'end': 1268}, {'entity': 'I-MISC', 'score': 0.9509301, 'index': 339, 'word': '##g', 'start': 1268, 'end': 1269}, {'entity': 'I-MISC', 'score': 0.504613, 'index': 340, 'word': '##re', 'start': 1269, 'end': 1271}, {'entity': 'I-MISC', 'score': 0.6954333, 'index': 341, 'word': '##S', 'start': 1271, 'end': 1272}, {'entity': 'I-MISC', 'score': 0.93827736, 'index': 342, 'word': '##QL', 'start': 1272, 'end': 1274}, {'entity': 'B-MISC', 'score': 0.6953231, 'index': 426, 'word': 'Too', 'start': 1586, 'end': 1589}, {'entity': 'I-MISC', 'score': 0.6308577, 'index': 427, 'word': '##l', 'start': 1589, 'end': 1590}, {'entity': 'I-MISC', 'score': 0.755493, 'index': 428, 'word': '##box', 'start': 1590, 'end': 1593}, {'entity': 'B-MISC', 'score': 0.45483118, 'index': 430, 'word': 'Mode', 'start': 1596, 'end': 1600}, {'entity': 'B-MISC', 'score': 0.8282488, 'index': 432, 'word': 'Snow', 'start': 1603, 'end': 1607}, {'entity': 'I-MISC', 'score': 0.52949107, 'index': 433, 'word': '##f', 'start': 1607, 'end': 1608}, {'entity': 'I-MISC', 'score': 0.80236506, 'index': 434, 'word': '##lake', 'start': 1608, 'end': 1612}, {'entity': 'B-MISC', 'score': 0.9757627, 'index': 436, 'word': 'Python', 'start': 1615, 'end': 1621}, {'entity': 'B-MISC', 'score': 0.80914277, 'index': 438, 'word': 'G', 'start': 1624, 'end': 1625}, {'entity': 'I-MISC', 'score': 0.84764564, 'index': 439, 'word': '##it', 'start': 1625, 'end': 1627}, {'entity': 'B-MISC', 'score': 0.7989794, 'index': 441, 'word': 'Ex', 'start': 1630, 'end': 1632}, {'entity': 'I-MISC', 'score': 0.9518495, 'index': 442, 'word': '##cel', 'start': 1632, 'end': 1635}, {'entity': 'B-MISC', 'score': 0.70369786, 'index': 444, 'word': 'Ji', 'start': 1638, 'end': 1640}, {'entity': 'I-MISC', 'score': 0.7523932, 'index': 445, 'word': '##ra', 'start': 1640, 'end': 1642}, {'entity': 'B-MISC', 'score': 0.7561057, 'index': 447, 'word': 'G', 'start': 1645, 'end': 1646}, {'entity': 'I-MISC', 'score': 0.7193479, 'index': 449, 'word': '##ub', 'start': 1649, 'end': 1651}, {'entity': 'B-MISC', 'score': 0.8931582, 'index': 451, 'word': 'S', 'start': 1654, 'end': 1655}, {'entity': 'I-MISC', 'score': 0.8487313, 'index': 452, 'word': '##QL', 'start': 1655, 'end': 1657}, {'entity': 'I-MISC', 'score': 0.7483065, 'index': 454, 'word': '##l', 'start': 1658, 'end': 1659}, {'entity': 'I-MISC', 'score': 0.48789972, 'index': 455, 'word': '##che', 'start': 1659, 'end': 1662}, {'entity': 'B-MISC', 'score': 0.75202966, 'index': 458, 'word': 'Ju', 'start': 1667, 'end': 1669}, {'entity': 'I-MISC', 'score': 0.72536516, 'index': 459, 'word': '##py', 'start': 1669, 'end': 1671}, {'entity': 'I-MISC', 'score': 0.7788544, 'index': 460, 'word': '##tern', 'start': 1671, 'end': 1675}, {'entity': 'I-MISC', 'score': 0.8714082, 'index': 461, 'word': '##ote', 'start': 1675, 'end': 1678}, {'entity': 'B-MISC', 'score': 0.86834604, 'index': 464, 'word': 'N', 'start': 1685, 'end': 1686}, {'entity': 'B-MISC', 'score': 0.84672487, 'index': 468, 'word': 'Pan', 'start': 1692, 'end': 1695}, {'entity': 'I-MISC', 'score': 0.7435038, 'index': 469, 'word': '##das', 'start': 1695, 'end': 1698}, {'entity': 'B-MISC', 'score': 0.7858469, 'index': 471, 'word': 'Sc', 'start': 1700, 'end': 1702}, {'entity': 'I-MISC', 'score': 0.963419, 'index': 472, 'word': '##iki', 'start': 1702, 'end': 1705}, {'entity': 'B-MISC', 'score': 0.7488168, 'index': 474, 'word': 'Lea', 'start': 1707, 'end': 1710}, {'entity': 'B-MISC', 'score': 0.80736345, 'index': 477, 'word': 'X', 'start': 1714, 'end': 1715}, {'entity': 'I-MISC', 'score': 0.6715482, 'index': 478, 'word': '##GB', 'start': 1715, 'end': 1717}, {'entity': 'I-MISC', 'score': 0.59331894, 'index': 479, 'word': '##oos', 'start': 1717, 'end': 1720}, {'entity': 'B-MISC', 'score': 0.7694315, 'index': 482, 'word': 'Sc', 'start': 1723, 'end': 1725}, {'entity': 'I-MISC', 'score': 0.47350207, 'index': 483, 'word': '##ip', 'start': 1725, 'end': 1727}, {'entity': 'B-MISC', 'score': 0.7402566, 'index': 486, 'word': 'Map', 'start': 1730, 'end': 1733}, {'entity': 'I-MISC', 'score': 0.74707603, 'index': 487, 'word': '##lot', 'start': 1733, 'end': 1736}, {'entity': 'I-MISC', 'score': 0.81082875, 'index': 488, 'word': '##li', 'start': 1736, 'end': 1738}, {'entity': 'B-MISC', 'score': 0.77755964, 'index': 491, 'word': 'Pi', 'start': 1741, 'end': 1743}, {'entity': 'I-MISC', 'score': 0.5006492, 'index': 492, 'word': '##pen', 'start': 1743, 'end': 1746}, {'entity': 'B-MISC', 'score': 0.8374021, 'index': 495, 'word': 'Sea', 'start': 1749, 'end': 1752}, {'entity': 'B-MISC', 'score': 0.82767844, 'index': 498, 'word': 'Post', 'start': 1758, 'end': 1762}, {'entity': 'I-MISC', 'score': 0.5561043, 'index': 500, 'word': '##re', 'start': 1763, 'end': 1765}, {'entity': 'I-MISC', 'score': 0.83330667, 'index': 501, 'word': '##S', 'start': 1765, 'end': 1766}, {'entity': 'I-MISC', 'score': 0.8251161, 'index': 502, 'word': '##QL', 'start': 1766, 'end': 1768}, {'entity': 'B-MISC', 'score': 0.6853065, 'index': 507, 'word': 'Car', 'start': 1775, 'end': 1778}, {'entity': 'I-MISC', 'score': 0.49426702, 'index': 508, 'word': '##ta', 'start': 1778, 'end': 1780}, {'entity': 'B-MISC', 'score': 0.93621564, 'index': 510, 'word': 'S', 'start': 1782, 'end': 1783}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline\n",
    "\n",
    "bert_tokenizer = AutoTokenizer.from_pretrained('dslim/bert-large-NER')\n",
    "bert_model = AutoModelForTokenClassification.from_pretrained('dslim/bert-large-NER')\n",
    "\n",
    "nlp = pipeline('ner', model=bert_model, tokenizer=bert_tokenizer)\n",
    "ner_list = nlp(text_extraction(\"C:/Users/cfont/Downloads/Cristian Fontana - CV-Resume.pdf\"))\n",
    "print(ner_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[['C']], [['##rist', '##ian', 'F', '##ont', '##ana']]]\n"
     ]
    }
   ],
   "source": [
    "this_name = []\n",
    "all_names_list_tmp = []\n",
    "\n",
    "for ner_dict in ner_list:\n",
    "    if ner_dict['entity'] == 'B-PER':\n",
    "        if len(this_name) == 0:\n",
    "            this_name.append(ner_dict['word'])\n",
    "        else:\n",
    "            all_names_list_tmp.append([this_name])\n",
    "            this_name = []\n",
    "            this_name.append(ner_dict['word'])\n",
    "    elif ner_dict['entity'] == 'I-PER':\n",
    "        this_name.append(ner_dict['word'])\n",
    "\n",
    "all_names_list_tmp.append([this_name])\n",
    "\n",
    "print(all_names_list_tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_spanish_full_names(text):\n",
    "    entities = ner_pipeline(text)\n",
    "    person_names = []\n",
    "    current_name = \"\"\n",
    "\n",
    "    for entity in entities:\n",
    "        if entity[\"entity\"] == \"I-PER\":\n",
    "            current_name += entity[\"word\"] + \" \"\n",
    "        elif current_name:\n",
    "            person_names.append(current_name.strip())\n",
    "            current_name = \"\"\n",
    "\n",
    "    if current_name:\n",
    "        person_names.append(current_name.strip())\n",
    "\n",
    "    return person_names\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_spanish_full_names(text_extraction(\"C:/Users/cfont/Downloads/Cristian Fontana - CV-Resume.pdf\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_spanish_full_names(text_extraction(\"C:/Users/cfont/Downloads/Cristian Fontana - CV-Resume.pdf\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'vocab'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[64], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# Use dispacy to visualize the NER\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m displacy\u001b[39m.\u001b[39;49mrender(nlp(text_extraction(\u001b[39m\"\u001b[39;49m\u001b[39mC:/Users/cfont/Downloads/Cristian Fontana - CV-Resume.pdf\u001b[39;49m\u001b[39m\"\u001b[39;49m)),jupyter\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\spacy\\displacy\\__init__.py:59\u001b[0m, in \u001b[0;36mrender\u001b[1;34m(docs, style, page, minify, jupyter, options, manual)\u001b[0m\n\u001b[0;32m     57\u001b[0m renderer_func, converter \u001b[39m=\u001b[39m factories[style]\n\u001b[0;32m     58\u001b[0m renderer \u001b[39m=\u001b[39m renderer_func(options\u001b[39m=\u001b[39moptions)\n\u001b[1;32m---> 59\u001b[0m parsed \u001b[39m=\u001b[39m [converter(doc, options) \u001b[39mfor\u001b[39;49;00m doc \u001b[39min\u001b[39;49;00m docs] \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m manual \u001b[39melse\u001b[39;00m docs  \u001b[39m# type: ignore\u001b[39;00m\n\u001b[0;32m     60\u001b[0m \u001b[39mif\u001b[39;00m manual:\n\u001b[0;32m     61\u001b[0m     \u001b[39mfor\u001b[39;00m doc \u001b[39min\u001b[39;00m docs:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\spacy\\displacy\\__init__.py:59\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     57\u001b[0m renderer_func, converter \u001b[39m=\u001b[39m factories[style]\n\u001b[0;32m     58\u001b[0m renderer \u001b[39m=\u001b[39m renderer_func(options\u001b[39m=\u001b[39moptions)\n\u001b[1;32m---> 59\u001b[0m parsed \u001b[39m=\u001b[39m [converter(doc, options) \u001b[39mfor\u001b[39;00m doc \u001b[39min\u001b[39;00m docs] \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m manual \u001b[39melse\u001b[39;00m docs  \u001b[39m# type: ignore\u001b[39;00m\n\u001b[0;32m     60\u001b[0m \u001b[39mif\u001b[39;00m manual:\n\u001b[0;32m     61\u001b[0m     \u001b[39mfor\u001b[39;00m doc \u001b[39min\u001b[39;00m docs:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\spacy\\displacy\\__init__.py:139\u001b[0m, in \u001b[0;36mparse_deps\u001b[1;34m(orig_doc, options)\u001b[0m\n\u001b[0;32m    137\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(orig_doc, Span):\n\u001b[0;32m    138\u001b[0m     orig_doc \u001b[39m=\u001b[39m orig_doc\u001b[39m.\u001b[39mas_doc()\n\u001b[1;32m--> 139\u001b[0m doc \u001b[39m=\u001b[39m Doc(orig_doc\u001b[39m.\u001b[39;49mvocab)\u001b[39m.\u001b[39mfrom_bytes(\n\u001b[0;32m    140\u001b[0m     orig_doc\u001b[39m.\u001b[39mto_bytes(exclude\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39muser_data\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39muser_hooks\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[0;32m    141\u001b[0m )\n\u001b[0;32m    142\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m doc\u001b[39m.\u001b[39mhas_annotation(\u001b[39m\"\u001b[39m\u001b[39mDEP\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m    143\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(Warnings\u001b[39m.\u001b[39mW005)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'dict' object has no attribute 'vocab'"
     ]
    }
   ],
   "source": [
    "# Use dispacy to visualize the NER\n",
    "displacy.render(nlp(text_extraction(\"C:/Users/cfont/Downloads/Cristian Fontana - CV-Resume.pdf\")),jupyter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForTokenClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "ner_pipeline = pipeline(\"ner\", model=\"bert-base-uncased\", tokenizer=\"bert-base-uncased\")\n",
    "\n",
    "def find_person_names(text):\n",
    "    entities = ner_pipeline(text)\n",
    "    person_names = [entity[\"word\"] for entity in entities if entity[\"entity\"] == \"PER\"]\n",
    "    return person_names\n",
    "\n",
    "find_person_names(re.sub(r'\\.(?!\\))', r'. ', text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def find_full_names(text):\n",
    "    entities = ner_pipeline(text)\n",
    "    person_names = []\n",
    "    current_name = \"\"\n",
    "\n",
    "    for entity in entities:\n",
    "        if entity[\"entity\"] == \"PER\":\n",
    "            current_name += entity[\"word\"] + \" \"\n",
    "        elif current_name:\n",
    "            person_names.append(current_name.strip())\n",
    "            current_name = \"\"\n",
    "\n",
    "    if current_name:\n",
    "        person_names.append(current_name.strip())\n",
    "\n",
    "    return person_names\n",
    "\n",
    "find_full_names(re.sub(r'\\.(?!\\))', r'. ', text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Saved more than 20 000 USD annually to small startups building a financial app with AI that take inputs from multiple models and make decisions in real time Develop input and assumptions based on preexisting models to estimate the costs and savings opportunities associated with varying levels of network growth and operations Toolbox AWS GCP Python Git Numpy Pandas Sickit Learn Matplotlib Pipenv Statsmodels Plotly SQL No SQL Boto3 Jupyternotebook Dash Lambda S3 DynamoDB Serverless Deployment Docker Notion Figma Slack Github Spark RedShift Athena Azure Scipy PostgreSQL Reduced the churn rate by 15 in 3 months by finding the importance of the clients two first days of operating In charge of the dashboards and analytic presentations about the status of the company clients and commercial status Toolbox Mode Snowflake Python Git Excel Jira Github SQLAlchemy Jupyternotebook Numpy Pandas Scikit Learn XGBoost Scipy Maplotlib Pipenv Seaborn PostgreSQL dbt Carta Slack Looker Power BI Docker Mode Agile methodologies Scipy PostgreSQL Saved 20 to 40 hours monthly to small businesses with the deployment of different dashboards to follow up KPIs Finding valuable insights and trends from the data Deployed a recommendation system that recommends other menu items based on past order history increasing average spend by 10 Toolbox Python SQL Excel Macros VBA Tableau Power BI Pandas Numpy Keras Statsmodel Tensorflow Looker Sagemarker AutoML Azure dbt Slack PostgreSQL Development of analytics and machine learning based solutions for distinct initiatives Handling forecasting and visualizing data for macroeconomic microeconomic and other financial projects Recognize internal and external stakeholder requirements Toolbox Python SQL Excel Macros VBA Tableau Power BI Keras Statsmodel Tensorflow GitHub dbt Trello Slack Powerpoint PyTorch LightGBM XGBoost Scipy PostgreSQL Reduced cost by 20 by implementing a forecast of the needed inventory and avoiding unnecessary costs Built sales forecast using parameters trend lines and reference lines saving more than 30 monthly hours of manual work Implemented a long term pricing system that improve the customer lifetime value by 20 Build out the data and reporting infrastructure from the ground up using Tableau and SQL to provide real time insights into the product marketing funnels and business KPIs Developed and maintained reports with more than 95 on time delivery Toolbox Excel PowerBI Tableau SQL Python R Powerpoint Pandas Numpy PostgreSQL MySQL DATA SCIENTIST LEANTK CONTRACT 01 2023 06 2023 6 MONTHS DATA SCIENTIST MUNDI CONTRACT 06 2022 12 2022 6 MONTHS DATA SCIENTIST ANALYTICS FREELANCE 11 2021 05 2022 6 MONTHS DATA SCIENTIST PWC 01 2021 11 2021 10 MONTHS DATA ANALYTICS FOBEN 09 2017 12 2020 4 YEARS Data Analyst DataCamp Data Scientist DataCamp Google Data Analytics Specialization Coursera Power BI Crehana IBM Data Science Specialization Coursera Macroeconomic Diagnostics FMI Edx Financial Market Analysis FMI Edx Essentials of Corporate Finance Specialization University of Melbourne CE IFICATESSKILLS PYTHON Numpy Pandas Scikit Learn PyTorch XGBoost LightGBM SciPy Matplotlib Keras Pipenv Tensorflow Statsmodels Plotly Seaborn SQLAlchemy Boto3 Bokeh Dash Jupyternotebook PySpark CLOUD DATABASES AWS GCP Azure SQL No SQL Lambda Sagemaker S3 Athena MongoDB RedShift DynamoDB Containers Storage Serverless Deployment PostgreSQL SQL SQL Server MySQL Docker Git Github Bash AutoML VISUALIZATION Power BI Tableau Looker Mode Dash Plotly OTHERS Agile methodologies CRISP Notion Jira Trello Excel dbt R Powerpoint VBA Macros Figma Carta Slack Clickup Pendo and Productboard LANGUAGES Spanish Native English Advanced Portuguese Intermediate Cristian Fontana Data Scientist Italian Citizen Economist with a strong background in Data Science and Business Intelligence with more than 5 years of experience building quantitative solutions doing forecasts regression time series machine learning and creating dashboards Advanced understanding of statistical algebraic and other analytical techniques Highly organized motivated and diligent I enjoy finding problems and working on projects to resolve them holistically Experience EDUCATION 2014 2020 Bachelor s degree in Economics Universidad de Buenos Aires Argentina CONTACT 393444465965 c fontana95 '"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_function(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name:  Sickit Learn\n"
     ]
    }
   ],
   "source": [
    "def extract_name(resume_text):\n",
    "    nlp_text = nlp(resume_text)\n",
    "    \n",
    "    # First name and Last name are always Proper Nouns\n",
    "    pattern = [{'POS': 'PROPN'}, {'POS': 'PROPN'}]\n",
    "    \n",
    "    matcher.add('NAME', [pattern])\n",
    "    \n",
    "    matches = matcher(nlp_text)\n",
    "    \n",
    "    for match_id, start, end in matches:\n",
    "        span = nlp_text[start:end]\n",
    "        return span.text\n",
    "print('Name: ',extract_name(pdftotext(\"C:/Users/cfont/Downloads/Cristian Fontana - CV-Resume.pdf\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict Profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pickle models.\n",
    "with open('C:/Users/cfont/OneDrive/Documents/GitHub/Projects/Read Resume - NLP/src/models/model.pkl', 'rb') as f:\n",
    "    predict_profile = pickle.load(f)\n",
    "\n",
    "with open('C:/Users/cfont/OneDrive/Documents/GitHub/Projects/Read Resume - NLP/src/models/vectorizer.pkl', 'rb') as f:\n",
    "    vectorizer = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the resume text.\n",
    "def clean_function(resumeText):\n",
    "    resumeText = re.sub('http\\S+\\s*', ' ', resumeText)  # remove URLs\n",
    "    resumeText = re.sub('RT|cc', ' ', resumeText)  # remove RT and cc\n",
    "    resumeText = re.sub('#\\S+', '', resumeText)  # remove hashtags\n",
    "    resumeText = re.sub('@\\S+', '  ', resumeText)  # remove mentions\n",
    "    resumeText = re.sub('[%s]' % re.escape(\"\"\"!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\"\"\"), ' ', resumeText)  # remove punctuations\n",
    "    resumeText = re.sub(r'[^\\x00-\\x7f]',r' ', resumeText) \n",
    "    resumeText = re.sub('\\s+', ' ', resumeText)  # remove extra whitespace\n",
    "    return resumeText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleand and vectorize the resume text.\n",
    "def new_inputs(resumes):\n",
    "    cleaned_resumes = resumes.apply(lambda x:clean_function(x))\n",
    "    transformed_resumes = vectorizer.transform(cleaned_resumes)\n",
    "    return transformed_resumes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Profile: Data Science\n"
     ]
    }
   ],
   "source": [
    "# Predict the profile.\n",
    "print('Predicted Profile:', predict_profile.predict(new_inputs(pd.Series(text)))[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Qualification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full sentence:  {\"Experience\\nEDUCATION\\n2014-2020\\nBachelor's degree in Economics.\"} \n",
      "Degree:  Bachelors \n",
      "Subject:  economic \n",
      "GPA:  [] \n",
      "Years:  ['2014', '2020']\n"
     ]
    }
   ],
   "source": [
    "# Grad all general stop words\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "\n",
    "# Education Degrees\n",
    "EDUCATION = [\n",
    "            'BE','B.E.', 'B.E', 'BS', 'B.S', \n",
    "            'ME', 'M.E', 'M.E.', 'M.B.A', 'MBA', 'MS', 'M.S', \n",
    "            'BTECH', 'B.TECH', 'M.TECH', 'MTECH', \n",
    "            'SSLC', 'SSC' 'HSC', 'CBSE', 'ICSE', 'X', 'XII',\n",
    "            'BACHELOR', 'MASTER', 'PHD', 'BACHELORS', 'MASTERS', 'Ph.D.',\n",
    "            'Licenciatura', 'Ingeniería', 'Maestría', 'Maestria',\n",
    "            'Maestra', 'Maestro', 'Doctorado', 'Doctora', 'Doctor', 'Licenciado', 'Licenciada',\n",
    "            'Ingeniero', 'Ingeniera', 'Maestrante', 'Doctorante', 'L'\n",
    "            'Lic', 'Ing'\n",
    "        ]\n",
    "\n",
    "# Make every word in EDUCATION lowercase.\n",
    "EDUCATION = [x.lower() for x in EDUCATION]\n",
    "\n",
    "bachelor_subjects = [\n",
    "    \"Computer Science\",\n",
    "    \"Physics\",\n",
    "    \"Chemistry\",\n",
    "    \"Biology\",\n",
    "    \"Mathematics\",\n",
    "    \"Engineering\",\n",
    "    \"Psychology\",\n",
    "    \"English Literature\",\n",
    "    \"History\",\n",
    "    \"Sociology\",\n",
    "    \"Economics\",\n",
    "    \"Political Science\",\n",
    "    \"Business Administration\",\n",
    "    \"Marketing\",\n",
    "    \"Accounting\",\n",
    "    \"Finance\",\n",
    "    \"Nursing\",\n",
    "    \"Environmental Science\",\n",
    "    \"Art\",\n",
    "    \"Music\",\n",
    "    \"Film Studies\",\n",
    "    \"Philosophy\",\n",
    "    \"Anthropology\",\n",
    "    \"Communications\",\n",
    "    \"Languages\",\n",
    "    \"Geography\",\n",
    "    \"Architecture\",\n",
    "    \"Urban Planning\",\n",
    "    \"Graphic Design\",\n",
    "    \"Journalism\",\n",
    "    \"Criminal Justice\",\n",
    "    \"Law\",\n",
    "    \"International Relations\",\n",
    "    \"Sports Science\",\n",
    "    \"Theater\",\n",
    "    \"Dance\",\n",
    "    \"Religious Studies\",\n",
    "    \"Information Technology\",\n",
    "    \"Health Sciences\",\n",
    "    \"Social Work\",\n",
    "    \"Public Health\",\n",
    "    \"Nutrition\",\n",
    "    \"Linguistics\",\n",
    "    \"Human Resources\",\n",
    "    \"Hospitality Management\",\n",
    "    \"Tourism\",\n",
    "    \"Fashion Design\",\n",
    "    \"Interior Design\"\n",
    "]\n",
    "\n",
    "# Lemmatization of every word in bachelor_subjects\n",
    "bachelor_subjects = [nlp(text) for text in bachelor_subjects]\n",
    "\n",
    "# For each sentence in bachelor_subjects keep the lemma of each word and keep them as strings.\n",
    "bachelor_subjects_lemma = [' '.join([word.lemma_ for word in subject]) for subject in bachelor_subjects]\n",
    "\n",
    "def extract_education(resume_text):\n",
    "    nlp_text = nlp(resume_text)\n",
    "\n",
    "    # Sentence Tokenizer\n",
    "    nlp_text = [sent.text.strip() for sent in nlp_text.sents]\n",
    "\n",
    "    edu = {}\n",
    "\n",
    "    for index, text in enumerate(nlp_text):\n",
    "        for tex in text.split():\n",
    "            # Replace all special symbols\n",
    "                tex = re.sub(r'[?|$|.|!|,\\']', r'', tex)\n",
    "                if tex.lower() in EDUCATION and tex not in STOPWORDS:\n",
    "                    edu['Sentence'] = text\n",
    "                    edu['Education'] = tex\n",
    "\n",
    "                    # Find the GPA.\n",
    "                    edu['GPA'] = re.findall(r'\\b\\d\\.\\d\\b', text)\n",
    "                    \n",
    "                    text_lemma = [nlp(word) for word in text.split()]\n",
    "\n",
    "                    for word in text_lemma:\n",
    "                        for single in word:\n",
    "                            if single.lemma_ in bachelor_subjects_lemma:\n",
    "                                edu['Subject'] = single.lemma_\n",
    "                                \n",
    "                    # Find dates using regex.\n",
    "                    # If you want to extract the year from the text, change the regex to r'(\\d{4})'\n",
    "                    edu['Years'] = re.findall(r'(20\\d{2}|19\\d{2})', text)\n",
    "    \n",
    "    print('Full sentence: ', {edu['Sentence']},\n",
    "            '\\nDegree: ', edu['Education'],\n",
    "            '\\nSubject: ', edu['Subject'],\n",
    "            '\\nGPA: ', edu['GPA'],\n",
    "            '\\nYears: ', edu['Years'])\n",
    "\n",
    "extract_education(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Sentence': \"Experience\\nEDUCATION\\n2014-2020\\nBachelor's degree in Economics.\",\n",
       " 'Education': 'Bachelors',\n",
       " 'GPA': [],\n",
       " 'Subject': 'economic',\n",
       " 'Years': ['2014', '2020']}"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_education(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Experience\\nEDUCATION\\n2014-2020\\nBachelor's degree in Economics.\""
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_education(text)['Sentence']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Email"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mail id:  ['c.fontana95@gmail.com']\n"
     ]
    }
   ],
   "source": [
    "def extract_email_addresses(string):\n",
    "    r = re.compile(r'[\\w\\.-]+@[\\w\\.-]+')\n",
    "    return r.findall(string)\n",
    "print('Mail id: ',extract_email_addresses(text))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phone Number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mobile Number:  3934444659\n"
     ]
    }
   ],
   "source": [
    "def extract_mobile_number(resume_text):\n",
    "    phone = re.findall(re.compile(r'(?:(?:\\+?([1-9]|[0-9][0-9]|[0-9][0-9][0-9])\\s*(?:[.-]\\s*)?)?(?:\\(\\s*([2-9]1[02-9]|[2-9][02-8]1|[2-9][02-8][02-9])\\s*\\)|([0-9][1-9]|[0-9]1[02-9]|[2-9][02-8]1|[2-9][02-8][02-9]))\\s*(?:[.-]\\s*)?)?([2-9]1[02-9]|[2-9][02-9]1|[2-9][02-9]{2})\\s*(?:[.-]\\s*)?([0-9]{4})(?:\\s*(?:#|x\\.?|ext\\.?|extension)\\s*(\\d+))?'), resume_text)\n",
    "    \n",
    "    if phone:\n",
    "        number = ''.join(phone[0])\n",
    "        if len(number) > 10:\n",
    "            return number\n",
    "        else:\n",
    "            return number\n",
    "print('Mobile Number: ',extract_mobile_number(text))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Skills"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "general_skills_list = [\n",
    "    \"Communication Skills\",\n",
    "    \"Verbal Communication\",\n",
    "    \"Written Communication\",\n",
    "    \"Presentation Skills\",\n",
    "    \"Public Speaking\",\n",
    "    \"Interpersonal Skills\",\n",
    "    \"Negotiation Skills\",\n",
    "    \"Listening Skills\",\n",
    "    \"Persuasion Skills\",\n",
    "    \"Teamwork\",\n",
    "    \"Collaboration\",\n",
    "    \"Leadership\",\n",
    "    \"Conflict Resolution\",\n",
    "    \"Relationship Building\",\n",
    "    \"Networking\",\n",
    "    \"Analytical Skills\",\n",
    "    \"Critical Thinking\",\n",
    "    \"Problem Solving\",\n",
    "    \"Research Skills\",\n",
    "    \"Data Analysis\",\n",
    "    \"Quantitative Analysis\",\n",
    "    \"Qualitative Analysis\",\n",
    "    \"Decision Making\",\n",
    "    \"Attention to Detail\",\n",
    "    \"Logical Reasoning\",\n",
    "    \"Technical Skills\",\n",
    "    \"Computer Literacy\",\n",
    "    \"Programming Languages\",\n",
    "    \"Software Proficiency\",\n",
    "    \"Web Development\",\n",
    "    \"Database Management\",\n",
    "    \"Information Technology\",\n",
    "    \"Troubleshooting\",\n",
    "    \"Systems Administration\",\n",
    "    \"Network Security\",\n",
    "    \"Creativity\",\n",
    "    \"Innovation\",\n",
    "    \"Graphic Design\",\n",
    "    \"Artistic Skills\",\n",
    "    \"Photography\",\n",
    "    \"Video Editing\",\n",
    "    \"Content Creation\",\n",
    "    \"Writing Skills\",\n",
    "    \"Copywriting\",\n",
    "    \"Proofreading and Editing\",\n",
    "    \"Organization\",\n",
    "    \"Time Management\",\n",
    "    \"Project Management\",\n",
    "    \"Planning and Coordination\",\n",
    "    \"Multitasking\",\n",
    "    \"Prioritization\",\n",
    "    \"Detail Orientation\",\n",
    "    \"Meeting Deadlines\",\n",
    "    \"Resource Management\",\n",
    "    \"Customer Service\",\n",
    "    \"Client Management\",\n",
    "    \"Relationship Management\",\n",
    "    \"Conflict Resolution (customer-facing)\",\n",
    "    \"Sales Skills\",\n",
    "    \"Account Management\",\n",
    "    \"Marketing Skills\",\n",
    "    \"Market Research\",\n",
    "    \"Advertising\",\n",
    "    \"Social Media Marketing\",\n",
    "    \"Search Engine Optimization (SEO)\",\n",
    "    \"Language Skills\",\n",
    "    \"Bilingualism\",\n",
    "    \"Translation\",\n",
    "    \"Interpretation\",\n",
    "    \"Financial Skills\",\n",
    "    \"Accounting\",\n",
    "    \"Financial Analysis\",\n",
    "    \"Budgeting\",\n",
    "    \"Financial Planning\",\n",
    "    \"Risk Management\",\n",
    "    \"Teaching and Training\",\n",
    "    \"Instructional Design\",\n",
    "    \"Curriculum Development\",\n",
    "    \"Tutoring\",\n",
    "    \"Mentoring\",\n",
    "    \"Project Coordination\",\n",
    "    \"Event Planning\",\n",
    "    \"Event Management\",\n",
    "    \"Logistics\",\n",
    "    \"Supply Chain Management\",\n",
    "    \"Research and Development\",\n",
    "    \"Scientific Methodology\",\n",
    "    \"Lab Techniques\",\n",
    "    \"Experimental Design\",\n",
    "    \"Statistical Analysis\",\n",
    "    \"Problem Diagnosis\",\n",
    "    \"Troubleshooting (Technical)\",\n",
    "    \"Maintenance and Repair\",\n",
    "    \"Equipment Handling\",\n",
    "    \"Mechanical Skills\",\n",
    "    \"Health and Safety\",\n",
    "    \"First Aid\",\n",
    "    \"CPR\",\n",
    "    \"Occupational Health and Safety\",\n",
    "    \"Risk Assessment\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert general skills to lowercase.\n",
    "general_skills_list = [skill.lower() for skill in general_skills_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "General Skills []\n"
     ]
    }
   ],
   "source": [
    "def general_skills(resume_text):\n",
    "    nlp_text = nlp(resume_text)\n",
    "\n",
    "    noun_chunks = nlp_text.noun_chunks\n",
    "\n",
    "    # removing stop words and implementing word tokenization\n",
    "    tokens = [token.text for token in nlp_text if not token.is_stop]\n",
    "    \n",
    "    # extract values\n",
    "    skillset = []\n",
    "    \n",
    "    # check for one-grams (example: python)\n",
    "    for token in tokens:\n",
    "        if token.lower() in general_skills_list:\n",
    "            skillset.append(token)\n",
    "   \n",
    "    for token in noun_chunks:\n",
    "        token = token.text.lower().strip()\n",
    "        if token in general_skills_list:\n",
    "            skillset.append(token)\n",
    "    return [i.capitalize() for i in set([i.lower() for i in skillset])]\n",
    "  \n",
    "print ('General Skills',general_skills(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "programming_skills_list = [\n",
    "    \"Programming Languages\",\n",
    "    \"Python\",\n",
    "    \"JavaScript\",\n",
    "    \"Java\",\n",
    "    \"C++\",\n",
    "    \"C#\",\n",
    "    \"Ruby\",\n",
    "    \"PHP\",\n",
    "    \"Swift\",\n",
    "    \"Go\",\n",
    "    \"Rust\",\n",
    "    \"TypeScript\",\n",
    "    \"HTML\",\n",
    "    \"CSS\",\n",
    "    \"SQL\",\n",
    "    \"Shell Scripting\",\n",
    "    \"Version Control\",\n",
    "    \"Git\",\n",
    "    \"SVN\",\n",
    "    \"Continuous Integration/Continuous Deployment (CI/CD)\",\n",
    "    \"Agile Development\",\n",
    "    \"Test-Driven Development (TDD)\",\n",
    "    \"Object-Oriented Programming (OOP)\",\n",
    "    \"Functional Programming\",\n",
    "    \"Web Development\",\n",
    "    \"Front-end Development\",\n",
    "    \"Back-end Development\",\n",
    "    \"Full-Stack Development\",\n",
    "    \"Mobile Development\",\n",
    "    \"iOS Development\",\n",
    "    \"Android Development\",\n",
    "    \"Database Management\",\n",
    "    \"Database Design\",\n",
    "    \"Query Optimization\",\n",
    "    \"API Development\",\n",
    "    \"RESTful APIs\",\n",
    "    \"Web Services\",\n",
    "    \"Microservices\",\n",
    "    \"Cloud Computing\",\n",
    "    \"Amazon Web Services (AWS)\",\n",
    "    \"Microsoft Azure\",\n",
    "    \"Google Cloud Platform (GCP)\",\n",
    "    \"Containerization\",\n",
    "    \"Docker\",\n",
    "    \"Kubernetes\",\n",
    "    \"Server Administration\",\n",
    "    \"Linux\",\n",
    "    \"Windows Server\",\n",
    "    \"Networking\",\n",
    "    \"Security\",\n",
    "    \"Cybersecurity\",\n",
    "    \"Data Structures\",\n",
    "    \"Algorithms\",\n",
    "    \"Software Development\",\n",
    "    \"Software Architecture\",\n",
    "    \"Software Testing\",\n",
    "    \"Debugging\",\n",
    "    \"Problem Solving\",\n",
    "    \"Code Optimization\",\n",
    "    \"Performance Tuning\",\n",
    "    \"Code Review\",\n",
    "    \"Documentation\",\n",
    "    \"Unit Testing\",\n",
    "    \"Integration Testing\",\n",
    "    \"System Testing\",\n",
    "    \"Front-end Frameworks\",\n",
    "    \"React\",\n",
    "    \"Angular\",\n",
    "    \"Vue.js\",\n",
    "    \"Back-end Frameworks\",\n",
    "    \"Django\",\n",
    "    \"Ruby on Rails\",\n",
    "    \"Node.js\",\n",
    "    \"Flask\",\n",
    "    \"ASP.NET\",\n",
    "    \"PHP Frameworks\",\n",
    "    \"Laravel\",\n",
    "    \"Symfony\",\n",
    "    \"CodeIgniter\",\n",
    "    \"Testing Frameworks\",\n",
    "    \"JUnit\",\n",
    "    \"PyTest\",\n",
    "    \"Mocha\",\n",
    "    \"Jest\",\n",
    "    \"Database Systems\",\n",
    "    \"MySQL\",\n",
    "    \"PostgreSQL\",\n",
    "    \"Oracle\",\n",
    "    \"MongoDB\",\n",
    "    \"Redis\",\n",
    "    \"Machine Learning\",\n",
    "    \"Data Analysis\",\n",
    "    \"Data Visualization\",\n",
    "    \"Artificial Intelligence\",\n",
    "    \"Natural Language Processing (NLP)\",\n",
    "    \"Big Data\",\n",
    "    \"Hadoop\",\n",
    "    \"Spark\",\n",
    "    \"Blockchain Development\",\n",
    "    \"Internet of Things (IoT)\",\n",
    "    \"DevOps\",\n",
    "    \"Infrastructure as Code (IaC)\",\n",
    "    \"Configuration Management\",\n",
    "    \"Scripting\",\n",
    "    \"Problem Diagnosis\",\n",
    "    \"Technical Support\",\n",
    "    \"API Integration\",\n",
    "    \"Project Management\",\n",
    "    \"Agile Methodologies\",\n",
    "    \"Scrum\",\n",
    "    \"Kanban\",\n",
    "    \"Software Documentation\",\n",
    "    \"Collaboration Tools\",\n",
    "    \"Jira\",\n",
    "    \"Confluence\",\n",
    "    \"Slack\",\n",
    "    \"Version Control Systems\",\n",
    "    \"Git\",\n",
    "    \"SVN\",\n",
    "    \"Code Editors\",\n",
    "    \"Visual Studio Code\",\n",
    "    \"PyCharm\",\n",
    "    \"IntelliJ IDEA\",\n",
    "    \"Eclipse\",\n",
    "    \"Sublime Text\",\n",
    "    \"Atom\",\n",
    "    \"Operating Systems\",\n",
    "    \"Linux\",\n",
    "    \"Windows\",\n",
    "    \"macOS\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert programming skills to lower case\n",
    "programming_skills_list = [i.lower() for i in programming_skills_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skills ['Docker', 'Slack', 'Agile methodologies', 'Postgresql', 'Python', 'Spark', 'Mysql', 'Machine learning', 'Git', 'Mongodb', 'Jira', 'Sql']\n"
     ]
    }
   ],
   "source": [
    "def programming_skills(resume_text):\n",
    "    nlp_text = nlp(resume_text)\n",
    "\n",
    "    noun_chunks = nlp_text.noun_chunks\n",
    "\n",
    "    # removing stop words and implementing word tokenization\n",
    "    tokens = [token.text for token in nlp_text if not token.is_stop]\n",
    "\n",
    "    skillset = []\n",
    "    \n",
    "    # check for one-grams (example: python)\n",
    "    for token in tokens:\n",
    "        if token.lower() in programming_skills_list:\n",
    "            skillset.append(token)\n",
    "   \n",
    "    for token in noun_chunks:\n",
    "        token = token.text.lower().strip()\n",
    "        if token in programming_skills_list:\n",
    "            skillset.append(token)\n",
    "    return [i.capitalize() for i in set([i.lower() for i in skillset])]\n",
    "  \n",
    "print ('Skills',programming_skills(text))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "languages_list = [\n",
    "    \"English\",\n",
    "    \"Spanish\",\n",
    "    \"French\",\n",
    "    \"German\",\n",
    "    \"Chinese\",\n",
    "    \"Mandarin\",\n",
    "    \"Arabic\",\n",
    "    \"Hindi\",\n",
    "    \"Portuguese\",\n",
    "    \"Bengali\",\n",
    "    \"Russian\",\n",
    "    \"Japanese\",\n",
    "    \"Lahnda\",\n",
    "    \"Javanese\",\n",
    "    \"Wu\",\n",
    "    \"Telugu\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert languages to lower case\n",
    "languages_list = [i.lower() for i in languages_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "language_level = [\n",
    "    \"Elementary Proficiency\",\n",
    "    \"Limited Working Proficiency\",\n",
    "    \"Professional Working Proficiency\",\n",
    "    \"Full Professional Proficiency\",\n",
    "    \"Native or Bilingual Proficiency\",\n",
    "    \"Native\",\n",
    "    \"Advanced\",\n",
    "    \"Intermediate\",\n",
    "    \"A1\",\n",
    "    \"A2\",\n",
    "    \"B1\",\n",
    "    \"B2\",\n",
    "    \"C1\",\n",
    "    \"C2\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert language levels to lower case\n",
    "language_level = [i.lower() for i in language_level]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Languages: {'Spanish': 'Native', 'English': 'Advanced', 'Portuguese': 'Intermediate'}\n"
     ]
    }
   ],
   "source": [
    "def language_skill(resume_text):\n",
    "\n",
    "    resume_text = re.sub(r'\\.(?!\\))', r'. ', resume_text)\n",
    "\n",
    "    nlp_text = nlp(resume_text)\n",
    "\n",
    "    # Removing stop words and implementing word tokenization\n",
    "    tokens = [token.text for token in nlp_text if not token.is_stop]\n",
    "\n",
    "    # Drop every token that is equal to a special character\n",
    "    tokens = [token for token in tokens if not token in string.punctuation]\n",
    "\n",
    "    skillset = {}\n",
    "\n",
    "    # check for one-grams (example: python)\n",
    "    for token in tokens:\n",
    "        if token.lower() in languages_list:\n",
    "            skillset[token] = 'Level Not Specified'\n",
    "            if tokens[tokens.index(token) + 1].lower() in language_level:\n",
    "                skillset[token] = tokens[tokens.index(token) + 1]\n",
    "\n",
    "    return skillset\n",
    "  \n",
    "print ('Languages:' ,language_skill(text))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus Points\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "bonus_points_list = [\n",
    "    'Projects',\n",
    "    'Achievements',\n",
    "    'Hobbies'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bonus: {'Projects': 'Yes', 'Achievements': 'No', 'Hobbies': 'No'}\n"
     ]
    }
   ],
   "source": [
    "def bonus(resume_text):\n",
    "    nlp_text = nlp(resume_text)\n",
    "\n",
    "    # removing stop words and implementing word tokenization\n",
    "    tokens = [token.text for token in nlp_text if not token.is_stop]\n",
    "\n",
    "    # Put tokens list in lower case\n",
    "    tokens = [token.lower() for token in tokens]\n",
    "\n",
    "    bonus_points = {}\n",
    "\n",
    "    # check for one-grams (example: python)\n",
    "    for bonus_piece in bonus_points_list:\n",
    "        if bonus_piece.lower() in tokens:\n",
    "            bonus_points[bonus_piece] = 'Yes'\n",
    "        else:\n",
    "            bonus_points[bonus_piece] = 'No'\n",
    "\n",
    "    return bonus_points\n",
    "  \n",
    "print ('Bonus:' , bonus(text))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: ['Cristian Fontana']\n",
      "Predicted Profile: Data Science\n",
      "Full sentence:  {\"Experience\\nEDUCATION\\n2014-2020\\nBachelor's degree in Economics.\"} \n",
      "Degree:  Bachelors \n",
      "Subject:  economic \n",
      "GPA:  [] \n",
      "Years:  ['2014', '2020']\n",
      "Mail id:  ['c.fontana95@gmail.com']\n",
      "Mobile Number:  3934444659\n",
      "General Skills: []\n",
      "Programming Skills: ['Docker', 'Slack', 'Agile methodologies', 'Postgresql', 'Python', 'Spark', 'Mysql', 'Machine learning', 'Git', 'Mongodb', 'Jira', 'Sql']\n",
      "Languages: {'Spanish': 'Native', 'English': 'Advanced', 'Portuguese': 'Intermediate'}\n",
      "Bonus: {'Projects': 'Yes', 'Achievements': 'No', 'Hobbies': 'No'}\n"
     ]
    }
   ],
   "source": [
    "org_name(\"C:/Users/cfont/Downloads/Cristian Fontana - CV-Resume.pdf\")\n",
    "print('Predicted Profile:', predict_profile.predict(new_inputs(pd.Series(text)))[0])\n",
    "extract_education(text)\n",
    "print('Mail id: ',extract_email_addresses(text))\n",
    "print('Mobile Number: ',extract_mobile_number(text))\n",
    "print ('General Skills:',general_skills(text))\n",
    "print ('Programming Skills:',programming_skills(text))\n",
    "print ('Languages:' ,language_skill(text))\n",
    "print ('Bonus:' , bonus(text))\n",
    "\n",
    "# Add github.\n",
    "# add linkedin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid non-printable character U+00A0 (14378996.py, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[27], line 3\u001b[1;36m\u001b[0m\n\u001b[1;33m    ya eso Ayrton puede consumirlo en la app de HR q esta haciendo con PHP Laravel\u001b[0m\n\u001b[1;37m                                                                  ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid non-printable character U+00A0\n"
     ]
    }
   ],
   "source": [
    "el punto 2, si usas Python, y haces una Lambda en AWS que de entrada tiene un Json ó CSV\n",
    "con los datos del CSV y de salida te devuelve un Json o CSV con los datos del candidato,\n",
    "ya eso Ayrton puede consumirlo en la app de HR q esta haciendo con PHP Laravel\n",
    "\n",
    "y de respuesta, es un json q diga por ej: \n",
    "\n",
    "Perfil: front end developer\n",
    "Edad: 20 años\n",
    "Tech principal: JavaScript\n",
    "Framework principal: React Native\n",
    "Tech secundaria: CSS\n",
    "Ultima empresa donde trabajo: Amazon\n",
    "Años en ultima empresa: 4.5\n",
    "Idioma principal: Ingles\n",
    "Nivel del idioma Principal: 8"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

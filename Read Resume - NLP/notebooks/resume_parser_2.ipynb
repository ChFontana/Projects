{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import docx2txt\n",
    "from PyPDF2 import PdfReader, PdfFileWriter, PdfFileMerger\n",
    "\n",
    "import spacy\n",
    "import en_core_web_sm\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "import pickle\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from spacy import displacy\n",
    "\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\cfont\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<spacy.lang.en.English at 0x29f573bc410>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "spacy.load('en_core_web_sm')\n",
    "spacy.load('en_core_web_lg')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extracting text from DOCX\n",
    "def doctotext(m):\n",
    "    temp = docx2txt.process(m)\n",
    "    resume_text = [line.replace('\\t', ' ') for line in temp.split('\\n') if line]\n",
    "    text = ' '.join(resume_text)\n",
    "    return (text)\n",
    "    \n",
    "#Extracting text from PDF\n",
    "def pdftotext(m):\n",
    "    # pdf file object\n",
    "    # you can find find the pdf file with complete code in below\n",
    "    pdfFileObj = open(m, 'rb')\n",
    "\n",
    "    # pdf reader object\n",
    "    pdfFileReader = PdfReader(pdfFileObj)\n",
    "\n",
    "    # number of pages in pdf\n",
    "    num_pages = len(pdfFileReader.pages)\n",
    "\n",
    "    currentPageNumber = 0\n",
    "    text = ''\n",
    "\n",
    "    # Loop in all the pdf pages.\n",
    "    while(currentPageNumber < num_pages ):\n",
    "\n",
    "        # Get the specified pdf page object.\n",
    "        pdfPage = pdfFileReader.pages[currentPageNumber]\n",
    "\n",
    "        # Get pdf page text.\n",
    "        text = text + pdfPage.extract_text()\n",
    "\n",
    "        # Process next page.\n",
    "        currentPageNumber += 1\n",
    "\n",
    "    return (text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = pdftotext(\"C:/Users/cfont/Downloads/Cristian Fontana - CV-Resume.pdf\")\n",
    "\n",
    "# load pre-trained model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "# nlp_big = spacy.load('en_core_web_lg')\n",
    "\n",
    "# initialize matcher with a vocab\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "# displacy.render(nlp(text),style=\"ent\",jupyter=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cfont\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The time of execution of above program is : 7.7\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "from subprocess import list2cmdline\n",
    "from pdfminer.high_level import extract_text\n",
    "import docx2txt\n",
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "import time\n",
    "start = time.time()\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "model_checkpoint = \"xlm-roberta-large-finetuned-conll03-english\"\n",
    "token_classifier = pipeline(\n",
    "    \"token-classification\", model=model_checkpoint, aggregation_strategy=\"simple\"\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "def text_extraction(file):\n",
    "    \"\"\"\"\n",
    "    To extract texts from both pdf and word\n",
    "    \"\"\"\n",
    "    if file.endswith(\".pdf\"):\n",
    "        return extract_text(file)\n",
    "    else:\n",
    "        resume_text = docx2txt.process(file)\n",
    "    if resume_text:\n",
    "        return resume_text.replace('\\t', ' ')\n",
    "    return None\n",
    "\n",
    "\n",
    "\n",
    "# Organisation names extraction\n",
    "def org_name(file):\n",
    "    # Extract the complete text in the resume\n",
    "    extracted_text = text_extraction(file)\n",
    "    classifier = token_classifier(extracted_text)\n",
    "    # Get the list of dictionary with key value pair \"entity\":'ORG'\n",
    "    values = [item for item in classifier if item[\"entity_group\"] == \"PER\"]\n",
    "    # Get the list of dictionary with key value pair \"entity\":'ORG'\n",
    "    res = [sub['word'] for sub in values]\n",
    "    final1 = list(set(res))  # Remove duplicates\n",
    "    final = list(filter(None, final1)) # Remove empty strings\n",
    "    print('Name:', final)\n",
    "\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(\"The time of execution of above program is :\", round((end - start), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ner_pipeline = pipeline(\"ner\", model=\"dccuchile/bert-base-spanish-wwm-uncased\", tokenizer=\"dccuchile/bert-base-spanish-wwm-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline\n",
    "\n",
    "# bert_tokenizer = AutoTokenizer.from_pretrained('dslim/bert-large-NER')\n",
    "# bert_model = AutoModelForTokenClassification.from_pretrained('dslim/bert-large-NER')\n",
    "\n",
    "# nlp = pipeline('ner', model=bert_model, tokenizer=bert_tokenizer)\n",
    "# ner_list = nlp(text_extraction(\"C:/Users/cfont/Downloads/Cristian Fontana - CV-Resume.pdf\"))\n",
    "# print(ner_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this_name = []\n",
    "# all_names_list_tmp = []\n",
    "\n",
    "# for ner_dict in ner_list:\n",
    "#     if ner_dict['entity'] == 'B-PER':\n",
    "#         if len(this_name) == 0:\n",
    "#             this_name.append(ner_dict['word'])\n",
    "#         else:\n",
    "#             all_names_list_tmp.append([this_name])\n",
    "#             this_name = []\n",
    "#             this_name.append(ner_dict['word'])\n",
    "#     elif ner_dict['entity'] == 'I-PER':\n",
    "#         this_name.append(ner_dict['word'])\n",
    "\n",
    "# all_names_list_tmp.append([this_name])\n",
    "\n",
    "# print(all_names_list_tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def find_spanish_full_names(text):\n",
    "#     entities = ner_pipeline(text)\n",
    "#     person_names = []\n",
    "#     current_name = \"\"\n",
    "\n",
    "#     for entity in entities:\n",
    "#         if entity[\"entity\"] == \"I-PER\":\n",
    "#             current_name += entity[\"word\"] + \" \"\n",
    "#         elif current_name:\n",
    "#             person_names.append(current_name.strip())\n",
    "#             current_name = \"\"\n",
    "\n",
    "#     if current_name:\n",
    "#         person_names.append(current_name.strip())\n",
    "\n",
    "#     return person_names\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find_spanish_full_names(text_extraction(\"C:/Users/cfont/Downloads/Cristian Fontana - CV-Resume.pdf\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import pipeline\n",
    "\n",
    "# ner_pipeline = pipeline(\"ner\", model=\"bert-base-uncased\", tokenizer=\"bert-base-uncased\")\n",
    "\n",
    "# def find_person_names(text):\n",
    "#     entities = ner_pipeline(text)\n",
    "#     person_names = [entity[\"word\"] for entity in entities if entity[\"entity\"] == \"PER\"]\n",
    "#     return person_names\n",
    "\n",
    "# find_person_names(re.sub(r'\\.(?!\\))', r'. ', text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def find_full_names(text):\n",
    "#     entities = ner_pipeline(text)\n",
    "#     person_names = []\n",
    "#     current_name = \"\"\n",
    "\n",
    "#     for entity in entities:\n",
    "#         if entity[\"entity\"] == \"PER\":\n",
    "#             current_name += entity[\"word\"] + \" \"\n",
    "#         elif current_name:\n",
    "#             person_names.append(current_name.strip())\n",
    "#             current_name = \"\"\n",
    "\n",
    "#     if current_name:\n",
    "#         person_names.append(current_name.strip())\n",
    "\n",
    "#     return person_names\n",
    "\n",
    "# find_full_names(re.sub(r'\\.(?!\\))', r'. ', text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean_function(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def extract_name(resume_text):\n",
    "#     nlp_text = nlp(resume_text)\n",
    "    \n",
    "#     # First name and Last name are always Proper Nouns\n",
    "#     pattern = [{'POS': 'PROPN'}, {'POS': 'PROPN'}]\n",
    "    \n",
    "#     matcher.add('NAME', [pattern])\n",
    "    \n",
    "#     matches = matcher(nlp_text)\n",
    "    \n",
    "#     for match_id, start, end in matches:\n",
    "#         span = nlp_text[start:end]\n",
    "#         return span.text\n",
    "# print('Name: ',extract_name(pdftotext(\"C:/Users/cfont/Downloads/Cristian Fontana - CV-Resume.pdf\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict Profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pickle models.\n",
    "with open('C:/Users/cfont/OneDrive/Documents/GitHub/Projects/Read Resume - NLP/src/models/model.pkl', 'rb') as f:\n",
    "    predict_profile = pickle.load(f)\n",
    "\n",
    "with open('C:/Users/cfont/OneDrive/Documents/GitHub/Projects/Read Resume - NLP/src/models/vectorizer.pkl', 'rb') as f:\n",
    "    vectorizer = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the resume text.\n",
    "def clean_function(resumeText):\n",
    "    resumeText = re.sub('http\\S+\\s*', ' ', resumeText)  # remove URLs\n",
    "    resumeText = re.sub('RT|cc', ' ', resumeText)  # remove RT and cc\n",
    "    resumeText = re.sub('#\\S+', '', resumeText)  # remove hashtags\n",
    "    resumeText = re.sub('@\\S+', '  ', resumeText)  # remove mentions\n",
    "    resumeText = re.sub('[%s]' % re.escape(\"\"\"!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\"\"\"), ' ', resumeText)  # remove punctuations\n",
    "    resumeText = re.sub(r'[^\\x00-\\x7f]',r' ', resumeText) \n",
    "    resumeText = re.sub('\\s+', ' ', resumeText)  # remove extra whitespace\n",
    "    return resumeText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleand and vectorize the resume text.\n",
    "def new_inputs(resumes):\n",
    "    cleaned_resumes = resumes.apply(lambda x:clean_function(x))\n",
    "    transformed_resumes = vectorizer.transform(cleaned_resumes)\n",
    "    return transformed_resumes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Profile: Data Science\n"
     ]
    }
   ],
   "source": [
    "# Predict the profile.\n",
    "print('Predicted Profile:', predict_profile.predict(new_inputs(pd.Series(text)))[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Qualification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pre-trained model\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full sentence:  {\"Experience\\nEDUCATION\\n2014-2020\\nBachelor's degree in Economics.\"} \n",
      "Degree:  Bachelors \n",
      "Subject:  economic \n",
      "GPA:  [] \n",
      "Years:  ['2014', '2020']\n"
     ]
    }
   ],
   "source": [
    "# Grad all general stop words\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "\n",
    "# Education Degrees\n",
    "EDUCATION = [\n",
    "            'BE','B.E.', 'B.E', 'BS', 'B.S', \n",
    "            'ME', 'M.E', 'M.E.', 'M.B.A', 'MBA', 'MS', 'M.S', \n",
    "            'BTECH', 'B.TECH', 'M.TECH', 'MTECH', \n",
    "            'SSLC', 'SSC' 'HSC', 'CBSE', 'ICSE', 'X', 'XII',\n",
    "            'BACHELOR', 'MASTER', 'PHD', 'BACHELORS', 'MASTERS', 'Ph.D.',\n",
    "            'Licenciatura', 'Ingeniería', 'Maestría', 'Maestria',\n",
    "            'Maestra', 'Maestro', 'Doctorado', 'Doctora', 'Doctor', 'Licenciado', 'Licenciada',\n",
    "            'Ingeniero', 'Ingeniera', 'Maestrante', 'Doctorante', 'L'\n",
    "            'Lic', 'Ing'\n",
    "        ]\n",
    "\n",
    "# Make every word in EDUCATION lowercase.\n",
    "EDUCATION = [x.lower() for x in EDUCATION]\n",
    "\n",
    "bachelor_subjects = [\n",
    "    \"Computer Science\",\n",
    "    \"Physics\",\n",
    "    \"Chemistry\",\n",
    "    \"Biology\",\n",
    "    \"Mathematics\",\n",
    "    \"Engineering\",\n",
    "    \"Psychology\",\n",
    "    \"English Literature\",\n",
    "    \"History\",\n",
    "    \"Sociology\",\n",
    "    \"Economics\",\n",
    "    \"Political Science\",\n",
    "    \"Business Administration\",\n",
    "    \"Marketing\",\n",
    "    \"Accounting\",\n",
    "    \"Finance\",\n",
    "    \"Nursing\",\n",
    "    \"Environmental Science\",\n",
    "    \"Art\",\n",
    "    \"Music\",\n",
    "    \"Film Studies\",\n",
    "    \"Philosophy\",\n",
    "    \"Anthropology\",\n",
    "    \"Communications\",\n",
    "    \"Languages\",\n",
    "    \"Geography\",\n",
    "    \"Architecture\",\n",
    "    \"Urban Planning\",\n",
    "    \"Graphic Design\",\n",
    "    \"Journalism\",\n",
    "    \"Criminal Justice\",\n",
    "    \"Law\",\n",
    "    \"International Relations\",\n",
    "    \"Sports Science\",\n",
    "    \"Theater\",\n",
    "    \"Dance\",\n",
    "    \"Religious Studies\",\n",
    "    \"Information Technology\",\n",
    "    \"Health Sciences\",\n",
    "    \"Social Work\",\n",
    "    \"Public Health\",\n",
    "    \"Nutrition\",\n",
    "    \"Linguistics\",\n",
    "    \"Human Resources\",\n",
    "    \"Hospitality Management\",\n",
    "    \"Tourism\",\n",
    "    \"Fashion Design\",\n",
    "    \"Interior Design\"\n",
    "]\n",
    "\n",
    "# Lemmatization of every word in bachelor_subjects\n",
    "bachelor_subjects = [nlp(text) for text in bachelor_subjects]\n",
    "\n",
    "# For each sentence in bachelor_subjects keep the lemma of each word and keep them as strings.\n",
    "bachelor_subjects_lemma = [' '.join([word.lemma_ for word in subject]) for subject in bachelor_subjects]\n",
    "\n",
    "def extract_education(resume_text):\n",
    "    nlp_text = nlp(resume_text)\n",
    "\n",
    "    # Sentence Tokenizer\n",
    "    nlp_text = [sent.text.strip() for sent in nlp_text.sents]\n",
    "\n",
    "    edu = {}\n",
    "\n",
    "    for index, text in enumerate(nlp_text):\n",
    "        for tex in text.split():\n",
    "            # Replace all special symbols\n",
    "                tex = re.sub(r'[?|$|.|!|,\\']', r'', tex)\n",
    "                if tex.lower() in EDUCATION and tex not in STOPWORDS:\n",
    "                    edu['Sentence'] = text\n",
    "                    edu['Education'] = tex\n",
    "\n",
    "                    # Find the GPA.\n",
    "                    edu['GPA'] = re.findall(r'\\b\\d\\.\\d\\b', text)\n",
    "                    \n",
    "                    text_lemma = [nlp(word) for word in text.split()]\n",
    "\n",
    "                    for word in text_lemma:\n",
    "                        for single in word:\n",
    "                            if single.lemma_ in bachelor_subjects_lemma:\n",
    "                                edu['Subject'] = single.lemma_\n",
    "                                \n",
    "                    # Find dates using regex.\n",
    "                    # If you want to extract the year from the text, change the regex to r'(\\d{4})'\n",
    "                    edu['Years'] = re.findall(r'(20\\d{2}|19\\d{2})', text)\n",
    "    \n",
    "    print('Full sentence: ', {edu['Sentence']},\n",
    "            '\\nDegree: ', edu['Education'],\n",
    "            '\\nSubject: ', edu['Subject'],\n",
    "            '\\nGPA: ', edu['GPA'],\n",
    "            '\\nYears: ', edu['Years'])\n",
    "\n",
    "extract_education(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full sentence:  {\"Experience\\nEDUCATION\\n2014-2020\\nBachelor's degree in Economics.\"} \n",
      "Degree:  Bachelors \n",
      "Subject:  economic \n",
      "GPA:  [] \n",
      "Years:  ['2014', '2020']\n"
     ]
    }
   ],
   "source": [
    "extract_education(text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Email"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mail id:  ['c.fontana95@gmail.com']\n"
     ]
    }
   ],
   "source": [
    "def extract_email_addresses(string):\n",
    "    r = re.compile(r'[\\w\\.-]+@[\\w\\.-]+')\n",
    "    return r.findall(string)\n",
    "print('Mail id: ',extract_email_addresses(text))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phone Number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mobile Number:  3934444659\n"
     ]
    }
   ],
   "source": [
    "def extract_mobile_number(resume_text):\n",
    "    phone = re.findall(re.compile(r'(?:(?:\\+?([1-9]|[0-9][0-9]|[0-9][0-9][0-9])\\s*(?:[.-]\\s*)?)?(?:\\(\\s*([2-9]1[02-9]|[2-9][02-8]1|[2-9][02-8][02-9])\\s*\\)|([0-9][1-9]|[0-9]1[02-9]|[2-9][02-8]1|[2-9][02-8][02-9]))\\s*(?:[.-]\\s*)?)?([2-9]1[02-9]|[2-9][02-9]1|[2-9][02-9]{2})\\s*(?:[.-]\\s*)?([0-9]{4})(?:\\s*(?:#|x\\.?|ext\\.?|extension)\\s*(\\d+))?'), resume_text)\n",
    "    \n",
    "    if phone:\n",
    "        number = ''.join(phone[0])\n",
    "        if len(number) > 10:\n",
    "            return number\n",
    "        else:\n",
    "            return number\n",
    "print('Mobile Number: ',extract_mobile_number(text))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Skills"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "general_skills_list = [\n",
    "    \"Communication Skills\",\n",
    "    \"Verbal Communication\",\n",
    "    \"Written Communication\",\n",
    "    \"Presentation Skills\",\n",
    "    \"Public Speaking\",\n",
    "    \"Interpersonal Skills\",\n",
    "    \"Negotiation Skills\",\n",
    "    \"Listening Skills\",\n",
    "    \"Persuasion Skills\",\n",
    "    \"Teamwork\",\n",
    "    \"Collaboration\",\n",
    "    \"Leadership\",\n",
    "    \"Conflict Resolution\",\n",
    "    \"Relationship Building\",\n",
    "    \"Networking\",\n",
    "    \"Analytical Skills\",\n",
    "    \"Critical Thinking\",\n",
    "    \"Problem Solving\",\n",
    "    \"Research Skills\",\n",
    "    \"Data Analysis\",\n",
    "    \"Quantitative Analysis\",\n",
    "    \"Qualitative Analysis\",\n",
    "    \"Decision Making\",\n",
    "    \"Attention to Detail\",\n",
    "    \"Logical Reasoning\",\n",
    "    \"Technical Skills\",\n",
    "    \"Computer Literacy\",\n",
    "    \"Programming Languages\",\n",
    "    \"Software Proficiency\",\n",
    "    \"Web Development\",\n",
    "    \"Database Management\",\n",
    "    \"Information Technology\",\n",
    "    \"Troubleshooting\",\n",
    "    \"Systems Administration\",\n",
    "    \"Network Security\",\n",
    "    \"Creativity\",\n",
    "    \"Innovation\",\n",
    "    \"Graphic Design\",\n",
    "    \"Artistic Skills\",\n",
    "    \"Photography\",\n",
    "    \"Video Editing\",\n",
    "    \"Content Creation\",\n",
    "    \"Writing Skills\",\n",
    "    \"Copywriting\",\n",
    "    \"Proofreading and Editing\",\n",
    "    \"Organization\",\n",
    "    \"Time Management\",\n",
    "    \"Project Management\",\n",
    "    \"Planning and Coordination\",\n",
    "    \"Multitasking\",\n",
    "    \"Prioritization\",\n",
    "    \"Detail Orientation\",\n",
    "    \"Meeting Deadlines\",\n",
    "    \"Resource Management\",\n",
    "    \"Customer Service\",\n",
    "    \"Client Management\",\n",
    "    \"Relationship Management\",\n",
    "    \"Conflict Resolution (customer-facing)\",\n",
    "    \"Sales Skills\",\n",
    "    \"Account Management\",\n",
    "    \"Marketing Skills\",\n",
    "    \"Market Research\",\n",
    "    \"Advertising\",\n",
    "    \"Social Media Marketing\",\n",
    "    \"Search Engine Optimization (SEO)\",\n",
    "    \"Language Skills\",\n",
    "    \"Bilingualism\",\n",
    "    \"Translation\",\n",
    "    \"Interpretation\",\n",
    "    \"Financial Skills\",\n",
    "    \"Accounting\",\n",
    "    \"Financial Analysis\",\n",
    "    \"Budgeting\",\n",
    "    \"Financial Planning\",\n",
    "    \"Risk Management\",\n",
    "    \"Teaching and Training\",\n",
    "    \"Instructional Design\",\n",
    "    \"Curriculum Development\",\n",
    "    \"Tutoring\",\n",
    "    \"Mentoring\",\n",
    "    \"Project Coordination\",\n",
    "    \"Event Planning\",\n",
    "    \"Event Management\",\n",
    "    \"Logistics\",\n",
    "    \"Supply Chain Management\",\n",
    "    \"Research and Development\",\n",
    "    \"Scientific Methodology\",\n",
    "    \"Lab Techniques\",\n",
    "    \"Experimental Design\",\n",
    "    \"Statistical Analysis\",\n",
    "    \"Problem Diagnosis\",\n",
    "    \"Troubleshooting (Technical)\",\n",
    "    \"Maintenance and Repair\",\n",
    "    \"Equipment Handling\",\n",
    "    \"Mechanical Skills\",\n",
    "    \"Health and Safety\",\n",
    "    \"First Aid\",\n",
    "    \"CPR\",\n",
    "    \"Occupational Health and Safety\",\n",
    "    \"Risk Assessment\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert general skills to lowercase.\n",
    "general_skills_list = [skill.lower() for skill in general_skills_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "General Skills []\n"
     ]
    }
   ],
   "source": [
    "def general_skills(resume_text):\n",
    "    nlp_text = nlp(resume_text)\n",
    "\n",
    "    noun_chunks = nlp_text.noun_chunks\n",
    "\n",
    "    # removing stop words and implementing word tokenization\n",
    "    tokens = [token.text for token in nlp_text if not token.is_stop]\n",
    "    \n",
    "    # extract values\n",
    "    skillset = []\n",
    "    \n",
    "    # check for one-grams (example: python)\n",
    "    for token in tokens:\n",
    "        if token.lower() in general_skills_list:\n",
    "            skillset.append(token)\n",
    "   \n",
    "    for token in noun_chunks:\n",
    "        token = token.text.lower().strip()\n",
    "        if token in general_skills_list:\n",
    "            skillset.append(token)\n",
    "    return [i.capitalize() for i in set([i.lower() for i in skillset])]\n",
    "  \n",
    "print ('General Skills',general_skills(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "programming_skills_list = [\n",
    "    \"Programming Languages\",\n",
    "    \"Python\",\n",
    "    \"JavaScript\",\n",
    "    \"Java\",\n",
    "    \"C++\",\n",
    "    \"C#\",\n",
    "    \"Ruby\",\n",
    "    \"PHP\",\n",
    "    \"Swift\",\n",
    "    \"Go\",\n",
    "    \"Rust\",\n",
    "    \"TypeScript\",\n",
    "    \"HTML\",\n",
    "    \"CSS\",\n",
    "    \"SQL\",\n",
    "    \"Shell Scripting\",\n",
    "    \"Version Control\",\n",
    "    \"Git\",\n",
    "    \"SVN\",\n",
    "    \"Continuous Integration/Continuous Deployment (CI/CD)\",\n",
    "    \"Agile Development\",\n",
    "    \"Test-Driven Development (TDD)\",\n",
    "    \"Object-Oriented Programming (OOP)\",\n",
    "    \"Functional Programming\",\n",
    "    \"Web Development\",\n",
    "    \"Front-end Development\",\n",
    "    \"Back-end Development\",\n",
    "    \"Full-Stack Development\",\n",
    "    \"Mobile Development\",\n",
    "    \"iOS Development\",\n",
    "    \"Android Development\",\n",
    "    \"Database Management\",\n",
    "    \"Database Design\",\n",
    "    \"Query Optimization\",\n",
    "    \"API Development\",\n",
    "    \"RESTful APIs\",\n",
    "    \"Web Services\",\n",
    "    \"Microservices\",\n",
    "    \"Cloud Computing\",\n",
    "    \"Amazon Web Services (AWS)\",\n",
    "    \"Microsoft Azure\",\n",
    "    \"Google Cloud Platform (GCP)\",\n",
    "    \"Containerization\",\n",
    "    \"Docker\",\n",
    "    \"Kubernetes\",\n",
    "    \"Server Administration\",\n",
    "    \"Linux\",\n",
    "    \"Windows Server\",\n",
    "    \"Networking\",\n",
    "    \"Security\",\n",
    "    \"Cybersecurity\",\n",
    "    \"Data Structures\",\n",
    "    \"Algorithms\",\n",
    "    \"Software Development\",\n",
    "    \"Software Architecture\",\n",
    "    \"Software Testing\",\n",
    "    \"Debugging\",\n",
    "    \"Problem Solving\",\n",
    "    \"Code Optimization\",\n",
    "    \"Performance Tuning\",\n",
    "    \"Code Review\",\n",
    "    \"Documentation\",\n",
    "    \"Unit Testing\",\n",
    "    \"Integration Testing\",\n",
    "    \"System Testing\",\n",
    "    \"Front-end Frameworks\",\n",
    "    \"React\",\n",
    "    \"Angular\",\n",
    "    \"Vue.js\",\n",
    "    \"Back-end Frameworks\",\n",
    "    \"Django\",\n",
    "    \"Ruby on Rails\",\n",
    "    \"Node.js\",\n",
    "    \"Flask\",\n",
    "    \"ASP.NET\",\n",
    "    \"PHP Frameworks\",\n",
    "    \"Laravel\",\n",
    "    \"Symfony\",\n",
    "    \"CodeIgniter\",\n",
    "    \"Testing Frameworks\",\n",
    "    \"JUnit\",\n",
    "    \"PyTest\",\n",
    "    \"Mocha\",\n",
    "    \"Jest\",\n",
    "    \"Database Systems\",\n",
    "    \"MySQL\",\n",
    "    \"PostgreSQL\",\n",
    "    \"Oracle\",\n",
    "    \"MongoDB\",\n",
    "    \"Redis\",\n",
    "    \"Machine Learning\",\n",
    "    \"Data Analysis\",\n",
    "    \"Data Visualization\",\n",
    "    \"Artificial Intelligence\",\n",
    "    \"Natural Language Processing (NLP)\",\n",
    "    \"Big Data\",\n",
    "    \"Hadoop\",\n",
    "    \"Spark\",\n",
    "    \"Blockchain Development\",\n",
    "    \"Internet of Things (IoT)\",\n",
    "    \"DevOps\",\n",
    "    \"Infrastructure as Code (IaC)\",\n",
    "    \"Configuration Management\",\n",
    "    \"Scripting\",\n",
    "    \"Problem Diagnosis\",\n",
    "    \"Technical Support\",\n",
    "    \"API Integration\",\n",
    "    \"Project Management\",\n",
    "    \"Agile Methodologies\",\n",
    "    \"Scrum\",\n",
    "    \"Kanban\",\n",
    "    \"Software Documentation\",\n",
    "    \"Collaboration Tools\",\n",
    "    \"Jira\",\n",
    "    \"Confluence\",\n",
    "    \"Slack\",\n",
    "    \"Version Control Systems\",\n",
    "    \"Git\",\n",
    "    \"SVN\",\n",
    "    \"Code Editors\",\n",
    "    \"Visual Studio Code\",\n",
    "    \"PyCharm\",\n",
    "    \"IntelliJ IDEA\",\n",
    "    \"Eclipse\",\n",
    "    \"Sublime Text\",\n",
    "    \"Atom\",\n",
    "    \"Operating Systems\",\n",
    "    \"Linux\",\n",
    "    \"Windows\",\n",
    "    \"macOS\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert programming skills to lower case\n",
    "programming_skills_list = [i.lower() for i in programming_skills_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skills ['Slack', 'Machine learning', 'Python', 'Docker', 'Git', 'Mongodb', 'Mysql', 'Jira', 'Sql', 'Postgresql', 'Agile methodologies', 'Spark']\n"
     ]
    }
   ],
   "source": [
    "def programming_skills(resume_text):\n",
    "    nlp_text = nlp(resume_text)\n",
    "\n",
    "    noun_chunks = nlp_text.noun_chunks\n",
    "\n",
    "    # removing stop words and implementing word tokenization\n",
    "    tokens = [token.text for token in nlp_text if not token.is_stop]\n",
    "\n",
    "    skillset = []\n",
    "    \n",
    "    # check for one-grams (example: python)\n",
    "    for token in tokens:\n",
    "        if token.lower() in programming_skills_list:\n",
    "            skillset.append(token)\n",
    "   \n",
    "    for token in noun_chunks:\n",
    "        token = token.text.lower().strip()\n",
    "        if token in programming_skills_list:\n",
    "            skillset.append(token)\n",
    "    return [i.capitalize() for i in set([i.lower() for i in skillset])]\n",
    "  \n",
    "print ('Skills',programming_skills(text))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "languages_list = [\n",
    "    \"English\",\n",
    "    \"Spanish\",\n",
    "    \"French\",\n",
    "    \"German\",\n",
    "    \"Chinese\",\n",
    "    \"Mandarin\",\n",
    "    \"Arabic\",\n",
    "    \"Hindi\",\n",
    "    \"Portuguese\",\n",
    "    \"Bengali\",\n",
    "    \"Russian\",\n",
    "    \"Japanese\",\n",
    "    \"Lahnda\",\n",
    "    \"Javanese\",\n",
    "    \"Wu\",\n",
    "    \"Telugu\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert languages to lower case\n",
    "languages_list = [i.lower() for i in languages_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "language_level = [\n",
    "    \"Elementary Proficiency\",\n",
    "    \"Limited Working Proficiency\",\n",
    "    \"Professional Working Proficiency\",\n",
    "    \"Full Professional Proficiency\",\n",
    "    \"Native or Bilingual Proficiency\",\n",
    "    \"Native\",\n",
    "    \"Advanced\",\n",
    "    \"Intermediate\",\n",
    "    \"A1\",\n",
    "    \"A2\",\n",
    "    \"B1\",\n",
    "    \"B2\",\n",
    "    \"C1\",\n",
    "    \"C2\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert language levels to lower case\n",
    "language_level = [i.lower() for i in language_level]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Languages: {'Spanish': 'Native', 'English': 'Advanced', 'Portuguese': 'Intermediate'}\n"
     ]
    }
   ],
   "source": [
    "def language_skill(resume_text):\n",
    "\n",
    "    resume_text = re.sub(r'\\.(?!\\))', r'. ', resume_text)\n",
    "\n",
    "    nlp_text = nlp(resume_text)\n",
    "\n",
    "    # Removing stop words and implementing word tokenization\n",
    "    tokens = [token.text for token in nlp_text if not token.is_stop]\n",
    "\n",
    "    # Drop every token that is equal to a special character\n",
    "    tokens = [token for token in tokens if not token in string.punctuation]\n",
    "\n",
    "    skillset = {}\n",
    "\n",
    "    # check for one-grams (example: python)\n",
    "    for token in tokens:\n",
    "        if token.lower() in languages_list:\n",
    "            skillset[token] = 'Level Not Specified'\n",
    "            if tokens[tokens.index(token) + 1].lower() in language_level:\n",
    "                skillset[token] = tokens[tokens.index(token) + 1]\n",
    "\n",
    "    return skillset\n",
    "  \n",
    "print ('Languages:' ,language_skill(text))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus Points\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "bonus_points_list = [\n",
    "    'Projects',\n",
    "    'Achievements',\n",
    "    'Hobbies'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bonus: {'Projects': 'Yes', 'Achievements': 'No', 'Hobbies': 'No'}\n"
     ]
    }
   ],
   "source": [
    "def bonus(resume_text):\n",
    "    nlp_text = nlp(resume_text)\n",
    "\n",
    "    # removing stop words and implementing word tokenization\n",
    "    tokens = [token.text for token in nlp_text if not token.is_stop]\n",
    "\n",
    "    # Put tokens list in lower case\n",
    "    tokens = [token.lower() for token in tokens]\n",
    "\n",
    "    bonus_points = {}\n",
    "\n",
    "    # check for one-grams (example: python)\n",
    "    for bonus_piece in bonus_points_list:\n",
    "        if bonus_piece.lower() in tokens:\n",
    "            bonus_points[bonus_piece] = 'Yes'\n",
    "        else:\n",
    "            bonus_points[bonus_piece] = 'No'\n",
    "\n",
    "    return bonus_points\n",
    "  \n",
    "print ('Bonus:' , bonus(text))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: ['Cristian Fontana']\n",
      "Predicted Profile: Data Science\n",
      "Full sentence:  {\"Experience\\nEDUCATION\\n2014-2020\\nBachelor's degree in Economics.\"} \n",
      "Degree:  Bachelors \n",
      "Subject:  economic \n",
      "GPA:  [] \n",
      "Years:  ['2014', '2020']\n",
      "Mail id:  ['c.fontana95@gmail.com']\n",
      "Mobile Number:  3934444659\n",
      "General Skills: []\n",
      "Programming Skills: ['Slack', 'Machine learning', 'Python', 'Docker', 'Git', 'Mongodb', 'Mysql', 'Jira', 'Sql', 'Postgresql', 'Agile methodologies', 'Spark']\n",
      "Languages: {'Spanish': 'Native', 'English': 'Advanced', 'Portuguese': 'Intermediate'}\n",
      "Bonus: {'Projects': 'Yes', 'Achievements': 'No', 'Hobbies': 'No'}\n"
     ]
    }
   ],
   "source": [
    "org_name(\"C:/Users/cfont/Downloads/Cristian Fontana - CV-Resume.pdf\")\n",
    "print('Predicted Profile:', predict_profile.predict(new_inputs(pd.Series(text)))[0])\n",
    "extract_education(text)\n",
    "print('Mail id: ',extract_email_addresses(text))\n",
    "print('Mobile Number: ',extract_mobile_number(text))\n",
    "print ('General Skills:',general_skills(text))\n",
    "print ('Programming Skills:',programming_skills(text))\n",
    "print ('Languages:' ,language_skill(text))\n",
    "print ('Bonus:' , bonus(text))\n",
    "\n",
    "# Add github.\n",
    "# add linkedin."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "import re\n",
    "from pdfminer.high_level import extract_text\n",
    "\n",
    "import subprocess  # noqa: S404"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_file_path = '../src/data/example.pdf'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Text from PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CRISTIAN FONTANA\n",
      "Data Scientist & Business Inteligence\n",
      "Bachelor's degree in Economics with specialization in Finance. UBA, Argentina.\n",
      "\n",
      "Languages: Spanish, English, Portuguese, Italian - Full Professional Proficiency.\n",
      "\n",
      "       c.fontana95@gmail.com                                                              www.linkedin.com/in/cristianfontana/\n",
      "\n",
      "S K I L L S\n",
      "PYTHON\n",
      "Numpy,  Pandas,  Scikit  Learn,  PyTorch,  XGBoost,\n",
      "LightGBM,  SciPy,  Matplotlib,  Keras,  Pipenv,\n",
      "Seaborn,\n",
      "Tensorflow, \n",
      "SQLAlchemy, \n",
      "Dash,\n",
      "Jupyternotebook.\n",
      "\n",
      "Statsmodels, \n",
      "Boto3, \n",
      "\n",
      "Plotly, \n",
      "Bokeh, \n",
      "\n",
      "A B O U T   M E\n",
      "\n",
      "Strong  background  in  Data  Science  and  Business  Intelligence  with  more  than  5  years  of\n",
      "experience,  building  quantitative  solutions,  doing  forecasts,  regression,  time  series,  machine\n",
      "learning, and creating dashboards. \n",
      "Advanced  understanding  of  statistical,  algebraic  and  other  analytical  techniques.  Highly\n",
      "\n",
      "organized, motivated and diligent. I enjoy finding problems and working on projects to resolve\n",
      "them holistically.\n",
      "\n",
      "CLOUD & DATABASES\n",
      "AWS,  GCP,  SQL,  No-SQL,  Lambda,  Sagemaker,  S3,\n",
      "DynamoDB,\n",
      "Athena,  MongoDB, \n",
      "Serverless,  Deployment,\n",
      "Storage, \n",
      "Containers, \n",
      "\n",
      "RedShift, \n",
      "\n",
      "W O R K   E X P E R I E N C E\n",
      "D A T A   S C I E N T I S T   @   L E A N T K\n",
      "01/2023 - 05/2023 (5 MONTHS)\n",
      "\n",
      "PostgreSQL,  MySQL,  SQL,  SQL  Server,  MySQL,\n",
      "\n",
      "Saved  more  than  20.000  USD  annually  to  small  startups  building  a  financial  app  with  AI\n",
      "\n",
      "Docker, Git, Github, Bash.\n",
      "\n",
      "that take inputs from multiple models and make decisions in real-time. \n",
      "\n",
      "VISUALIZATION\n",
      "Power BI, Tableau, Looker, Mode, Dash, Plotly.\n",
      "\n",
      "OTHERS\n",
      "Agile methodologies, CRISP, Notion, Jira, Trello,\n",
      "\n",
      "Excel, Word, Powerpoint, VBA, Macros, Figma,\n",
      "\n",
      "Carta, Slack, Clickup, Pendo, and Productboard.\n",
      "\n",
      "C E R T I F I C A T E S\n",
      "\n",
      "Data Analyst - DataCamp.\n",
      "\n",
      "Data Scientist - DataCamp.\n",
      "Google Data Analytics Specialization - Coursera.\n",
      "\n",
      "Power BI - Crehana.\n",
      "\n",
      "IBM Data Science - Specialization - Coursera.\n",
      "Macroeconomic Diagnostics - FMI - Edx.\n",
      "Financial Market Analysis - FMI - Edx.\n",
      "\n",
      "Essentials of Corporate Finance - Specialization -\n",
      "University of Melbourne.\n",
      "\n",
      "P R O J E C T S\n",
      "\n",
      "Build  an  application  that  from  simple  inputs,\n",
      "gives back the statements for a startup.\n",
      "\n",
      "Used  machine  learning  to  output  the  principal\n",
      "sells  and  bankruptcy\n",
      "metrics, \n",
      "\n",
      "forecasted \n",
      "\n",
      "probability.\n",
      "\n",
      "Orchestrate  the  architecture  for  an  application\n",
      "\n",
      "deployed  serverless  with  SQL  and  No-SQL\n",
      "\n",
      "databases, in GCP and AWS.\n",
      "\n",
      "Develop input and assumptions based preexisting models to estimate the costs and savings\n",
      "\n",
      "opportunities associated with varying levels of network growth and operations.\n",
      "\n",
      "Toolbox: AWS - GCP, Python, Git.\n",
      "\n",
      "S S R .   D A T A   S C I E N T I S T   @   M U N D I\n",
      "06/2022 - 12/2022 (6 MONTHS)\n",
      "\n",
      "Reduced the churn rate by 15% in 3 months by finding the importance of the two first days\n",
      "\n",
      "of operating of the clients.\n",
      "\n",
      "In  charge  of  the  dashboards  and  analytic  presentations  about  the  status  of  the  company,\n",
      "\n",
      "clients and commercial status.\n",
      "\n",
      "Toolbox: Mode, Snowflake, Python and Git.\n",
      "\n",
      "D A T A   S C I E N T I S T / A N A L Y T I C S   -   F R E E L A N C E\n",
      "11/2021 - 05/2022 (6 MONTHS)\n",
      "\n",
      "Saved  20  to  40  hours  monthly  to  small  businesses  with  the  deployment  of  different\n",
      "\n",
      "dashboards to follow up KPIs. \n",
      "\n",
      "Finding valuable insights, and trends from the data.\n",
      "\n",
      "Helping taking decisions based on the data.\n",
      "\n",
      "Toolbox: Python, SQL, Excel, Macros VBA, Tableau, Power BI, and others.\n",
      "\n",
      "D A T A   S C I E N T I S T   @   P W C\n",
      "01/2021 - 11/2021 (10 MONTHS)\n",
      "\n",
      "Development  of  analytics  and  machine  learning-based  solutions  for  distinct  initiatives.\n",
      "\n",
      "Handling,  forecasting  and  visualizing  data  for  macroeconomic,  microeconomic  and  other\n",
      "\n",
      "financial projects. \n",
      "\n",
      "Recognize internal and external stakeholder requirements. \n",
      "\n",
      "Toolbox: Python, SQL, Excel, Macros VBA, Tableau, Power BI, and others.\n",
      "\n",
      "S S R .   D A T A   A N A L Y T I C S   @   F O B E N  \n",
      "09/2017 - 12/2020 (4 YEARS)\n",
      "\n",
      "Reduced  cost  by  20%  by  implementing  a  forecast  of  the  needed  inventory  and  avoiding\n",
      "\n",
      "unnecessary costs.\n",
      "Work with stakeholders in order to answer key business questions.\n",
      "Creating dashboards using Tableau and Power BI to present data insights.\n",
      "\n",
      "Toolbox: Excel, PowerBI, Tableau, SQL and Python.\n",
      "\n",
      "\f\n"
     ]
    }
   ],
   "source": [
    "def extract_text_from_pdf(pdf_path):\n",
    "    return extract_text(pdf_path)\n",
    "  \n",
    "if __name__ == '__main__':\n",
    "    print(extract_text_from_pdf('../src/data/example.pdf'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       c.fontana95@gmail.com                                                              www.linkedin.com/in/cristianfontana/\n",
      "Saved more than 20.000 USD annually to small startups building a financial app with AI\n",
      "that take inputs from multiple models and make decisions in real-time. \n",
      "Develop input and assumptions based preexisting models to estimate the costs and savings\n",
      "opportunities associated with varying levels of network growth and operations.\n",
      "Toolbox: AWS - GCP, Python, Git.\n",
      "Reduced the churn rate by 15% in 3 months by finding the importance of the two first days\n",
      "of operating of the clients.\n",
      "In charge of the dashboards and analytic presentations about the status of the company,\n",
      "clients and commercial status.\n",
      "Toolbox: Mode, Snowflake, Python and Git.\n",
      "Saved 20 to 40 hours monthly to small businesses with the deployment of different\n",
      "dashboards to follow up KPIs. \n",
      "Finding valuable insights, and trends from the data.\n",
      "Helping taking decisions based on the data.\n",
      "Toolbox: Python, SQL, Excel, Macros VBA, Tableau, Power BI, and others.\n",
      "Development of analytics and machine learning-based solutions for distinct initiatives.\n",
      "Handling, forecasting and visualizing data for macroeconomic, microeconomic and other\n",
      "financial projects. \n",
      "Recognize internal and external stakeholder requirements. \n",
      "Toolbox: Python, SQL, Excel, Macros VBA, Tableau, Power BI, and others.\n",
      "Reduced cost by 20% by implementing a forecast of the needed inventory and avoiding\n",
      "unnecessary costs.\n",
      "Work with stakeholders in order to answer key business questions.\n",
      "Creating dashboards using Tableau and Power BI to present data insights.\n",
      "Toolbox: Excel, PowerBI, Tableau, SQL and Python.ABOUT ME\n",
      "Strong background in Data Science and Business Intelligence with more than 5 years of\n",
      "experience, building quantitative solutions, doing forecasts, regression, time series, machine\n",
      "learning, and creating dashboards. \n",
      "Advanced understanding of statistical, algebraic and other analytical techniques. Highly\n",
      "organized, motivated and diligent. I enjoy finding problems and working on projects to resolve\n",
      "them holistically.\n",
      "WORK EXPERIENCE\n",
      "DATA SCIENTIST @ LEANTK\n",
      "01/2023 - 05/2023 (5 MONTHS)\n",
      "SSR. DATA SCIENTIST @ MUNDI\n",
      "06/2022 - 12/2022 (6 MONTHS)\n",
      "DATA SCIENTIST/ANALYTICS - FREELANCE\n",
      "11/2021 - 05/2022 (6 MONTHS)\n",
      "DATA SCIENTIST @ PWC\n",
      "01/2021 - 11/2021 (10 MONTHS)\n",
      "SSR. DATA ANALYTICS @ FOBEN \n",
      "09/2017 - 12/2020 (4 YEARS)Data Analyst - DataCamp.\n",
      "Data Scientist - DataCamp.\n",
      "Google Data Analytics Specialization - Coursera.\n",
      "Power BI - Crehana.\n",
      "IBM Data Science - Specialization - Coursera.\n",
      "Macroeconomic Diagnostics - FMI - Edx.\n",
      "Financial Market Analysis - FMI - Edx.\n",
      "Essentials of Corporate Finance - Specialization -\n",
      "University of Melbourne.CERTIFICATESCRISTIAN FONTANA\n",
      "Data Scientist & Business Inteligence\n",
      "Bachelor's degree in Economics with specialization in Finance. UBA, Argentina.\n",
      "Languages: Spanish, English, Portuguese, Italian - Full Professional Proficiency.\n",
      "SKILLS\n",
      "PYTHON\n",
      "Numpy, Pandas, Scikit Learn, PyTorch, XGBoost,\n",
      "LightGBM, SciPy, Matplotlib, Keras, Pipenv,\n",
      "Tensorflow, Statsmodels, Plotly, Seaborn,\n",
      "SQLAlchemy, Boto3, Bokeh, Dash,\n",
      "Jupyternotebook.\n",
      "CLOUD & DATABASES\n",
      "AWS, GCP, SQL, No-SQL, Lambda, Sagemaker, S3,\n",
      "Athena, MongoDB, RedShift, DynamoDB,\n",
      "Containers, Storage, Serverless, Deployment,\n",
      "PostgreSQL, MySQL, SQL, SQL Server, MySQL,\n",
      "Docker, Git, Github, Bash.\n",
      "VISUALIZATION\n",
      "Power BI, Tableau, Looker, Mode, Dash, Plotly.\n",
      "OTHERS\n",
      "Agile methodologies, CRISP, Notion, Jira, Trello,\n",
      "Excel, Word, Powerpoint, VBA, Macros, Figma,\n",
      "Carta, Slack, Clickup, Pendo, and Productboard.\n",
      "Build an application that from simple inputs,\n",
      "gives back the statements for a startup.\n",
      "Used machine learning to output the principal\n",
      "metrics, forecasted sells and bankruptcy\n",
      "probability.\n",
      "Orchestrate the architecture for an application\n",
      "deployed serverless with SQL and No-SQL\n",
      "databases, in GCP and AWS.PROJECTS\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# We can see that this fuction couldn't extract the top of the pdf.\n",
    "\n",
    "def extract_text_from_pdf(file_path):\n",
    "    text = \"\"\n",
    "    with open(file_path, \"rb\") as file:\n",
    "        pdf_reader = PyPDF2.PdfReader(file)\n",
    "        for page_number in range(len(pdf_reader.pages)):\n",
    "            page = pdf_reader.pages[page_number]\n",
    "            text += page.extract_text()\n",
    "    return text\n",
    "\n",
    "# Usage example\n",
    "extracted_text = extract_text_from_pdf(pdf_file_path)\n",
    "print(extracted_text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting Phone Numbers from resume\n",
    "\n",
    "Two ways of extracting phone number, anyways there is not phone number on the resume."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "def extract_phone_numbers_from_pdf(file_path):\n",
    "    phone_numbers = []\n",
    "    with open(file_path, \"rb\") as file:\n",
    "        pdf_reader = PyPDF2.PdfReader(file)\n",
    "        for page_number in range(len(pdf_reader.pages)):\n",
    "            page = pdf_reader.pages[page_number]\n",
    "            text = page.extract_text()\n",
    "            # Use regular expression to find phone numbers\n",
    "            numbers = re.findall(r\"\\b\\d{3}[-.]?\\d{3}[-.]?\\d{4}\\b\", text)\n",
    "            phone_numbers.extend(numbers)\n",
    "    return phone_numbers\n",
    "\n",
    "# Usage example\n",
    "phone_numbers = extract_phone_numbers_from_pdf(pdf_file_path)\n",
    "print(phone_numbers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "PHONE_REG = re.compile(r'[\\+\\(]?[1-9][0-9 .\\-\\(\\)]{8,}[0-9]') \n",
    " \n",
    "def doc_to_text_catdoc(file_path):\n",
    "    try:\n",
    "        process = subprocess.Popen(  # noqa: S607,S603\n",
    "            ['catdoc', '-w', file_path],\n",
    "            stdout=subprocess.PIPE,\n",
    "            stderr=subprocess.PIPE,\n",
    "            universal_newlines=True,\n",
    "        )\n",
    "    except (\n",
    "        FileNotFoundError,\n",
    "        ValueError,\n",
    "        subprocess.TimeoutExpired,\n",
    "        subprocess.SubprocessError,\n",
    "    ) as err:\n",
    "        return (None, str(err))\n",
    "    else:\n",
    "        stdout, stderr = process.communicate()\n",
    " \n",
    "    return (stdout.strip(), stderr.strip())\n",
    " \n",
    " \n",
    "def extract_phone_number(resume_text):\n",
    "    phone = re.findall(PHONE_REG, resume_text)\n",
    " \n",
    "    if phone:\n",
    "        number = ''.join(phone[0])\n",
    " \n",
    "        if resume_text.find(number) >= 0 and len(number) < 18:\n",
    "            return number\n",
    "    return None\n",
    " \n",
    " \n",
    "if __name__ == '__main__':\n",
    "    text = extract_text_from_pdf('../src/data/cristian_fontana.pdf')\n",
    "    phone_number = extract_phone_number(text)\n",
    " \n",
    "    print(phone_number)  # noqa: T001"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract email from PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Names: ['Data Science', 'Business Intelligence', 'Data Analyst', 'Data Scientist', 'Google Data', 'Analytics Specialization', 'Data Science', 'Macroeconomic Diagnostics', 'Financial Market', 'Corporate Finance', 'Data Scientist', 'Business Inteligence', 'Full Professional', 'Scikit Learn']\n",
      "Emails: ['c.fontana95@gmail.com']\n"
     ]
    }
   ],
   "source": [
    "def extract_name_and_email_from_pdf(file_path):\n",
    "    emails = []\n",
    "    with open(file_path, \"rb\") as file:\n",
    "        pdf_reader = PyPDF2.PdfReader(file)\n",
    "        for page_number in range(len(pdf_reader.pages)):\n",
    "            page = pdf_reader.pages[page_number]\n",
    "            text = page.extract_text()\n",
    "\n",
    "            # Extract emails using regex\n",
    "            email_regex = r\"\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b\"\n",
    "            emails += re.findall(email_regex, text)\n",
    "    return emails\n",
    "\n",
    "# Usage example\n",
    "extracted_emails = extract_name_and_email_from_pdf(pdf_file_path)\n",
    "print(\"Emails:\", extracted_emails)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find Skills or any special word in resume."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found Words:\n",
      "python\n",
      "machine learning\n"
     ]
    }
   ],
   "source": [
    "def extract_text_from_pdf(file_path):\n",
    "    with open(file_path, \"rb\") as file:\n",
    "        pdf_reader = PyPDF2.PdfReader(file)\n",
    "        text = \"\"\n",
    "        for page_number in range(len(pdf_reader.pages)):\n",
    "            page = pdf_reader.pages[page_number]\n",
    "            text += page.extract_text()\n",
    "        return text\n",
    "\n",
    "def check_word_presence(text, word_list):\n",
    "    found_words = []\n",
    "    for word in word_list:\n",
    "        if word in text:\n",
    "            found_words.append(word)\n",
    "    return found_words\n",
    "\n",
    "def clean_text(text):\n",
    "    # Remove symbols and special characters\n",
    "    text = ''.join(e for e in text if e.isalnum() or e.isspace())\n",
    "    \n",
    "    # Remove leading/trailing whitespace and extra spaces\n",
    "    text = ' '.join(text.split())\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Usage example\n",
    "word_list = [\"python\", \"machine learning\", \"data analysis\"]\n",
    "\n",
    "# Extract text from PDF\n",
    "text = extract_text_from_pdf(pdf_file_path)\n",
    "\n",
    "# Clean the text\n",
    "text = clean_text(text)\n",
    "\n",
    "# Check word presence\n",
    "found_words = check_word_presence(text, word_list)\n",
    "\n",
    "print(\"Found Words:\")\n",
    "for word in found_words:\n",
    "    print(word)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract education and school"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted Schools:\n",
      "Extracted education:\n",
      "degree\n"
     ]
    }
   ],
   "source": [
    "def extract_education_from_pdf(file_path):\n",
    "    education = []\n",
    "    with open(file_path, \"rb\") as file:\n",
    "        pdf_reader = PyPDF2.PdfReader (file)\n",
    "        for page_number in range(len(pdf_reader.pages)):\n",
    "            page = pdf_reader.pages[page_number]\n",
    "            text = page.extract_text()\n",
    "\n",
    "            # Extract education using regular expressions\n",
    "            pattern = r\"(?i)((bachelor|degree|education|qualification)[\\s:]+(.+))\"\n",
    "            matches = re.findall(pattern, text)\n",
    "            for match in matches:\n",
    "                education.append(match[1])\n",
    "    \n",
    "    return education\n",
    "\n",
    "def extract_schools_from_education(education):\n",
    "    schools = []\n",
    "    for edu in education:\n",
    "        # Extract schools using regular expressions\n",
    "        pattern = r\"(?i)((school|university|universidad|college)[:\\s]+(.+))\"\n",
    "        matches = re.findall(pattern, edu)\n",
    "        for match in matches:\n",
    "            schools.append(match[1])\n",
    "    \n",
    "    return schools\n",
    "\n",
    "# Usage example\n",
    "pdf_file_path = '../src/data/cristian_fontana.pdf'\n",
    "\n",
    "# Extract education from PDF\n",
    "extracted_education = extract_education_from_pdf(pdf_file_path)\n",
    "\n",
    "# Extract schools from education\n",
    "extracted_schools = extract_schools_from_education(extracted_education)\n",
    "\n",
    "print(\"Extracted Schools:\")\n",
    "for school in extracted_schools:\n",
    "    print(school)\n",
    "\n",
    "print(\"Extracted education:\")\n",
    "for education in extracted_education:\n",
    "    print(education)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import docx2txt\n",
    "from PyPDF2 import PdfReader, PdfFileWriter, PdfFileMerger\n",
    "\n",
    "import spacy\n",
    "import en_core_web_sm\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\cfont\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<spacy.lang.en.English at 0x1f3f80c8c50>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "spacy.load('en_core_web_sm')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extracting text from DOCX\n",
    "def doctotext(m):\n",
    "    temp = docx2txt.process(m)\n",
    "    resume_text = [line.replace('\\t', ' ') for line in temp.split('\\n') if line]\n",
    "    text = ' '.join(resume_text)\n",
    "    return (text)\n",
    "    \n",
    "#Extracting text from PDF\n",
    "def pdftotext(m):\n",
    "    # pdf file object\n",
    "    # you can find find the pdf file with complete code in below\n",
    "    pdfFileObj = open(m, 'rb')\n",
    "\n",
    "    # pdf reader object\n",
    "    pdfFileReader = PdfReader(pdfFileObj)\n",
    "\n",
    "    # number of pages in pdf\n",
    "    num_pages = len(pdfFileReader.pages)\n",
    "\n",
    "    currentPageNumber = 0\n",
    "    text = ''\n",
    "\n",
    "    # Loop in all the pdf pages.\n",
    "    while(currentPageNumber < num_pages ):\n",
    "\n",
    "        # Get the specified pdf page object.\n",
    "        pdfPage = pdfFileReader.pages[currentPageNumber]\n",
    "\n",
    "        # Get pdf page text.\n",
    "        text = text + pdfPage.extract_text()\n",
    "\n",
    "        # Process next page.\n",
    "        currentPageNumber += 1\n",
    "    return (text)\n",
    "\n",
    "# #main function\n",
    "# if __name__ == '__main__': \n",
    "\n",
    "#     FilePath = 'AI.pdf'\n",
    "#     FilePath.lower().endswith(('.png', '.docx'))\n",
    "#     if FilePath.endswith('.docx'):\n",
    "#       textinput = doctotext(FilePath) \n",
    "#     elif FilePath.endswith('.pdf'):\n",
    "#       textinput = pdftotext(FilePath)\n",
    "#     else:\n",
    "#       print(\"File not support\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Saved more than 20.000 USD annually to small startups building a financial app with AI that take inputs\\nfrom multiple models and make decisions in real time. \\nDevelop input and assumptions based on preexisting models to estimate the costs and savings\\nopportunities associated with varying levels of network growth and operations.\\nToolbox: AWS - GCP, Python, Git, Numpy, Pandas, Sickit Learn, Matplotlib, Pipenv, Statsmodels, Plotly,\\nSQL, No-SQL, Boto3, Jupyternotebook, Dash, Lambda, S3, DynamoDB, Serverless, Deployment,\\nDocker, Notion, Figma, Slack, Github, Spark, RedShift, Athena, Azure, Scipy, PostgreSQL.\\nReduced the churn rate by 15% in 3 months by finding the importance of the clients' two first days of\\noperating.\\nIn charge of the dashboards and analytic presentations about the status of the company, clients and\\ncommercial status.\\nToolbox: Mode, Snowflake, Python, Git, Excel, Jira, Github, SQLAlchemy, Jupyternotebook, Numpy,\\nPandas, Scikit Learn, XGBoost, Scipy, Maplotlib, Pipenv, Seaborn, PostgreSQL, dbt, Carta, Slack, Looker,\\nPower BI, Docker, Mode, Agile methodologies, Scipy, PostgreSQL.\\nSaved 20 to 40 hours monthly to small businesses with the deployment of different dashboards to\\nfollow up KPIs. \\nFinding valuable insights, and trends from the data.\\nDeployed a recommendation system that recommends other menu items based on past order history,\\nincreasing average spend by 10%.\\nToolbox: Python, SQL, Excel, Macros VBA, Tableau, Power BI, Pandas, Numpy, Keras, Statsmodel,\\nTensorflow, Looker, Sagemarker, AutoML, Azure, dbt, Slack, PostgreSQL.\\nDevelopment of analytics and machine learning-based solutions for distinct initiatives. Handling,\\nforecasting and visualizing data for macroeconomic, microeconomic and other financial projects. \\nRecognize internal and external stakeholder requirements. \\nToolbox: Python, SQL, Excel, Macros VBA, Tableau, Power BI, Keras, Statsmodel, Tensorflow, GitHub,\\ndbt, Trello, Slack, Powerpoint,  PyTorch, LightGBM, XGBoost, Scipy, PostgreSQL.\\nReduced cost by 20% by implementing a forecast of the needed inventory and avoiding unnecessary\\ncosts.\\nBuilt sales forecast using parameters, trend lines and reference lines, saving more than 30 monthly\\nhours of manual work.\\nImplemented a long-term pricing system that improve the customer lifetime value by 20%.\\nBuild out the data and reporting infrastructure from the ground up using Tableau and SQL to provide\\nreal-time insights into the product, marketing funnels and business KPIs.\\nDeveloped and maintained reports with more than 95% on-time delivery.\\nToolbox: Excel, PowerBI, Tableau, SQL, Python, R, Powerpoint, Pandas, Numpy, PostgreSQL, MySQL.DATA SCIENTIST\\nLEANTK (CONTRACT)\\n01/2023 - 06/2023 (6 MONTHS)\\nDATA SCIENTIST\\nMUNDI (CONTRACT)\\n06/2022 - 12/2022 (6 MONTHS)\\nDATA SCIENTIST/ANALYTICS\\nFREELANCE\\n11/2021 - 05/2022 (6 MONTHS)\\nDATA SCIENTIST\\nPWC\\n01/2021 - 11/2021 (10 MONTHS)\\nDATA ANALYTICS\\nFOBEN \\n09/2017 - 12/2020 (4 YEARS)Data Analyst - DataCamp.\\nData Scientist - DataCamp.\\nGoogle Data Analytics Specialization - Coursera.\\nPower BI - Crehana.\\nIBM Data Science - Specialization - Coursera.\\nMacroeconomic Diagnostics - FMI - Edx.\\nFinancial Market Analysis - FMI - Edx.\\nEssentials of Corporate Finance - Specialization\\n- University of Melbourne.CERTIFICATESSKILLS\\nPYTHON\\nNumpy, Pandas, Scikit Learn, PyTorch, XGBoost,\\nLightGBM, SciPy, Matplotlib, Keras, Pipenv,\\nTensorflow, Statsmodels, Plotly, Seaborn,\\nSQLAlchemy, Boto3, Bokeh, Dash,\\nJupyternotebook, PySpark.\\nCLOUD & DATABASES\\nAWS, GCP, Azure, SQL, No-SQL, Lambda,\\nSagemaker, S3, Athena, MongoDB, RedShift,\\nDynamoDB, Containers, Storage, Serverless,\\nDeployment, PostgreSQL, SQL, SQL Server, MySQL,\\nDocker, Git, Github, Bash, AutoML.\\nVISUALIZATION\\nPower BI, Tableau, Looker, Mode, Dash, Plotly.\\nOTHERS\\nAgile methodologies, CRISP, Notion, Jira, Trello,\\nExcel, dbt, R, Powerpoint, VBA, Macros, Figma,\\nCarta, Slack, Clickup, Pendo, and Productboard.\\nLANGUAGES\\nSpanish (Native)\\nEnglish (Advanced).\\nPortuguese (Intermediate).Cristian Fontana\\nData Scientist - Italian Citizen\\nEconomist with a strong background in Data Science and Business Intelligence with more than 5 years of\\nexperience, building quantitative solutions, doing forecasts, regression, time series, machine learning, and\\ncreating dashboards. \\nAdvanced understanding of statistical, algebraic and other analytical techniques. Highly organized,\\nmotivated and diligent. I enjoy finding problems and working on projects to resolve them holistically.\\nExperience\\nEDUCATION\\n2014-2020\\nBachelor's degree in Economics.\\nUniversidad de Buenos Aires, Argentina.CONTACT\\n+393444465965\\nc.fontana95@gmail.com\\nhttps://www.linkedin.com/in/cristianfontana/\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdftotext(\"C:/Users/cfont/Downloads/Cristian Fontana - CV-Resume.pdf\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = pdftotext(\"C:/Users/cfont/Downloads/Cristian Fontana - CV-Resume.pdf\")\n",
    "\n",
    "# load pre-trained model\n",
    "nlp = en_core_web_sm.load()\n",
    "\n",
    "# initialize matcher with a vocab\n",
    "matcher = Matcher(nlp.vocab)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name:  Sickit Learn\n"
     ]
    }
   ],
   "source": [
    "def extract_name(resume_text):\n",
    "    nlp_text = nlp(resume_text)\n",
    "    \n",
    "    # First name and Last name are always Proper Nouns\n",
    "    pattern = [{'POS': 'PROPN'}, {'POS': 'PROPN'}]\n",
    "    \n",
    "    matcher.add('NAME', [pattern])\n",
    "    \n",
    "    matches = matcher(nlp_text)\n",
    "    \n",
    "    for match_id, start, end in matches:\n",
    "        span = nlp_text[start:end]\n",
    "        return span.text\n",
    "print('Name: ',extract_name(pdftotext(\"C:/Users/cfont/Downloads/Cristian Fontana - CV-Resume.pdf\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Qualification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qualification:  ['Bachelors']\n"
     ]
    }
   ],
   "source": [
    "# Grad all general stop words\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "\n",
    "# Education Degrees\n",
    "EDUCATION = [\n",
    "            'BE','B.E.', 'B.E', 'BS', 'B.S', \n",
    "            'ME', 'M.E', 'M.E.', 'M.B.A', 'MBA', 'MS', 'M.S', \n",
    "            'BTECH', 'B.TECH', 'M.TECH', 'MTECH', \n",
    "            'SSLC', 'SSC' 'HSC', 'CBSE', 'ICSE', 'X', 'XII',\n",
    "            'BACHELOR', 'MASTER', 'PHD', 'BACHELORS', 'MASTERS', 'Ph.D.',\n",
    "            'Licenciatura', 'Ingeniería', 'Maestría', 'Maestria',\n",
    "            'Maestra', 'Maestro', 'Doctorado', 'Doctora', 'Doctor', 'Licenciado', 'Licenciada',\n",
    "            'Ingeniero', 'Ingeniera', 'Maestrante', 'Doctorante', 'L'\n",
    "            'Lic', 'Ing'\n",
    "        ]\n",
    "\n",
    "def extract_education(resume_text):\n",
    "    nlp_text = nlp(resume_text)\n",
    "\n",
    "    # Sentence Tokenizer\n",
    "    nlp_text = [sent.text.strip() for sent in nlp_text.sents]\n",
    "\n",
    "    edu = {}\n",
    "    # Extract education degree\n",
    "    for index, text in enumerate(nlp_text):\n",
    "        for tex in text.split():\n",
    "            # Replace all special symbols\n",
    "            tex = re.sub(r'[?|$|.|!|,\\']', r'', tex)\n",
    "            if tex.upper() in EDUCATION and tex not in STOPWORDS:\n",
    "                edu[tex] = text + nlp_text[index + 1]\n",
    "                \n",
    "                \n",
    "\n",
    "    # Extract year\n",
    "    education = []\n",
    "    for key in edu.keys():\n",
    "        year = re.search(re.compile(r'(((20|19)(\\d{})))'), edu[key])\n",
    "        if year:\n",
    "            education.append((key, ''.join(year[0])))\n",
    "        else:\n",
    "            education.append(key)\n",
    "    return education\n",
    "print('Qualification: ', extract_education(text))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Email"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mail id:  ['c.fontana95@gmail.com']\n"
     ]
    }
   ],
   "source": [
    "def extract_email_addresses(string):\n",
    "    r = re.compile(r'[\\w\\.-]+@[\\w\\.-]+')\n",
    "    return r.findall(string)\n",
    "print('Mail id: ',extract_email_addresses(text))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phone Number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mobile Number:  3934444659\n"
     ]
    }
   ],
   "source": [
    "def extract_mobile_number(resume_text):\n",
    "    phone = re.findall(re.compile(r'(?:(?:\\+?([1-9]|[0-9][0-9]|[0-9][0-9][0-9])\\s*(?:[.-]\\s*)?)?(?:\\(\\s*([2-9]1[02-9]|[2-9][02-8]1|[2-9][02-8][02-9])\\s*\\)|([0-9][1-9]|[0-9]1[02-9]|[2-9][02-8]1|[2-9][02-8][02-9]))\\s*(?:[.-]\\s*)?)?([2-9]1[02-9]|[2-9][02-9]1|[2-9][02-9]{2})\\s*(?:[.-]\\s*)?([0-9]{4})(?:\\s*(?:#|x\\.?|ext\\.?|extension)\\s*(\\d+))?'), resume_text)\n",
    "    \n",
    "    if phone:\n",
    "        number = ''.join(phone[0])\n",
    "        if len(number) > 10:\n",
    "            return number\n",
    "        else:\n",
    "            return number\n",
    "print('Mobile Number: ',extract_mobile_number(text))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Skills"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "general_skills_list = [\n",
    "    \"Communication Skills\",\n",
    "    \"Verbal Communication\",\n",
    "    \"Written Communication\",\n",
    "    \"Presentation Skills\",\n",
    "    \"Public Speaking\",\n",
    "    \"Interpersonal Skills\",\n",
    "    \"Negotiation Skills\",\n",
    "    \"Listening Skills\",\n",
    "    \"Persuasion Skills\",\n",
    "    \"Teamwork\",\n",
    "    \"Collaboration\",\n",
    "    \"Leadership\",\n",
    "    \"Conflict Resolution\",\n",
    "    \"Relationship Building\",\n",
    "    \"Networking\",\n",
    "    \"Analytical Skills\",\n",
    "    \"Critical Thinking\",\n",
    "    \"Problem Solving\",\n",
    "    \"Research Skills\",\n",
    "    \"Data Analysis\",\n",
    "    \"Quantitative Analysis\",\n",
    "    \"Qualitative Analysis\",\n",
    "    \"Decision Making\",\n",
    "    \"Attention to Detail\",\n",
    "    \"Logical Reasoning\",\n",
    "    \"Technical Skills\",\n",
    "    \"Computer Literacy\",\n",
    "    \"Programming Languages\",\n",
    "    \"Software Proficiency\",\n",
    "    \"Web Development\",\n",
    "    \"Database Management\",\n",
    "    \"Information Technology\",\n",
    "    \"Troubleshooting\",\n",
    "    \"Systems Administration\",\n",
    "    \"Network Security\",\n",
    "    \"Creativity\",\n",
    "    \"Innovation\",\n",
    "    \"Graphic Design\",\n",
    "    \"Artistic Skills\",\n",
    "    \"Photography\",\n",
    "    \"Video Editing\",\n",
    "    \"Content Creation\",\n",
    "    \"Writing Skills\",\n",
    "    \"Copywriting\",\n",
    "    \"Proofreading and Editing\",\n",
    "    \"Organization\",\n",
    "    \"Time Management\",\n",
    "    \"Project Management\",\n",
    "    \"Planning and Coordination\",\n",
    "    \"Multitasking\",\n",
    "    \"Prioritization\",\n",
    "    \"Detail Orientation\",\n",
    "    \"Meeting Deadlines\",\n",
    "    \"Resource Management\",\n",
    "    \"Customer Service\",\n",
    "    \"Client Management\",\n",
    "    \"Relationship Management\",\n",
    "    \"Conflict Resolution (customer-facing)\",\n",
    "    \"Sales Skills\",\n",
    "    \"Account Management\",\n",
    "    \"Marketing Skills\",\n",
    "    \"Market Research\",\n",
    "    \"Advertising\",\n",
    "    \"Social Media Marketing\",\n",
    "    \"Search Engine Optimization (SEO)\",\n",
    "    \"Language Skills\",\n",
    "    \"Bilingualism\",\n",
    "    \"Translation\",\n",
    "    \"Interpretation\",\n",
    "    \"Financial Skills\",\n",
    "    \"Accounting\",\n",
    "    \"Financial Analysis\",\n",
    "    \"Budgeting\",\n",
    "    \"Financial Planning\",\n",
    "    \"Risk Management\",\n",
    "    \"Teaching and Training\",\n",
    "    \"Instructional Design\",\n",
    "    \"Curriculum Development\",\n",
    "    \"Tutoring\",\n",
    "    \"Mentoring\",\n",
    "    \"Project Coordination\",\n",
    "    \"Event Planning\",\n",
    "    \"Event Management\",\n",
    "    \"Logistics\",\n",
    "    \"Supply Chain Management\",\n",
    "    \"Research and Development\",\n",
    "    \"Scientific Methodology\",\n",
    "    \"Lab Techniques\",\n",
    "    \"Experimental Design\",\n",
    "    \"Statistical Analysis\",\n",
    "    \"Problem Diagnosis\",\n",
    "    \"Troubleshooting (Technical)\",\n",
    "    \"Maintenance and Repair\",\n",
    "    \"Equipment Handling\",\n",
    "    \"Mechanical Skills\",\n",
    "    \"Health and Safety\",\n",
    "    \"First Aid\",\n",
    "    \"CPR\",\n",
    "    \"Occupational Health and Safety\",\n",
    "    \"Risk Assessment\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert general skills to lowercase.\n",
    "general_skills_list = [skill.lower() for skill in general_skills_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "General Skills []\n"
     ]
    }
   ],
   "source": [
    "def general_skills(resume_text):\n",
    "    nlp_text = nlp(resume_text)\n",
    "\n",
    "    noun_chunks = nlp_text.noun_chunks\n",
    "\n",
    "    # removing stop words and implementing word tokenization\n",
    "    tokens = [token.text for token in nlp_text if not token.is_stop]\n",
    "    \n",
    "    # extract values\n",
    "    skillset = []\n",
    "    \n",
    "    # check for one-grams (example: python)\n",
    "    for token in tokens:\n",
    "        if token.lower() in general_skills_list:\n",
    "            skillset.append(token)\n",
    "   \n",
    "    for token in noun_chunks:\n",
    "        token = token.text.lower().strip()\n",
    "        if token in general_skills_list:\n",
    "            skillset.append(token)\n",
    "    return [i.capitalize() for i in set([i.lower() for i in skillset])]\n",
    "  \n",
    "print ('General Skills',general_skills(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "programming_skills_list = [\n",
    "    \"Programming Languages\",\n",
    "    \"Python\",\n",
    "    \"JavaScript\",\n",
    "    \"Java\",\n",
    "    \"C++\",\n",
    "    \"C#\",\n",
    "    \"Ruby\",\n",
    "    \"PHP\",\n",
    "    \"Swift\",\n",
    "    \"Go\",\n",
    "    \"Rust\",\n",
    "    \"TypeScript\",\n",
    "    \"HTML\",\n",
    "    \"CSS\",\n",
    "    \"SQL\",\n",
    "    \"Shell Scripting\",\n",
    "    \"Version Control\",\n",
    "    \"Git\",\n",
    "    \"SVN\",\n",
    "    \"Continuous Integration/Continuous Deployment (CI/CD)\",\n",
    "    \"Agile Development\",\n",
    "    \"Test-Driven Development (TDD)\",\n",
    "    \"Object-Oriented Programming (OOP)\",\n",
    "    \"Functional Programming\",\n",
    "    \"Web Development\",\n",
    "    \"Front-end Development\",\n",
    "    \"Back-end Development\",\n",
    "    \"Full-Stack Development\",\n",
    "    \"Mobile Development\",\n",
    "    \"iOS Development\",\n",
    "    \"Android Development\",\n",
    "    \"Database Management\",\n",
    "    \"Database Design\",\n",
    "    \"Query Optimization\",\n",
    "    \"API Development\",\n",
    "    \"RESTful APIs\",\n",
    "    \"Web Services\",\n",
    "    \"Microservices\",\n",
    "    \"Cloud Computing\",\n",
    "    \"Amazon Web Services (AWS)\",\n",
    "    \"Microsoft Azure\",\n",
    "    \"Google Cloud Platform (GCP)\",\n",
    "    \"Containerization\",\n",
    "    \"Docker\",\n",
    "    \"Kubernetes\",\n",
    "    \"Server Administration\",\n",
    "    \"Linux\",\n",
    "    \"Windows Server\",\n",
    "    \"Networking\",\n",
    "    \"Security\",\n",
    "    \"Cybersecurity\",\n",
    "    \"Data Structures\",\n",
    "    \"Algorithms\",\n",
    "    \"Software Development\",\n",
    "    \"Software Architecture\",\n",
    "    \"Software Testing\",\n",
    "    \"Debugging\",\n",
    "    \"Problem Solving\",\n",
    "    \"Code Optimization\",\n",
    "    \"Performance Tuning\",\n",
    "    \"Code Review\",\n",
    "    \"Documentation\",\n",
    "    \"Unit Testing\",\n",
    "    \"Integration Testing\",\n",
    "    \"System Testing\",\n",
    "    \"Front-end Frameworks\",\n",
    "    \"React\",\n",
    "    \"Angular\",\n",
    "    \"Vue.js\",\n",
    "    \"Back-end Frameworks\",\n",
    "    \"Django\",\n",
    "    \"Ruby on Rails\",\n",
    "    \"Node.js\",\n",
    "    \"Flask\",\n",
    "    \"ASP.NET\",\n",
    "    \"PHP Frameworks\",\n",
    "    \"Laravel\",\n",
    "    \"Symfony\",\n",
    "    \"CodeIgniter\",\n",
    "    \"Testing Frameworks\",\n",
    "    \"JUnit\",\n",
    "    \"PyTest\",\n",
    "    \"Mocha\",\n",
    "    \"Jest\",\n",
    "    \"Database Systems\",\n",
    "    \"MySQL\",\n",
    "    \"PostgreSQL\",\n",
    "    \"Oracle\",\n",
    "    \"MongoDB\",\n",
    "    \"Redis\",\n",
    "    \"Machine Learning\",\n",
    "    \"Data Analysis\",\n",
    "    \"Data Visualization\",\n",
    "    \"Artificial Intelligence\",\n",
    "    \"Natural Language Processing (NLP)\",\n",
    "    \"Big Data\",\n",
    "    \"Hadoop\",\n",
    "    \"Spark\",\n",
    "    \"Blockchain Development\",\n",
    "    \"Internet of Things (IoT)\",\n",
    "    \"DevOps\",\n",
    "    \"Infrastructure as Code (IaC)\",\n",
    "    \"Configuration Management\",\n",
    "    \"Scripting\",\n",
    "    \"Problem Diagnosis\",\n",
    "    \"Technical Support\",\n",
    "    \"API Integration\",\n",
    "    \"Project Management\",\n",
    "    \"Agile Methodologies\",\n",
    "    \"Scrum\",\n",
    "    \"Kanban\",\n",
    "    \"Software Documentation\",\n",
    "    \"Collaboration Tools\",\n",
    "    \"Jira\",\n",
    "    \"Confluence\",\n",
    "    \"Slack\",\n",
    "    \"Version Control Systems\",\n",
    "    \"Git\",\n",
    "    \"SVN\",\n",
    "    \"Code Editors\",\n",
    "    \"Visual Studio Code\",\n",
    "    \"PyCharm\",\n",
    "    \"IntelliJ IDEA\",\n",
    "    \"Eclipse\",\n",
    "    \"Sublime Text\",\n",
    "    \"Atom\",\n",
    "    \"Operating Systems\",\n",
    "    \"Linux\",\n",
    "    \"Windows\",\n",
    "    \"macOS\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert programming skills to lower case\n",
    "programming_skills_list = [i.lower() for i in programming_skills_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skills ['Agile methodologies', 'Postgresql', 'Docker', 'Mysql', 'Python', 'Sql', 'Slack', 'Jira', 'Machine learning', 'Mongodb', 'Git', 'Spark']\n"
     ]
    }
   ],
   "source": [
    "def programming_skills(resume_text):\n",
    "    nlp_text = nlp(resume_text)\n",
    "\n",
    "    noun_chunks = nlp_text.noun_chunks\n",
    "\n",
    "    # removing stop words and implementing word tokenization\n",
    "    tokens = [token.text for token in nlp_text if not token.is_stop]\n",
    "\n",
    "    skillset = []\n",
    "    \n",
    "    # check for one-grams (example: python)\n",
    "    for token in tokens:\n",
    "        if token.lower() in programming_skills_list:\n",
    "            skillset.append(token)\n",
    "   \n",
    "    for token in noun_chunks:\n",
    "        token = token.text.lower().strip()\n",
    "        if token in programming_skills_list:\n",
    "            skillset.append(token)\n",
    "    return [i.capitalize() for i in set([i.lower() for i in skillset])]\n",
    "  \n",
    "print ('Skills',programming_skills(text))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "languages_list = [\n",
    "    \"English\",\n",
    "    \"Spanish\",\n",
    "    \"French\",\n",
    "    \"German\",\n",
    "    \"Chinese\",\n",
    "    \"Mandarin\",\n",
    "    \"Arabic\",\n",
    "    \"Hindi\",\n",
    "    \"Portuguese\",\n",
    "    \"Bengali\",\n",
    "    \"Russian\",\n",
    "    \"Japanese\",\n",
    "    \"Lahnda\",\n",
    "    \"Javanese\",\n",
    "    \"Wu\",\n",
    "    \"Telugu\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert languages to lower case\n",
    "languages_list = [i.lower() for i in languages_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "language_level = [\n",
    "    \"Elementary Proficiency\",\n",
    "    \"Limited Working Proficiency\",\n",
    "    \"Professional Working Proficiency\",\n",
    "    \"Full Professional Proficiency\",\n",
    "    \"Native or Bilingual Proficiency\",\n",
    "    \"Native\",\n",
    "    \"Advanced\",\n",
    "    \"A1\",\n",
    "    \"A2\",\n",
    "    \"B1\",\n",
    "    \"B2\",\n",
    "    \"C1\",\n",
    "    \"C2\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert language levels to lower case\n",
    "language_level = [i.lower() for i in language_level]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Languages: {'Spanish': 'Native', 'English': 'Advanced', 'Portuguese': 'Level Not Specified'}\n"
     ]
    }
   ],
   "source": [
    "def language_skill(resume_text):\n",
    "    nlp_text = nlp(resume_text)\n",
    "\n",
    "    # removing stop words and implementing word tokenization\n",
    "    tokens = [token.text for token in nlp_text if not token.is_stop]\n",
    "\n",
    "    # Drop every token that is equal to a special character\n",
    "    tokens = [token for token in tokens if not token in string.punctuation]\n",
    "\n",
    "    skillset = {}\n",
    "\n",
    "    # check for one-grams (example: python)\n",
    "    for token in tokens:\n",
    "        if token.lower() in languages_list:\n",
    "            skillset[token] = 'Level Not Specified'\n",
    "            if tokens[tokens.index(token) + 1].lower() in language_level:\n",
    "                skillset[token] = tokens[tokens.index(token) + 1]\n",
    "\n",
    "    return skillset\n",
    "  \n",
    "print ('Languages:' ,language_skill(text))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus Points\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "bonus_points_list = [\n",
    "    'Projects',\n",
    "    'Achievements',\n",
    "    'Hobbies'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bonus: {'Projects': 'Yes', 'Achievements': 'No', 'Hobbies': 'No'}\n"
     ]
    }
   ],
   "source": [
    "def bonus(resume_text):\n",
    "    nlp_text = nlp(resume_text)\n",
    "\n",
    "    # removing stop words and implementing word tokenization\n",
    "    tokens = [token.text for token in nlp_text if not token.is_stop]\n",
    "\n",
    "    # Put tokens list in lower case\n",
    "    tokens = [token.lower() for token in tokens]\n",
    "\n",
    "    bonus_points = {}\n",
    "\n",
    "    # check for one-grams (example: python)\n",
    "    for bonus_piece in bonus_points_list:\n",
    "        if bonus_piece.lower() in tokens:\n",
    "            bonus_points[bonus_piece] = 'Yes'\n",
    "        else:\n",
    "            bonus_points[bonus_piece] = 'No'\n",
    "\n",
    "    return bonus_points\n",
    "  \n",
    "print ('Bonus:' , bonus(text))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name:  Sickit Learn\n",
      "Qualification:  ['Bachelors']\n",
      "Mail id:  ['c.fontana95@gmail.com']\n",
      "Mobile Number:  3934444659\n",
      "General Skills: []\n",
      "Programming Skills: ['Agile methodologies', 'Postgresql', 'Docker', 'Mysql', 'Python', 'Sql', 'Slack', 'Jira', 'Machine learning', 'Mongodb', 'Git', 'Spark']\n",
      "Languages: {'Spanish': 'Native', 'English': 'Advanced', 'Portuguese': 'Level Not Specified'}\n",
      "Bonus: {'Projects': 'Yes', 'Achievements': 'No', 'Hobbies': 'No'}\n"
     ]
    }
   ],
   "source": [
    "print('Name: ',extract_name(text))\n",
    "print('Qualification: ', extract_education(text))\n",
    "print('Mail id: ',extract_email_addresses(text))\n",
    "print('Mobile Number: ',extract_mobile_number(text))\n",
    "print ('General Skills:',general_skills(text))\n",
    "print ('Programming Skills:',programming_skills(text))\n",
    "print ('Languages:' ,language_skill(text))\n",
    "print ('Bonus:' , bonus(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "el punto 2, si usas Python, y haces una Lambda en AWS que de entrada tiene un Json ó CSV\n",
    "con los datos del CSV y de salida te devuelve un Json o CSV con los datos del candidato,\n",
    "ya eso Ayrton puede consumirlo en la app de HR q esta haciendo con PHP Laravel\n",
    "\n",
    "y de respuesta, es un json q diga por ej: \n",
    "\n",
    "Perfil: front end developer\n",
    "Edad: 20 años\n",
    "Tech principal: JavaScript\n",
    "Framework principal: React Native\n",
    "Tech secundaria: CSS\n",
    "Ultima empresa donde trabajo: Amazon\n",
    "Años en ultima empresa: 4.5\n",
    "Idioma principal: Ingles\n",
    "Nivel del idioma Principal: 8"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "import docx2txt\n",
    "from PyPDF2 import PdfReader, PdfFileWriter, PdfFileMerger\n",
    "\n",
    "import spacy\n",
    "import en_core_web_sm\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from spacy import displacy\n",
    "\n",
    "\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\cfont\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<spacy.lang.en.English at 0x1fc8a5b1e90>"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "spacy.load('en_core_web_sm')\n",
    "spacy.load('en_core_web_lg')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extracting text from DOCX\n",
    "def doctotext(m):\n",
    "    temp = docx2txt.process(m)\n",
    "    resume_text = [line.replace('\\t', ' ') for line in temp.split('\\n') if line]\n",
    "    text = ' '.join(resume_text)\n",
    "    return (text)\n",
    "    \n",
    "#Extracting text from PDF\n",
    "def pdftotext(m):\n",
    "    # pdf file object\n",
    "    # you can find find the pdf file with complete code in below\n",
    "    pdfFileObj = open(m, 'rb')\n",
    "\n",
    "    # pdf reader object\n",
    "    pdfFileReader = PdfReader(pdfFileObj)\n",
    "\n",
    "    # number of pages in pdf\n",
    "    num_pages = len(pdfFileReader.pages)\n",
    "\n",
    "    currentPageNumber = 0\n",
    "    text = ''\n",
    "\n",
    "    # Loop in all the pdf pages.\n",
    "    while(currentPageNumber < num_pages ):\n",
    "\n",
    "        # Get the specified pdf page object.\n",
    "        pdfPage = pdfFileReader.pages[currentPageNumber]\n",
    "\n",
    "        # Get pdf page text.\n",
    "        text = text + pdfPage.extract_text()\n",
    "\n",
    "        # Process next page.\n",
    "        currentPageNumber += 1\n",
    "\n",
    "    return (text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = pdftotext(\"C:/Users/cfont/Downloads/Cristian Fontana - CV-Resume.pdf\")\n",
    "\n",
    "# load pre-trained model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "# nlp_big = spacy.load('en_core_web_lg')\n",
    "\n",
    "# initialize matcher with a vocab\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "# displacy.render(nlp(text),style=\"ent\",jupyter=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name:  Sickit Learn\n"
     ]
    }
   ],
   "source": [
    "def extract_name(resume_text):\n",
    "    nlp_text = nlp(resume_text)\n",
    "    \n",
    "    # First name and Last name are always Proper Nouns\n",
    "    pattern = [{'POS': 'PROPN'}, {'POS': 'PROPN'}]\n",
    "    \n",
    "    matcher.add('NAME', [pattern])\n",
    "    \n",
    "    matches = matcher(nlp_text)\n",
    "    \n",
    "    for match_id, start, end in matches:\n",
    "        span = nlp_text[start:end]\n",
    "        return span.text\n",
    "print('Name: ',extract_name(pdftotext(\"C:/Users/cfont/Downloads/Cristian Fontana - CV-Resume.pdf\")))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Qualification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_text = nlp(text)\n",
    "\n",
    "# Sentence Tokenizer\n",
    "nlp_text = [sent.text.strip() for sent in nlp_text.sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qualification:  {'Sentence': \"Experience\\nEDUCATION\\n2014-2020\\nBachelor's degree in Economics.\", 'Education': 'Bachelors', 'GPA': [], 'Subject': 'economic', 'Years': ['2014', '2020']}\n"
     ]
    }
   ],
   "source": [
    "# Grad all general stop words\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "\n",
    "# Education Degrees\n",
    "EDUCATION = [\n",
    "            'BE','B.E.', 'B.E', 'BS', 'B.S', \n",
    "            'ME', 'M.E', 'M.E.', 'M.B.A', 'MBA', 'MS', 'M.S', \n",
    "            'BTECH', 'B.TECH', 'M.TECH', 'MTECH', \n",
    "            'SSLC', 'SSC' 'HSC', 'CBSE', 'ICSE', 'X', 'XII',\n",
    "            'BACHELOR', 'MASTER', 'PHD', 'BACHELORS', 'MASTERS', 'Ph.D.',\n",
    "            'Licenciatura', 'Ingeniería', 'Maestría', 'Maestria',\n",
    "            'Maestra', 'Maestro', 'Doctorado', 'Doctora', 'Doctor', 'Licenciado', 'Licenciada',\n",
    "            'Ingeniero', 'Ingeniera', 'Maestrante', 'Doctorante', 'L'\n",
    "            'Lic', 'Ing'\n",
    "        ]\n",
    "\n",
    "# Make every word in EDUCATION lowercase.\n",
    "EDUCATION = [x.lower() for x in EDUCATION]\n",
    "\n",
    "bachelor_subjects = [\n",
    "    \"Computer Science\",\n",
    "    \"Physics\",\n",
    "    \"Chemistry\",\n",
    "    \"Biology\",\n",
    "    \"Mathematics\",\n",
    "    \"Engineering\",\n",
    "    \"Psychology\",\n",
    "    \"English Literature\",\n",
    "    \"History\",\n",
    "    \"Sociology\",\n",
    "    \"Economics\",\n",
    "    \"Political Science\",\n",
    "    \"Business Administration\",\n",
    "    \"Marketing\",\n",
    "    \"Accounting\",\n",
    "    \"Finance\",\n",
    "    \"Nursing\",\n",
    "    \"Environmental Science\",\n",
    "    \"Art\",\n",
    "    \"Music\",\n",
    "    \"Film Studies\",\n",
    "    \"Philosophy\",\n",
    "    \"Anthropology\",\n",
    "    \"Communications\",\n",
    "    \"Languages\",\n",
    "    \"Geography\",\n",
    "    \"Architecture\",\n",
    "    \"Urban Planning\",\n",
    "    \"Graphic Design\",\n",
    "    \"Journalism\",\n",
    "    \"Criminal Justice\",\n",
    "    \"Law\",\n",
    "    \"International Relations\",\n",
    "    \"Sports Science\",\n",
    "    \"Theater\",\n",
    "    \"Dance\",\n",
    "    \"Religious Studies\",\n",
    "    \"Information Technology\",\n",
    "    \"Health Sciences\",\n",
    "    \"Social Work\",\n",
    "    \"Public Health\",\n",
    "    \"Nutrition\",\n",
    "    \"Linguistics\",\n",
    "    \"Human Resources\",\n",
    "    \"Hospitality Management\",\n",
    "    \"Tourism\",\n",
    "    \"Fashion Design\",\n",
    "    \"Interior Design\"\n",
    "]\n",
    "\n",
    "# Lemmatization of every word in bachelor_subjects\n",
    "bachelor_subjects = [nlp(text) for text in bachelor_subjects]\n",
    "\n",
    "# For each sentence in bachelor_subjects keep the lemma of each word and keep them as strings.\n",
    "bachelor_subjects_lemma = [' '.join([word.lemma_ for word in subject]) for subject in bachelor_subjects]\n",
    "\n",
    "def extract_education(resume_text):\n",
    "    nlp_text = nlp(resume_text)\n",
    "\n",
    "    # Sentence Tokenizer\n",
    "    nlp_text = [sent.text.strip() for sent in nlp_text.sents]\n",
    "\n",
    "    edu = {}\n",
    "\n",
    "    for index, text in enumerate(nlp_text):\n",
    "        for tex in text.split():\n",
    "            # Replace all special symbols\n",
    "                tex = re.sub(r'[?|$|.|!|,\\']', r'', tex)\n",
    "                if tex.lower() in EDUCATION and tex not in STOPWORDS:\n",
    "                    edu['Sentence'] = text\n",
    "                    edu['Education'] = tex\n",
    "\n",
    "                    # Find the GPA.\n",
    "                    edu['GPA'] = re.findall(r'\\b\\d\\.\\d\\b', text)\n",
    "                    \n",
    "                    text_lemma = [nlp(word) for word in text.split()]\n",
    "\n",
    "                    for word in text_lemma:\n",
    "                        for single in word:\n",
    "                            if single.lemma_ in bachelor_subjects_lemma:\n",
    "                                edu['Subject'] = single.lemma_\n",
    "                                \n",
    "                    # Find dates using regex.\n",
    "                    # If you want to extract the year from the text, change the regex to r'(\\d{4})'\n",
    "                    edu['Years'] = re.findall(r'(20\\d{2}|19\\d{2})', text)\n",
    "    \n",
    "    return edu\n",
    "print('Qualification: ', extract_education(text))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Email"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mail id:  ['c.fontana95@gmail.com']\n"
     ]
    }
   ],
   "source": [
    "def extract_email_addresses(string):\n",
    "    r = re.compile(r'[\\w\\.-]+@[\\w\\.-]+')\n",
    "    return r.findall(string)\n",
    "print('Mail id: ',extract_email_addresses(text))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phone Number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mobile Number:  3934444659\n"
     ]
    }
   ],
   "source": [
    "def extract_mobile_number(resume_text):\n",
    "    phone = re.findall(re.compile(r'(?:(?:\\+?([1-9]|[0-9][0-9]|[0-9][0-9][0-9])\\s*(?:[.-]\\s*)?)?(?:\\(\\s*([2-9]1[02-9]|[2-9][02-8]1|[2-9][02-8][02-9])\\s*\\)|([0-9][1-9]|[0-9]1[02-9]|[2-9][02-8]1|[2-9][02-8][02-9]))\\s*(?:[.-]\\s*)?)?([2-9]1[02-9]|[2-9][02-9]1|[2-9][02-9]{2})\\s*(?:[.-]\\s*)?([0-9]{4})(?:\\s*(?:#|x\\.?|ext\\.?|extension)\\s*(\\d+))?'), resume_text)\n",
    "    \n",
    "    if phone:\n",
    "        number = ''.join(phone[0])\n",
    "        if len(number) > 10:\n",
    "            return number\n",
    "        else:\n",
    "            return number\n",
    "print('Mobile Number: ',extract_mobile_number(text))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Skills"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "general_skills_list = [\n",
    "    \"Communication Skills\",\n",
    "    \"Verbal Communication\",\n",
    "    \"Written Communication\",\n",
    "    \"Presentation Skills\",\n",
    "    \"Public Speaking\",\n",
    "    \"Interpersonal Skills\",\n",
    "    \"Negotiation Skills\",\n",
    "    \"Listening Skills\",\n",
    "    \"Persuasion Skills\",\n",
    "    \"Teamwork\",\n",
    "    \"Collaboration\",\n",
    "    \"Leadership\",\n",
    "    \"Conflict Resolution\",\n",
    "    \"Relationship Building\",\n",
    "    \"Networking\",\n",
    "    \"Analytical Skills\",\n",
    "    \"Critical Thinking\",\n",
    "    \"Problem Solving\",\n",
    "    \"Research Skills\",\n",
    "    \"Data Analysis\",\n",
    "    \"Quantitative Analysis\",\n",
    "    \"Qualitative Analysis\",\n",
    "    \"Decision Making\",\n",
    "    \"Attention to Detail\",\n",
    "    \"Logical Reasoning\",\n",
    "    \"Technical Skills\",\n",
    "    \"Computer Literacy\",\n",
    "    \"Programming Languages\",\n",
    "    \"Software Proficiency\",\n",
    "    \"Web Development\",\n",
    "    \"Database Management\",\n",
    "    \"Information Technology\",\n",
    "    \"Troubleshooting\",\n",
    "    \"Systems Administration\",\n",
    "    \"Network Security\",\n",
    "    \"Creativity\",\n",
    "    \"Innovation\",\n",
    "    \"Graphic Design\",\n",
    "    \"Artistic Skills\",\n",
    "    \"Photography\",\n",
    "    \"Video Editing\",\n",
    "    \"Content Creation\",\n",
    "    \"Writing Skills\",\n",
    "    \"Copywriting\",\n",
    "    \"Proofreading and Editing\",\n",
    "    \"Organization\",\n",
    "    \"Time Management\",\n",
    "    \"Project Management\",\n",
    "    \"Planning and Coordination\",\n",
    "    \"Multitasking\",\n",
    "    \"Prioritization\",\n",
    "    \"Detail Orientation\",\n",
    "    \"Meeting Deadlines\",\n",
    "    \"Resource Management\",\n",
    "    \"Customer Service\",\n",
    "    \"Client Management\",\n",
    "    \"Relationship Management\",\n",
    "    \"Conflict Resolution (customer-facing)\",\n",
    "    \"Sales Skills\",\n",
    "    \"Account Management\",\n",
    "    \"Marketing Skills\",\n",
    "    \"Market Research\",\n",
    "    \"Advertising\",\n",
    "    \"Social Media Marketing\",\n",
    "    \"Search Engine Optimization (SEO)\",\n",
    "    \"Language Skills\",\n",
    "    \"Bilingualism\",\n",
    "    \"Translation\",\n",
    "    \"Interpretation\",\n",
    "    \"Financial Skills\",\n",
    "    \"Accounting\",\n",
    "    \"Financial Analysis\",\n",
    "    \"Budgeting\",\n",
    "    \"Financial Planning\",\n",
    "    \"Risk Management\",\n",
    "    \"Teaching and Training\",\n",
    "    \"Instructional Design\",\n",
    "    \"Curriculum Development\",\n",
    "    \"Tutoring\",\n",
    "    \"Mentoring\",\n",
    "    \"Project Coordination\",\n",
    "    \"Event Planning\",\n",
    "    \"Event Management\",\n",
    "    \"Logistics\",\n",
    "    \"Supply Chain Management\",\n",
    "    \"Research and Development\",\n",
    "    \"Scientific Methodology\",\n",
    "    \"Lab Techniques\",\n",
    "    \"Experimental Design\",\n",
    "    \"Statistical Analysis\",\n",
    "    \"Problem Diagnosis\",\n",
    "    \"Troubleshooting (Technical)\",\n",
    "    \"Maintenance and Repair\",\n",
    "    \"Equipment Handling\",\n",
    "    \"Mechanical Skills\",\n",
    "    \"Health and Safety\",\n",
    "    \"First Aid\",\n",
    "    \"CPR\",\n",
    "    \"Occupational Health and Safety\",\n",
    "    \"Risk Assessment\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert general skills to lowercase.\n",
    "general_skills_list = [skill.lower() for skill in general_skills_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "General Skills []\n"
     ]
    }
   ],
   "source": [
    "def general_skills(resume_text):\n",
    "    nlp_text = nlp(resume_text)\n",
    "\n",
    "    noun_chunks = nlp_text.noun_chunks\n",
    "\n",
    "    # removing stop words and implementing word tokenization\n",
    "    tokens = [token.text for token in nlp_text if not token.is_stop]\n",
    "    \n",
    "    # extract values\n",
    "    skillset = []\n",
    "    \n",
    "    # check for one-grams (example: python)\n",
    "    for token in tokens:\n",
    "        if token.lower() in general_skills_list:\n",
    "            skillset.append(token)\n",
    "   \n",
    "    for token in noun_chunks:\n",
    "        token = token.text.lower().strip()\n",
    "        if token in general_skills_list:\n",
    "            skillset.append(token)\n",
    "    return [i.capitalize() for i in set([i.lower() for i in skillset])]\n",
    "  \n",
    "print ('General Skills',general_skills(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "programming_skills_list = [\n",
    "    \"Programming Languages\",\n",
    "    \"Python\",\n",
    "    \"JavaScript\",\n",
    "    \"Java\",\n",
    "    \"C++\",\n",
    "    \"C#\",\n",
    "    \"Ruby\",\n",
    "    \"PHP\",\n",
    "    \"Swift\",\n",
    "    \"Go\",\n",
    "    \"Rust\",\n",
    "    \"TypeScript\",\n",
    "    \"HTML\",\n",
    "    \"CSS\",\n",
    "    \"SQL\",\n",
    "    \"Shell Scripting\",\n",
    "    \"Version Control\",\n",
    "    \"Git\",\n",
    "    \"SVN\",\n",
    "    \"Continuous Integration/Continuous Deployment (CI/CD)\",\n",
    "    \"Agile Development\",\n",
    "    \"Test-Driven Development (TDD)\",\n",
    "    \"Object-Oriented Programming (OOP)\",\n",
    "    \"Functional Programming\",\n",
    "    \"Web Development\",\n",
    "    \"Front-end Development\",\n",
    "    \"Back-end Development\",\n",
    "    \"Full-Stack Development\",\n",
    "    \"Mobile Development\",\n",
    "    \"iOS Development\",\n",
    "    \"Android Development\",\n",
    "    \"Database Management\",\n",
    "    \"Database Design\",\n",
    "    \"Query Optimization\",\n",
    "    \"API Development\",\n",
    "    \"RESTful APIs\",\n",
    "    \"Web Services\",\n",
    "    \"Microservices\",\n",
    "    \"Cloud Computing\",\n",
    "    \"Amazon Web Services (AWS)\",\n",
    "    \"Microsoft Azure\",\n",
    "    \"Google Cloud Platform (GCP)\",\n",
    "    \"Containerization\",\n",
    "    \"Docker\",\n",
    "    \"Kubernetes\",\n",
    "    \"Server Administration\",\n",
    "    \"Linux\",\n",
    "    \"Windows Server\",\n",
    "    \"Networking\",\n",
    "    \"Security\",\n",
    "    \"Cybersecurity\",\n",
    "    \"Data Structures\",\n",
    "    \"Algorithms\",\n",
    "    \"Software Development\",\n",
    "    \"Software Architecture\",\n",
    "    \"Software Testing\",\n",
    "    \"Debugging\",\n",
    "    \"Problem Solving\",\n",
    "    \"Code Optimization\",\n",
    "    \"Performance Tuning\",\n",
    "    \"Code Review\",\n",
    "    \"Documentation\",\n",
    "    \"Unit Testing\",\n",
    "    \"Integration Testing\",\n",
    "    \"System Testing\",\n",
    "    \"Front-end Frameworks\",\n",
    "    \"React\",\n",
    "    \"Angular\",\n",
    "    \"Vue.js\",\n",
    "    \"Back-end Frameworks\",\n",
    "    \"Django\",\n",
    "    \"Ruby on Rails\",\n",
    "    \"Node.js\",\n",
    "    \"Flask\",\n",
    "    \"ASP.NET\",\n",
    "    \"PHP Frameworks\",\n",
    "    \"Laravel\",\n",
    "    \"Symfony\",\n",
    "    \"CodeIgniter\",\n",
    "    \"Testing Frameworks\",\n",
    "    \"JUnit\",\n",
    "    \"PyTest\",\n",
    "    \"Mocha\",\n",
    "    \"Jest\",\n",
    "    \"Database Systems\",\n",
    "    \"MySQL\",\n",
    "    \"PostgreSQL\",\n",
    "    \"Oracle\",\n",
    "    \"MongoDB\",\n",
    "    \"Redis\",\n",
    "    \"Machine Learning\",\n",
    "    \"Data Analysis\",\n",
    "    \"Data Visualization\",\n",
    "    \"Artificial Intelligence\",\n",
    "    \"Natural Language Processing (NLP)\",\n",
    "    \"Big Data\",\n",
    "    \"Hadoop\",\n",
    "    \"Spark\",\n",
    "    \"Blockchain Development\",\n",
    "    \"Internet of Things (IoT)\",\n",
    "    \"DevOps\",\n",
    "    \"Infrastructure as Code (IaC)\",\n",
    "    \"Configuration Management\",\n",
    "    \"Scripting\",\n",
    "    \"Problem Diagnosis\",\n",
    "    \"Technical Support\",\n",
    "    \"API Integration\",\n",
    "    \"Project Management\",\n",
    "    \"Agile Methodologies\",\n",
    "    \"Scrum\",\n",
    "    \"Kanban\",\n",
    "    \"Software Documentation\",\n",
    "    \"Collaboration Tools\",\n",
    "    \"Jira\",\n",
    "    \"Confluence\",\n",
    "    \"Slack\",\n",
    "    \"Version Control Systems\",\n",
    "    \"Git\",\n",
    "    \"SVN\",\n",
    "    \"Code Editors\",\n",
    "    \"Visual Studio Code\",\n",
    "    \"PyCharm\",\n",
    "    \"IntelliJ IDEA\",\n",
    "    \"Eclipse\",\n",
    "    \"Sublime Text\",\n",
    "    \"Atom\",\n",
    "    \"Operating Systems\",\n",
    "    \"Linux\",\n",
    "    \"Windows\",\n",
    "    \"macOS\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert programming skills to lower case\n",
    "programming_skills_list = [i.lower() for i in programming_skills_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skills ['Agile methodologies', 'Slack', 'Mysql', 'Sql', 'Mongodb', 'Git', 'Docker', 'Postgresql', 'Jira', 'Python', 'Machine learning', 'Spark']\n"
     ]
    }
   ],
   "source": [
    "def programming_skills(resume_text):\n",
    "    nlp_text = nlp(resume_text)\n",
    "\n",
    "    noun_chunks = nlp_text.noun_chunks\n",
    "\n",
    "    # removing stop words and implementing word tokenization\n",
    "    tokens = [token.text for token in nlp_text if not token.is_stop]\n",
    "\n",
    "    skillset = []\n",
    "    \n",
    "    # check for one-grams (example: python)\n",
    "    for token in tokens:\n",
    "        if token.lower() in programming_skills_list:\n",
    "            skillset.append(token)\n",
    "   \n",
    "    for token in noun_chunks:\n",
    "        token = token.text.lower().strip()\n",
    "        if token in programming_skills_list:\n",
    "            skillset.append(token)\n",
    "    return [i.capitalize() for i in set([i.lower() for i in skillset])]\n",
    "  \n",
    "print ('Skills',programming_skills(text))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "languages_list = [\n",
    "    \"English\",\n",
    "    \"Spanish\",\n",
    "    \"French\",\n",
    "    \"German\",\n",
    "    \"Chinese\",\n",
    "    \"Mandarin\",\n",
    "    \"Arabic\",\n",
    "    \"Hindi\",\n",
    "    \"Portuguese\",\n",
    "    \"Bengali\",\n",
    "    \"Russian\",\n",
    "    \"Japanese\",\n",
    "    \"Lahnda\",\n",
    "    \"Javanese\",\n",
    "    \"Wu\",\n",
    "    \"Telugu\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert languages to lower case\n",
    "languages_list = [i.lower() for i in languages_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "language_level = [\n",
    "    \"Elementary Proficiency\",\n",
    "    \"Limited Working Proficiency\",\n",
    "    \"Professional Working Proficiency\",\n",
    "    \"Full Professional Proficiency\",\n",
    "    \"Native or Bilingual Proficiency\",\n",
    "    \"Native\",\n",
    "    \"Advanced\",\n",
    "    \"A1\",\n",
    "    \"A2\",\n",
    "    \"B1\",\n",
    "    \"B2\",\n",
    "    \"C1\",\n",
    "    \"C2\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert language levels to lower case\n",
    "language_level = [i.lower() for i in language_level]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Languages: {'Spanish': 'Native', 'English': 'Advanced', 'Portuguese': 'Level Not Specified'}\n"
     ]
    }
   ],
   "source": [
    "def language_skill(resume_text):\n",
    "    nlp_text = nlp(resume_text)\n",
    "\n",
    "    # removing stop words and implementing word tokenization\n",
    "    tokens = [token.text for token in nlp_text if not token.is_stop]\n",
    "\n",
    "    # Drop every token that is equal to a special character\n",
    "    tokens = [token for token in tokens if not token in string.punctuation]\n",
    "\n",
    "    skillset = {}\n",
    "\n",
    "    # check for one-grams (example: python)\n",
    "    for token in tokens:\n",
    "        if token.lower() in languages_list:\n",
    "            skillset[token] = 'Level Not Specified'\n",
    "            if tokens[tokens.index(token) + 1].lower() in language_level:\n",
    "                skillset[token] = tokens[tokens.index(token) + 1]\n",
    "\n",
    "    return skillset\n",
    "  \n",
    "print ('Languages:' ,language_skill(text))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus Points\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "bonus_points_list = [\n",
    "    'Projects',\n",
    "    'Achievements',\n",
    "    'Hobbies'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bonus: {'Projects': 'Yes', 'Achievements': 'No', 'Hobbies': 'No'}\n"
     ]
    }
   ],
   "source": [
    "def bonus(resume_text):\n",
    "    nlp_text = nlp(resume_text)\n",
    "\n",
    "    # removing stop words and implementing word tokenization\n",
    "    tokens = [token.text for token in nlp_text if not token.is_stop]\n",
    "\n",
    "    # Put tokens list in lower case\n",
    "    tokens = [token.lower() for token in tokens]\n",
    "\n",
    "    bonus_points = {}\n",
    "\n",
    "    # check for one-grams (example: python)\n",
    "    for bonus_piece in bonus_points_list:\n",
    "        if bonus_piece.lower() in tokens:\n",
    "            bonus_points[bonus_piece] = 'Yes'\n",
    "        else:\n",
    "            bonus_points[bonus_piece] = 'No'\n",
    "\n",
    "    return bonus_points\n",
    "  \n",
    "print ('Bonus:' , bonus(text))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name:  Sickit Learn\n",
      "Qualification:  {'Sentence': \"Experience\\nEDUCATION\\n2014-2020\\nBachelor's degree in Economics.\", 'Education': 'Bachelors', 'GPA': [], 'Subject': 'economic', 'Years': ['2014', '2020']}\n",
      "Mail id:  ['c.fontana95@gmail.com']\n",
      "Mobile Number:  3934444659\n",
      "General Skills: []\n",
      "Programming Skills: ['Agile methodologies', 'Slack', 'Mysql', 'Sql', 'Mongodb', 'Git', 'Docker', 'Postgresql', 'Jira', 'Python', 'Machine learning', 'Spark']\n",
      "Languages: {'Spanish': 'Native', 'English': 'Advanced', 'Portuguese': 'Level Not Specified'}\n",
      "Bonus: {'Projects': 'Yes', 'Achievements': 'No', 'Hobbies': 'No'}\n"
     ]
    }
   ],
   "source": [
    "print('Name: ',extract_name(text))\n",
    "print('Qualification: ', extract_education(text))\n",
    "print('Mail id: ',extract_email_addresses(text))\n",
    "print('Mobile Number: ',extract_mobile_number(text))\n",
    "print ('General Skills:',general_skills(text))\n",
    "print ('Programming Skills:',programming_skills(text))\n",
    "print ('Languages:' ,language_skill(text))\n",
    "print ('Bonus:' , bonus(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid non-printable character U+00A0 (14378996.py, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[195], line 3\u001b[1;36m\u001b[0m\n\u001b[1;33m    ya eso Ayrton puede consumirlo en la app de HR q esta haciendo con PHP Laravel\u001b[0m\n\u001b[1;37m                                                                  ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid non-printable character U+00A0\n"
     ]
    }
   ],
   "source": [
    "el punto 2, si usas Python, y haces una Lambda en AWS que de entrada tiene un Json ó CSV\n",
    "con los datos del CSV y de salida te devuelve un Json o CSV con los datos del candidato,\n",
    "ya eso Ayrton puede consumirlo en la app de HR q esta haciendo con PHP Laravel\n",
    "\n",
    "y de respuesta, es un json q diga por ej: \n",
    "\n",
    "Perfil: front end developer\n",
    "Edad: 20 años\n",
    "Tech principal: JavaScript\n",
    "Framework principal: React Native\n",
    "Tech secundaria: CSS\n",
    "Ultima empresa donde trabajo: Amazon\n",
    "Años en ultima empresa: 4.5\n",
    "Idioma principal: Ingles\n",
    "Nivel del idioma Principal: 8"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import docx2txt\n",
    "from PyPDF2 import PdfReader, PdfFileWriter, PdfFileMerger\n",
    "\n",
    "import spacy\n",
    "import en_core_web_sm\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document\n",
    "\n",
    "text = pdftotext(\"C:/Users/cfont/Downloads/Cristian Fontana - CV-Resume.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract text from pdf.\n",
    "pdf2library\n",
    "\n",
    "# Install spacy and donwload en model.\n",
    "pip install spacy\n",
    "python -m spacy download en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "el punto 2, si usas Python, y haces una Lambda en AWS que de entrada tiene un Json ó CSV\n",
    "con los datos del CSV y de salida te devuelve un Json o CSV con los datos del candidato,\n",
    "ya eso Ayrton puede consumirlo en la app de HR q esta haciendo con PHP Laravel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y de respuesta, es un json q diga por ej: \n",
    "\n",
    "Perfil: front end developer\n",
    "Edad: 20 años\n",
    "Tech principal: JavaScript\n",
    "Framework principal: React Native\n",
    "Tech secundaria: CSS\n",
    "Ultima empresa donde trabajo: Amazon\n",
    "Años en ultima empresa: 4.5\n",
    "Idioma principal: Ingles\n",
    "Nivel del idioma Principal: 8"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extracting text from DOCX\n",
    "def doctotext(m):\n",
    "    temp = docx2txt.process(m)\n",
    "    resume_text = [line.replace('\\t', ' ') for line in temp.split('\\n') if line]\n",
    "    text = ' '.join(resume_text)\n",
    "    return (text)\n",
    "    \n",
    "#Extracting text from PDF\n",
    "def pdftotext(m):\n",
    "    # pdf file object\n",
    "    # you can find find the pdf file with complete code in below\n",
    "    pdfFileObj = open(m, 'rb')\n",
    "\n",
    "    # pdf reader object\n",
    "    pdfFileReader = PdfReader(pdfFileObj)\n",
    "\n",
    "    # number of pages in pdf\n",
    "    num_pages = len(pdfFileReader.pages)\n",
    "\n",
    "    currentPageNumber = 0\n",
    "    text = ''\n",
    "\n",
    "    # Loop in all the pdf pages.\n",
    "    while(currentPageNumber < num_pages ):\n",
    "\n",
    "        # Get the specified pdf page object.\n",
    "        pdfPage = pdfFileReader.pages[currentPageNumber]\n",
    "\n",
    "        # Get pdf page text.\n",
    "        text = text + pdfPage.extract_text()\n",
    "\n",
    "        # Process next page.\n",
    "        currentPageNumber += 1\n",
    "    return (text)\n",
    "\n",
    "# #main function\n",
    "# if __name__ == '__main__': \n",
    "\n",
    "#     FilePath = 'AI.pdf'\n",
    "#     FilePath.lower().endswith(('.png', '.docx'))\n",
    "#     if FilePath.endswith('.docx'):\n",
    "#       textinput = doctotext(FilePath) \n",
    "#     elif FilePath.endswith('.pdf'):\n",
    "#       textinput = pdftotext(FilePath)\n",
    "#     else:\n",
    "#       print(\"File not support\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved more than 20.000 USD annually to small startups building a financial app with AI that take inputs\n",
      "from multiple models and make decisions in real time. \n",
      "Develop input and assumptions based on preexisting models to estimate the costs and savings\n",
      "opportunities associated with varying levels of network growth and operations.\n",
      "Toolbox: AWS - GCP, Python, Git, Numpy, Pandas, Sickit Learn, Matplotlib, Pipenv, Statsmodels, Plotly,\n",
      "SQL, No-SQL, Boto3, Jupyternotebook, Dash, Lambda, S3, DynamoDB, Serverless, Deployment,\n",
      "Docker, Notion, Figma, Slack, Github, Spark, RedShift, Athena, Azure, Scipy, PostgreSQL.\n",
      "Reduced the churn rate by 15% in 3 months by finding the importance of the clients' two first days of\n",
      "operating.\n",
      "In charge of the dashboards and analytic presentations about the status of the company, clients and\n",
      "commercial status.\n",
      "Toolbox: Mode, Snowflake, Python, Git, Excel, Jira, Github, SQLAlchemy, Jupyternotebook, Numpy,\n",
      "Pandas, Scikit Learn, XGBoost, Scipy, Maplotlib, Pipenv, Seaborn, PostgreSQL, dbt, Carta, Slack, Looker,\n",
      "Power BI, Docker, Mode, Agile methodologies, Scipy, PostgreSQL.\n",
      "Saved 20 to 40 hours monthly to small businesses with the deployment of different dashboards to\n",
      "follow up KPIs. \n",
      "Finding valuable insights, and trends from the data.\n",
      "Deployed a recommendation system that recommends other menu items based on past order history,\n",
      "increasing average spend by 10%.\n",
      "Toolbox: Python, SQL, Excel, Macros VBA, Tableau, Power BI, Pandas, Numpy, Keras, Statsmodel,\n",
      "Tensorflow, Looker, Sagemarker, AutoML, Azure, dbt, Slack, PostgreSQL.\n",
      "Development of analytics and machine learning-based solutions for distinct initiatives. Handling,\n",
      "forecasting and visualizing data for macroeconomic, microeconomic and other financial projects. \n",
      "Recognize internal and external stakeholder requirements. \n",
      "Toolbox: Python, SQL, Excel, Macros VBA, Tableau, Power BI, Keras, Statsmodel, Tensorflow, GitHub,\n",
      "dbt, Trello, Slack, Powerpoint,  PyTorch, LightGBM, XGBoost, Scipy, PostgreSQL.\n",
      "Reduced cost by 20% by implementing a forecast of the needed inventory and avoiding unnecessary\n",
      "costs.\n",
      "Built sales forecast using parameters, trend lines and reference lines, saving more than 30 monthly\n",
      "hours of manual work.\n",
      "Implemented a long-term pricing system that improve the customer lifetime value by 20%.\n",
      "Build out the data and reporting infrastructure from the ground up using Tableau and SQL to provide\n",
      "real-time insights into the product, marketing funnels and business KPIs.\n",
      "Developed and maintained reports with more than 95% on-time delivery.\n",
      "Toolbox: Excel, PowerBI, Tableau, SQL, Python, R, Powerpoint, Pandas, Numpy, PostgreSQL, MySQL.DATA SCIENTIST\n",
      "LEANTK (CONTRACT)\n",
      "01/2023 - 06/2023 (6 MONTHS)\n",
      "DATA SCIENTIST\n",
      "MUNDI (CONTRACT)\n",
      "06/2022 - 12/2022 (6 MONTHS)\n",
      "DATA SCIENTIST/ANALYTICS\n",
      "FREELANCE\n",
      "11/2021 - 05/2022 (6 MONTHS)\n",
      "DATA SCIENTIST\n",
      "PWC\n",
      "01/2021 - 11/2021 (10 MONTHS)\n",
      "DATA ANALYTICS\n",
      "FOBEN \n",
      "09/2017 - 12/2020 (4 YEARS)Data Analyst - DataCamp.\n",
      "Data Scientist - DataCamp.\n",
      "Google Data Analytics Specialization - Coursera.\n",
      "Power BI - Crehana.\n",
      "IBM Data Science - Specialization - Coursera.\n",
      "Macroeconomic Diagnostics - FMI - Edx.\n",
      "Financial Market Analysis - FMI - Edx.\n",
      "Essentials of Corporate Finance - Specialization\n",
      "- University of Melbourne.CERTIFICATESSKILLS\n",
      "PYTHON\n",
      "Numpy, Pandas, Scikit Learn, PyTorch, XGBoost,\n",
      "LightGBM, SciPy, Matplotlib, Keras, Pipenv,\n",
      "Tensorflow, Statsmodels, Plotly, Seaborn,\n",
      "SQLAlchemy, Boto3, Bokeh, Dash,\n",
      "Jupyternotebook, PySpark.\n",
      "CLOUD & DATABASES\n",
      "AWS, GCP, Azure, SQL, No-SQL, Lambda,\n",
      "Sagemaker, S3, Athena, MongoDB, RedShift,\n",
      "DynamoDB, Containers, Storage, Serverless,\n",
      "Deployment, PostgreSQL, SQL, SQL Server, MySQL,\n",
      "Docker, Git, Github, Bash, AutoML.\n",
      "VISUALIZATION\n",
      "Power BI, Tableau, Looker, Mode, Dash, Plotly.\n",
      "OTHERS\n",
      "Agile methodologies, CRISP, Notion, Jira, Trello,\n",
      "Excel, dbt, R, Powerpoint, VBA, Macros, Figma,\n",
      "Carta, Slack, Clickup, Pendo, and Productboard.\n",
      "LANGUAGES\n",
      "Spanish (Native)\n",
      "English (Advanced).\n",
      "Portuguese (Intermediate).Cristian Fontana\n",
      "Data Scientist - Italian Citizen\n",
      "Economist with a strong background in Data Science and Business Intelligence with more than 5 years of\n",
      "experience, building quantitative solutions, doing forecasts, regression, time series, machine learning, and\n",
      "creating dashboards. \n",
      "Advanced understanding of statistical, algebraic and other analytical techniques. Highly organized,\n",
      "motivated and diligent. I enjoy finding problems and working on projects to resolve them holistically.\n",
      "Experience\n",
      "EDUCATION\n",
      "2014-2020\n",
      "Bachelor's degree in Economics.\n",
      "Universidad de Buenos Aires, Argentina.CONTACT\n",
      "+393444465965\n",
      "c.fontana95@gmail.com\n",
      "https://www.linkedin.com/in/cristianfontana/\n"
     ]
    }
   ],
   "source": [
    "pdftotext(\"C:/Users/cfont/Downloads/Cristian Fontana - CV-Resume.pdf\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name:  Sickit Learn\n"
     ]
    }
   ],
   "source": [
    "# load pre-trained model\n",
    "nlp = en_core_web_sm.load()\n",
    "\n",
    "# initialize matcher with a vocab\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "\n",
    "def extract_name(resume_text):\n",
    "    nlp_text = nlp(resume_text)\n",
    "    \n",
    "    # First name and Last name are always Proper Nouns\n",
    "    pattern = [{'POS': 'PROPN'}, {'POS': 'PROPN'}]\n",
    "    \n",
    "    matcher.add('NAME', [pattern])\n",
    "    \n",
    "    matches = matcher(nlp_text)\n",
    "    \n",
    "    for match_id, start, end in matches:\n",
    "        span = nlp_text[start:end]\n",
    "        return span.text\n",
    "print('Name: ',extract_name(pdftotext(\"C:/Users/cfont/Downloads/Cristian Fontana - CV-Resume.pdf\")))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Qualification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qualification:  ['Bachelors']\n"
     ]
    }
   ],
   "source": [
    "# Grad all general stop words\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "\n",
    "# Education Degrees\n",
    "EDUCATION = [\n",
    "            'BE','B.E.', 'B.E', 'BS', 'B.S', \n",
    "            'ME', 'M.E', 'M.E.', 'M.B.A', 'MBA', 'MS', 'M.S', \n",
    "            'BTECH', 'B.TECH', 'M.TECH', 'MTECH', \n",
    "            'SSLC', 'SSC' 'HSC', 'CBSE', 'ICSE', 'X', 'XII',\n",
    "            'BACHELOR', 'MASTER', 'PHD', 'BACHELORS', 'MASTERS', 'Ph.D.',\n",
    "            'Licenciatura', 'Ingeniería', 'Maestría', 'Maestria',\n",
    "            'Maestra', 'Maestro', 'Doctorado', 'Doctora', 'Doctor', 'Licenciado', 'Licenciada',\n",
    "            'Ingeniero', 'Ingeniera', 'Maestrante', 'Doctorante', 'L'\n",
    "            'Lic', 'Ing'\n",
    "        ]\n",
    "\n",
    "def extract_education(resume_text):\n",
    "    nlp_text = nlp(resume_text)\n",
    "\n",
    "    # Sentence Tokenizer\n",
    "    nlp_text = [sent.text.strip() for sent in nlp_text.sents]\n",
    "\n",
    "    edu = {}\n",
    "    # Extract education degree\n",
    "    for index, text in enumerate(nlp_text):\n",
    "        for tex in text.split():\n",
    "            # Replace all special symbols\n",
    "            tex = re.sub(r'[?|$|.|!|,\\']', r'', tex)\n",
    "            if tex.upper() in EDUCATION and tex not in STOPWORDS:\n",
    "                edu[tex] = text + nlp_text[index + 1]\n",
    "                \n",
    "                \n",
    "\n",
    "    # Extract year\n",
    "    education = []\n",
    "    for key in edu.keys():\n",
    "        year = re.search(re.compile(r'(((20|19)(\\d{})))'), edu[key])\n",
    "        if year:\n",
    "            education.append((key, ''.join(year[0])))\n",
    "        else:\n",
    "            education.append(key)\n",
    "    return education\n",
    "print('Qualification: ', extract_education(text))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Email"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mail id:  ['c.fontana95@gmail.com']\n"
     ]
    }
   ],
   "source": [
    "def extract_email_addresses(string):\n",
    "    r = re.compile(r'[\\w\\.-]+@[\\w\\.-]+')\n",
    "    return r.findall(string)\n",
    "print('Mail id: ',extract_email_addresses(text))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phone Number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mobile Number:  3934444659\n"
     ]
    }
   ],
   "source": [
    "def extract_mobile_number(resume_text):\n",
    "    phone = re.findall(re.compile(r'(?:(?:\\+?([1-9]|[0-9][0-9]|[0-9][0-9][0-9])\\s*(?:[.-]\\s*)?)?(?:\\(\\s*([2-9]1[02-9]|[2-9][02-8]1|[2-9][02-8][02-9])\\s*\\)|([0-9][1-9]|[0-9]1[02-9]|[2-9][02-8]1|[2-9][02-8][02-9]))\\s*(?:[.-]\\s*)?)?([2-9]1[02-9]|[2-9][02-9]1|[2-9][02-9]{2})\\s*(?:[.-]\\s*)?([0-9]{4})(?:\\s*(?:#|x\\.?|ext\\.?|extension)\\s*(\\d+))?'), resume_text)\n",
    "    \n",
    "    if phone:\n",
    "        number = ''.join(phone[0])\n",
    "        if len(number) > 10:\n",
    "            return number\n",
    "        else:\n",
    "            return number\n",
    "print('Mobile Number: ',extract_mobile_number(text))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Skills"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "general_skills = [\n",
    "    \"Communication Skills\",\n",
    "    \"Verbal Communication\",\n",
    "    \"Written Communication\",\n",
    "    \"Presentation Skills\",\n",
    "    \"Public Speaking\",\n",
    "    \"Interpersonal Skills\",\n",
    "    \"Negotiation Skills\",\n",
    "    \"Listening Skills\",\n",
    "    \"Persuasion Skills\",\n",
    "    \"Teamwork\",\n",
    "    \"Collaboration\",\n",
    "    \"Leadership\",\n",
    "    \"Conflict Resolution\",\n",
    "    \"Relationship Building\",\n",
    "    \"Networking\",\n",
    "    \"Analytical Skills\",\n",
    "    \"Critical Thinking\",\n",
    "    \"Problem Solving\",\n",
    "    \"Research Skills\",\n",
    "    \"Data Analysis\",\n",
    "    \"Quantitative Analysis\",\n",
    "    \"Qualitative Analysis\",\n",
    "    \"Decision Making\",\n",
    "    \"Attention to Detail\",\n",
    "    \"Logical Reasoning\",\n",
    "    \"Technical Skills\",\n",
    "    \"Computer Literacy\",\n",
    "    \"Programming Languages\",\n",
    "    \"Software Proficiency\",\n",
    "    \"Web Development\",\n",
    "    \"Database Management\",\n",
    "    \"Information Technology\",\n",
    "    \"Troubleshooting\",\n",
    "    \"Systems Administration\",\n",
    "    \"Network Security\",\n",
    "    \"Creativity\",\n",
    "    \"Innovation\",\n",
    "    \"Graphic Design\",\n",
    "    \"Artistic Skills\",\n",
    "    \"Photography\",\n",
    "    \"Video Editing\",\n",
    "    \"Content Creation\",\n",
    "    \"Writing Skills\",\n",
    "    \"Copywriting\",\n",
    "    \"Proofreading and Editing\",\n",
    "    \"Organization\",\n",
    "    \"Time Management\",\n",
    "    \"Project Management\",\n",
    "    \"Planning and Coordination\",\n",
    "    \"Multitasking\",\n",
    "    \"Prioritization\",\n",
    "    \"Detail Orientation\",\n",
    "    \"Meeting Deadlines\",\n",
    "    \"Resource Management\",\n",
    "    \"Customer Service\",\n",
    "    \"Client Management\",\n",
    "    \"Relationship Management\",\n",
    "    \"Conflict Resolution (customer-facing)\",\n",
    "    \"Sales Skills\",\n",
    "    \"Account Management\",\n",
    "    \"Marketing Skills\",\n",
    "    \"Market Research\",\n",
    "    \"Advertising\",\n",
    "    \"Social Media Marketing\",\n",
    "    \"Search Engine Optimization (SEO)\",\n",
    "    \"Language Skills\",\n",
    "    \"Bilingualism\",\n",
    "    \"Translation\",\n",
    "    \"Interpretation\",\n",
    "    \"Financial Skills\",\n",
    "    \"Accounting\",\n",
    "    \"Financial Analysis\",\n",
    "    \"Budgeting\",\n",
    "    \"Financial Planning\",\n",
    "    \"Risk Management\",\n",
    "    \"Teaching and Training\",\n",
    "    \"Instructional Design\",\n",
    "    \"Curriculum Development\",\n",
    "    \"Tutoring\",\n",
    "    \"Mentoring\",\n",
    "    \"Project Coordination\",\n",
    "    \"Event Planning\",\n",
    "    \"Event Management\",\n",
    "    \"Logistics\",\n",
    "    \"Supply Chain Management\",\n",
    "    \"Research and Development\",\n",
    "    \"Scientific Methodology\",\n",
    "    \"Lab Techniques\",\n",
    "    \"Experimental Design\",\n",
    "    \"Statistical Analysis\",\n",
    "    \"Problem Diagnosis\",\n",
    "    \"Troubleshooting (Technical)\",\n",
    "    \"Maintenance and Repair\",\n",
    "    \"Equipment Handling\",\n",
    "    \"Mechanical Skills\",\n",
    "    \"Health and Safety\",\n",
    "    \"First Aid\",\n",
    "    \"CPR\",\n",
    "    \"Occupational Health and Safety\",\n",
    "    \"Risk Assessment\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "argument of type 'function' is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[101], line 23\u001b[0m\n\u001b[0;32m     20\u001b[0m             skillset\u001b[39m.\u001b[39mappend(token)\n\u001b[0;32m     21\u001b[0m     \u001b[39mreturn\u001b[39;00m [i\u001b[39m.\u001b[39mcapitalize() \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mset\u001b[39m([i\u001b[39m.\u001b[39mlower() \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m skillset])]\n\u001b[1;32m---> 23\u001b[0m \u001b[39mprint\u001b[39m (\u001b[39m'\u001b[39m\u001b[39mSkills\u001b[39m\u001b[39m'\u001b[39m,general_skills(text))\n",
      "Cell \u001b[1;32mIn[101], line 14\u001b[0m, in \u001b[0;36mgeneral_skills\u001b[1;34m(resume_text)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[39m# check for one-grams (example: python)\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[39mfor\u001b[39;00m token \u001b[39min\u001b[39;00m tokens:\n\u001b[1;32m---> 14\u001b[0m     \u001b[39mif\u001b[39;00m token\u001b[39m.\u001b[39;49mlower() \u001b[39min\u001b[39;49;00m general_skills:\n\u001b[0;32m     15\u001b[0m         skillset\u001b[39m.\u001b[39mappend(token)\n\u001b[0;32m     17\u001b[0m \u001b[39mfor\u001b[39;00m token \u001b[39min\u001b[39;00m noun_chunks:\n",
      "\u001b[1;31mTypeError\u001b[0m: argument of type 'function' is not iterable"
     ]
    }
   ],
   "source": [
    "def general_skills(resume_text):\n",
    "    nlp_text = nlp(resume_text)\n",
    "\n",
    "    noun_chunks = nlp_text.noun_chunks\n",
    "\n",
    "    # removing stop words and implementing word tokenization\n",
    "    tokens = [token.text for token in nlp_text if not token.is_stop]\n",
    "    \n",
    "    # extract values\n",
    "    skillset = []\n",
    "    \n",
    "    # check for one-grams (example: python)\n",
    "    for token in tokens:\n",
    "        if token.lower() in general_skills:\n",
    "            skillset.append(token)\n",
    "   \n",
    "    for token in noun_chunks:\n",
    "        token = token.text.lower().strip()\n",
    "        if token in general_skills:\n",
    "            skillset.append(token)\n",
    "    return [i.capitalize() for i in set([i.lower() for i in skillset])]\n",
    "  \n",
    "print ('Skills',general_skills(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def programming_skills(resume_text):\n",
    "    nlp_text = nlp(resume_text)\n",
    "\n",
    "    # removing stop words and implementing word tokenization\n",
    "    tokens = [token.text for token in nlp_text if not token.is_stop]\n",
    "    colnames = ['skill']\n",
    "    # reading the csv file\n",
    "    data = pd.read_csv('skill.csv', names=colnames) \n",
    "    \n",
    "    # extract values\n",
    "    skills = data.skill.tolist()\n",
    "    print(skills)\n",
    "    skillset = []\n",
    "    \n",
    "    # check for one-grams (example: python)\n",
    "    for token in tokens:\n",
    "        if token.lower() in skills:\n",
    "            skillset.append(token)\n",
    "   \n",
    "    for token in noun_chunks:\n",
    "        token = token.text.lower().strip()\n",
    "        if token in skills:\n",
    "            skillset.append(token)\n",
    "    return [i.capitalize() for i in set([i.lower() for i in skillset])]\n",
    "  \n",
    "print ('Skills',programming_skills(text))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Name: ',extract_name(text))\n",
    "print('Qualification: ', extract_education(text))\n",
    "print('Mail id: ',extract_email_addresses(text))\n",
    "print('Mobile Number: ',extract_mobile_number(text))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

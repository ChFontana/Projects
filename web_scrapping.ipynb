{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "import httpx\n",
    "from parsel import Selector\n",
    "\n",
    "response = httpx.get(\n",
    "    \"https://www.glassdoor.com/Overview/Working-at-eBay-EI_IE7853.11,15.htm\"\n",
    ")\n",
    "selector = Selector(response.text)\n",
    "# find description in the HTML:\n",
    "print(selector.css('[data-test=\"employerDescription\"]::text').get())\n",
    "# will print:\n",
    "# eBay is where the world goes to shop, sell, and give. Every day, our professionals connect millions of buyers and sellers around the globe, empowering people and creating opportunity. We're on a mission to build a better, more connected form of commerce that benefits individuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (108858732.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[10], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    function addGlobalStyle(css) {\u001b[0m\n\u001b[1;37m             ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "function addGlobalStyle(css) {\n",
    "    var head, style;\n",
    "    head = document.getElementsByTagName('head')[0];\n",
    "    if (!head) { return; }\n",
    "    style = document.createElement('style');\n",
    "    style.type = 'text/css';\n",
    "    style.innerHTML = css;\n",
    "    head.appendChild(style);\n",
    "}\n",
    "\n",
    "addGlobalStyle(\"#HardsellOverlay {display:none !important;}\");\n",
    "addGlobalStyle(\"body {overflow:auto !important; position: initial !important}\");\n",
    "\n",
    "window.addEventListener(\"scroll\", event => event.stopPropagation(), true);\n",
    "window.addEventListener(\"mousemove\", event => event.stopPropagation(), true);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "from parsel import Selector\n",
    "import httpx\n",
    "\n",
    "france_location_cookie = {\"tldp\": \"6\"}\n",
    "response = httpx.get(\n",
    "    \"https://www.glassdoor.com/Overview/Working-at-eBay-EI_IE7853.11,15.htm\",\n",
    "    cookies=france_location_cookie,\n",
    "    follow_redirects=True,\n",
    ")\n",
    "selector = Selector(response.text)\n",
    "# find employee count in the HTML:\n",
    "print(selector.css('[data-test=\"employer-size\"]::text').get())\n",
    "# will print:\n",
    "# Plus de 10 000 employÃ©s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "employer_name = 'Amazon-Milan'\n",
    "employer_id = 'I_IE6036.0,6_IL.7,12_IM1058'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'find'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 88\u001b[0m\n\u001b[0;32m     84\u001b[0m     jobs \u001b[39m=\u001b[39m \u001b[39mawait\u001b[39;00m scrape_jobs(\u001b[39m\"\u001b[39m\u001b[39meBay\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m7853\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     85\u001b[0m     \u001b[39mprint\u001b[39m(json\u001b[39m.\u001b[39mdumps(jobs, indent\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m))\n\u001b[1;32m---> 88\u001b[0m asyncio\u001b[39m.\u001b[39;49mrun(main())\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\nest_asyncio.py:35\u001b[0m, in \u001b[0;36m_patch_asyncio.<locals>.run\u001b[1;34m(main, debug)\u001b[0m\n\u001b[0;32m     33\u001b[0m task \u001b[39m=\u001b[39m asyncio\u001b[39m.\u001b[39mensure_future(main)\n\u001b[0;32m     34\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 35\u001b[0m     \u001b[39mreturn\u001b[39;00m loop\u001b[39m.\u001b[39;49mrun_until_complete(task)\n\u001b[0;32m     36\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     37\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m task\u001b[39m.\u001b[39mdone():\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\nest_asyncio.py:90\u001b[0m, in \u001b[0;36m_patch_loop.<locals>.run_until_complete\u001b[1;34m(self, future)\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m f\u001b[39m.\u001b[39mdone():\n\u001b[0;32m     88\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[0;32m     89\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mEvent loop stopped before Future completed.\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m---> 90\u001b[0m \u001b[39mreturn\u001b[39;00m f\u001b[39m.\u001b[39;49mresult()\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.1264.0_x64__qbz5n2kfra8p0\\Lib\\asyncio\\futures.py:203\u001b[0m, in \u001b[0;36mFuture.result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    201\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__log_traceback \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m    202\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 203\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception\u001b[39m.\u001b[39mwith_traceback(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception_tb)\n\u001b[0;32m    204\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_result\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.1264.0_x64__qbz5n2kfra8p0\\Lib\\asyncio\\tasks.py:267\u001b[0m, in \u001b[0;36mTask.__step\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m    263\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    264\u001b[0m     \u001b[39mif\u001b[39;00m exc \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    265\u001b[0m         \u001b[39m# We use the `send` method directly, because coroutines\u001b[39;00m\n\u001b[0;32m    266\u001b[0m         \u001b[39m# don't have `__iter__` and `__next__` methods.\u001b[39;00m\n\u001b[1;32m--> 267\u001b[0m         result \u001b[39m=\u001b[39m coro\u001b[39m.\u001b[39msend(\u001b[39mNone\u001b[39;00m)\n\u001b[0;32m    268\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    269\u001b[0m         result \u001b[39m=\u001b[39m coro\u001b[39m.\u001b[39mthrow(exc)\n",
      "Cell \u001b[1;32mIn[24], line 84\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[39masync\u001b[39;00m \u001b[39mdef\u001b[39;00m \u001b[39mmain\u001b[39m():\n\u001b[1;32m---> 84\u001b[0m     jobs \u001b[39m=\u001b[39m \u001b[39mawait\u001b[39;00m scrape_jobs(\u001b[39m\"\u001b[39m\u001b[39meBay\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m7853\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     85\u001b[0m     \u001b[39mprint\u001b[39m(json\u001b[39m.\u001b[39mdumps(jobs, indent\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m))\n",
      "Cell \u001b[1;32mIn[24], line 68\u001b[0m, in \u001b[0;36mscrape_jobs\u001b[1;34m(employer_name, employer_id)\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[39m# scrape first page of jobs:\u001b[39;00m\n\u001b[0;32m     65\u001b[0m first_page \u001b[39m=\u001b[39m \u001b[39mawait\u001b[39;00m session\u001b[39m.\u001b[39mget(\n\u001b[0;32m     66\u001b[0m     url\u001b[39m=\u001b[39m\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mhttps://www.glassdoor.com/Jobs/\u001b[39m\u001b[39m{\u001b[39;00memployer_name\u001b[39m}\u001b[39;00m\u001b[39m-Jobs-E\u001b[39m\u001b[39m{\u001b[39;00memployer_id\u001b[39m}\u001b[39;00m\u001b[39m.htm?filter.countryId=\u001b[39m\u001b[39m{\u001b[39;00msession\u001b[39m.\u001b[39mcookies\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39mtldp\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m \u001b[39m\u001b[39mor\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m0\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[0;32m     67\u001b[0m )\n\u001b[1;32m---> 68\u001b[0m jobs \u001b[39m=\u001b[39m parse_jobs(first_page\u001b[39m.\u001b[39;49mtext)\n\u001b[0;32m     69\u001b[0m total_pages \u001b[39m=\u001b[39m parse_job_page_count(first_page\u001b[39m.\u001b[39mtext)\n\u001b[0;32m     71\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mscraped first page of jobs, scraping remaining \u001b[39m\u001b[39m{\u001b[39;00mtotal_pages\u001b[39m \u001b[39m\u001b[39m-\u001b[39m\u001b[39m \u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m pages\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[24], line 48\u001b[0m, in \u001b[0;36mparse_jobs\u001b[1;34m(html)\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mparse_jobs\u001b[39m(html) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m List[Dict]:\n\u001b[0;32m     47\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"parse jobs page for job data and total amount of jobs\"\"\"\u001b[39;00m\n\u001b[1;32m---> 48\u001b[0m     cache \u001b[39m=\u001b[39m extract_apollo_cache(html)\n\u001b[0;32m     49\u001b[0m     \u001b[39mreturn\u001b[39;00m [v[\u001b[39m\"\u001b[39m\u001b[39mjobview\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39mfor\u001b[39;00m v \u001b[39min\u001b[39;00m cache[\u001b[39m\"\u001b[39m\u001b[39mJobListingSearchResult\u001b[39m\u001b[39m\"\u001b[39m]]\n",
      "Cell \u001b[1;32mIn[24], line 41\u001b[0m, in \u001b[0;36mextract_apollo_cache\u001b[1;34m(html)\u001b[0m\n\u001b[0;32m     39\u001b[0m script_with_cache \u001b[39m=\u001b[39m selector\u001b[39m.\u001b[39mxpath(\u001b[39m\"\u001b[39m\u001b[39m//script[contains(.,\u001b[39m\u001b[39m'\u001b[39m\u001b[39mwindow.appCache\u001b[39m\u001b[39m'\u001b[39m\u001b[39m)]/text()\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mget()\n\u001b[0;32m     40\u001b[0m cache \u001b[39m=\u001b[39m defaultdict(\u001b[39mlist\u001b[39m)\n\u001b[1;32m---> 41\u001b[0m \u001b[39mfor\u001b[39;00m key, data \u001b[39min\u001b[39;00m find_json_objects(script_with_cache):\n\u001b[0;32m     42\u001b[0m     cache[key]\u001b[39m.\u001b[39mappend(data)\n\u001b[0;32m     43\u001b[0m \u001b[39mreturn\u001b[39;00m cache\n",
      "Cell \u001b[1;32mIn[24], line 21\u001b[0m, in \u001b[0;36mfind_json_objects\u001b[1;34m(text, decoder)\u001b[0m\n\u001b[0;32m     19\u001b[0m pos \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m     20\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m---> 21\u001b[0m     match \u001b[39m=\u001b[39m text\u001b[39m.\u001b[39;49mfind(\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39m\u001b[39m\"\u001b[39m, pos)\n\u001b[0;32m     22\u001b[0m     \u001b[39mif\u001b[39;00m match \u001b[39m==\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m:\n\u001b[0;32m     23\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'find'"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import json\n",
    "import math\n",
    "from collections import defaultdict\n",
    "from typing import Dict, List\n",
    "\n",
    "import httpx\n",
    "from parsel import Selector\n",
    "\n",
    "session = httpx.AsyncClient(\n",
    "    timeout=httpx.Timeout(30.0),\n",
    "    cookies={\"tldp\": \"1\"},\n",
    "    follow_redirects=True,\n",
    ")\n",
    "\n",
    "\n",
    "def find_json_objects(text: str, decoder=json.JSONDecoder()):\n",
    "    \"\"\"Find JSON objects in text, and generate decoded JSON data and it's ID\"\"\"\n",
    "    pos = 0\n",
    "    while True:\n",
    "        match = text.find(\"{\", pos)\n",
    "        if match == -1:\n",
    "            break\n",
    "        try:\n",
    "            result, index = decoder.raw_decode(text[match:])\n",
    "            # backtrack to find the key/identifier for this json object:\n",
    "            key_end = text.rfind('\"', 0, match)\n",
    "            key_start = text.rfind('\"', 0, key_end)\n",
    "            key = text[key_start + 1 : key_end]\n",
    "            yield key, result\n",
    "            pos = match + index\n",
    "        except ValueError:\n",
    "            pos = match + 1\n",
    "\n",
    "\n",
    "def extract_apollo_cache(html):\n",
    "    \"\"\"Extract apollo graphql cache data from HTML source\"\"\"\n",
    "    selector = Selector(text=html)\n",
    "    script_with_cache = selector.xpath(\"//script[contains(.,'window.appCache')]/text()\").get()\n",
    "    cache = defaultdict(list)\n",
    "    for key, data in find_json_objects(script_with_cache):\n",
    "        cache[key].append(data)\n",
    "    return cache\n",
    "\n",
    "\n",
    "def parse_jobs(html) -> List[Dict]:\n",
    "    \"\"\"parse jobs page for job data and total amount of jobs\"\"\"\n",
    "    cache = extract_apollo_cache(html)\n",
    "    return [v[\"jobview\"] for v in cache[\"JobListingSearchResult\"]]\n",
    "\n",
    "\n",
    "def parse_job_page_count(html) -> int:\n",
    "    \"\"\"parse job page count from pagination details in Glassdoor jobs page\"\"\"\n",
    "    _total_results = Selector(html).css(\".paginationFooter::text\").get()\n",
    "    if not _total_results:\n",
    "        return 1\n",
    "    _total_results = int(_total_results.split()[-1])\n",
    "    _total_pages = math.ceil(_total_results / 40)\n",
    "    return _total_pages\n",
    "\n",
    "\n",
    "async def scrape_jobs(employer_name: str, employer_id: str):\n",
    "    \"\"\"Scrape job listings\"\"\"\n",
    "    # scrape first page of jobs:\n",
    "    first_page = await session.get(\n",
    "        url=f\"https://www.glassdoor.com/Jobs/{employer_name}-Jobs-E{employer_id}.htm?filter.countryId={session.cookies.get('tldp') or 0}\",\n",
    "    )\n",
    "    jobs = parse_jobs(first_page.text)\n",
    "    total_pages = parse_job_page_count(first_page.text)\n",
    "\n",
    "    print(f\"scraped first page of jobs, scraping remaining {total_pages - 1} pages\")\n",
    "    other_pages = [\n",
    "        session.get(\n",
    "            url=str(first_page.url).replace(\".htm\", f\"_P{page}.htm\"),\n",
    "        )\n",
    "        for page in range(2, total_pages + 1)\n",
    "    ]\n",
    "    for page in await asyncio.gather(*other_pages):\n",
    "        jobs.extend(parse_jobs(page.text))\n",
    "    return jobs\n",
    "\n",
    "\n",
    "async def main():\n",
    "    jobs = await scrape_jobs(\"eBay\", \"7853\")\n",
    "    print(json.dumps(jobs, indent=2))\n",
    "\n",
    "\n",
    "asyncio.run(main())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "JSONDecodeError",
     "evalue": "Expecting value: line 1 column 1 (char 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 13\u001b[0m\n\u001b[0;32m     10\u001b[0m     data \u001b[39m=\u001b[39m json\u001b[39m.\u001b[39mloads(result\u001b[39m.\u001b[39mcontent)\n\u001b[0;32m     11\u001b[0m     \u001b[39mreturn\u001b[39;00m data[\u001b[39m0\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39msuggestion\u001b[39m\u001b[39m\"\u001b[39m], data[\u001b[39m0\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39memployerId\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m---> 13\u001b[0m \u001b[39mprint\u001b[39m(find_companies(\u001b[39m\"\u001b[39;49m\u001b[39mebay\u001b[39;49m\u001b[39m\"\u001b[39;49m))\n\u001b[0;32m     14\u001b[0m [\u001b[39m\"\u001b[39m\u001b[39meBay\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m7853\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "Cell \u001b[1;32mIn[20], line 10\u001b[0m, in \u001b[0;36mfind_companies\u001b[1;34m(query)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"find company Glassdoor ID and name by query. e.g. \"ebay\" will return \"eBay\" with ID 7853\"\"\"\u001b[39;00m\n\u001b[0;32m      7\u001b[0m result \u001b[39m=\u001b[39m httpx\u001b[39m.\u001b[39mget(\n\u001b[0;32m      8\u001b[0m     url\u001b[39m=\u001b[39m\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mhttps://www.glassdoor.com/searchsuggest/typeahead?numSuggestions=8&source=GD_V2&version=NEW&rf=full&fallback=token&input=\u001b[39m\u001b[39m{\u001b[39;00mquery\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[0;32m      9\u001b[0m )\n\u001b[1;32m---> 10\u001b[0m data \u001b[39m=\u001b[39m json\u001b[39m.\u001b[39;49mloads(result\u001b[39m.\u001b[39;49mcontent)\n\u001b[0;32m     11\u001b[0m \u001b[39mreturn\u001b[39;00m data[\u001b[39m0\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39msuggestion\u001b[39m\u001b[39m\"\u001b[39m], data[\u001b[39m0\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39memployerId\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.1264.0_x64__qbz5n2kfra8p0\\Lib\\json\\__init__.py:346\u001b[0m, in \u001b[0;36mloads\u001b[1;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[0;32m    341\u001b[0m     s \u001b[39m=\u001b[39m s\u001b[39m.\u001b[39mdecode(detect_encoding(s), \u001b[39m'\u001b[39m\u001b[39msurrogatepass\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m    343\u001b[0m \u001b[39mif\u001b[39;00m (\u001b[39mcls\u001b[39m \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m object_hook \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m\n\u001b[0;32m    344\u001b[0m         parse_int \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m parse_float \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m\n\u001b[0;32m    345\u001b[0m         parse_constant \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m object_pairs_hook \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m kw):\n\u001b[1;32m--> 346\u001b[0m     \u001b[39mreturn\u001b[39;00m _default_decoder\u001b[39m.\u001b[39;49mdecode(s)\n\u001b[0;32m    347\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mcls\u001b[39m \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    348\u001b[0m     \u001b[39mcls\u001b[39m \u001b[39m=\u001b[39m JSONDecoder\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.1264.0_x64__qbz5n2kfra8p0\\Lib\\json\\decoder.py:337\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[1;34m(self, s, _w)\u001b[0m\n\u001b[0;32m    332\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecode\u001b[39m(\u001b[39mself\u001b[39m, s, _w\u001b[39m=\u001b[39mWHITESPACE\u001b[39m.\u001b[39mmatch):\n\u001b[0;32m    333\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[0;32m    334\u001b[0m \u001b[39m    containing a JSON document).\u001b[39;00m\n\u001b[0;32m    335\u001b[0m \n\u001b[0;32m    336\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 337\u001b[0m     obj, end \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mraw_decode(s, idx\u001b[39m=\u001b[39;49m_w(s, \u001b[39m0\u001b[39;49m)\u001b[39m.\u001b[39;49mend())\n\u001b[0;32m    338\u001b[0m     end \u001b[39m=\u001b[39m _w(s, end)\u001b[39m.\u001b[39mend()\n\u001b[0;32m    339\u001b[0m     \u001b[39mif\u001b[39;00m end \u001b[39m!=\u001b[39m \u001b[39mlen\u001b[39m(s):\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.1264.0_x64__qbz5n2kfra8p0\\Lib\\json\\decoder.py:355\u001b[0m, in \u001b[0;36mJSONDecoder.raw_decode\u001b[1;34m(self, s, idx)\u001b[0m\n\u001b[0;32m    353\u001b[0m     obj, end \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscan_once(s, idx)\n\u001b[0;32m    354\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[1;32m--> 355\u001b[0m     \u001b[39mraise\u001b[39;00m JSONDecodeError(\u001b[39m\"\u001b[39m\u001b[39mExpecting value\u001b[39m\u001b[39m\"\u001b[39m, s, err\u001b[39m.\u001b[39mvalue) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    356\u001b[0m \u001b[39mreturn\u001b[39;00m obj, end\n",
      "\u001b[1;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import httpx\n",
    "\n",
    "\n",
    "def find_companies(query: str):\n",
    "    \"\"\"find company Glassdoor ID and name by query. e.g. \"ebay\" will return \"eBay\" with ID 7853\"\"\"\n",
    "    result = httpx.get(\n",
    "        url=f\"https://www.glassdoor.com/searchsuggest/typeahead?numSuggestions=8&source=GD_V2&version=NEW&rf=full&fallback=token&input={query}\",\n",
    "    )\n",
    "    data = json.loads(result.content)\n",
    "    return data[0][\"suggestion\"], data[0][\"employerId\"]\n",
    "\n",
    "print(find_companies(\"ebay\"))\n",
    "[\"eBay\", \"7853\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting enlighten\n",
      "  Downloading enlighten-1.11.2-py2.py3-none-any.whl (53 kB)\n",
      "                                              0.0/53.7 kB ? eta -:--:--\n",
      "     ---------------                          20.5/53.7 kB ? eta -:--:--\n",
      "     ---------------                          20.5/53.7 kB ? eta -:--:--\n",
      "     ---------------                          20.5/53.7 kB ? eta -:--:--\n",
      "     ---------------                          20.5/53.7 kB ? eta -:--:--\n",
      "     ---------------                          20.5/53.7 kB ? eta -:--:--\n",
      "     ---------------                          20.5/53.7 kB ? eta -:--:--\n",
      "     ---------------                          20.5/53.7 kB ? eta -:--:--\n",
      "     ---------------                          20.5/53.7 kB ? eta -:--:--\n",
      "     ---------------                          20.5/53.7 kB ? eta -:--:--\n",
      "     ---------------                          20.5/53.7 kB ? eta -:--:--\n",
      "     ---------------                          20.5/53.7 kB ? eta -:--:--\n",
      "     ---------------                          20.5/53.7 kB ? eta -:--:--\n",
      "     ---------------                          20.5/53.7 kB ? eta -:--:--\n",
      "     ---------------                          20.5/53.7 kB ? eta -:--:--\n",
      "     ---------------                          20.5/53.7 kB ? eta -:--:--\n",
      "     ---------------                          20.5/53.7 kB ? eta -:--:--\n",
      "     ---------------                          20.5/53.7 kB ? eta -:--:--\n",
      "     ---------------                          20.5/53.7 kB ? eta -:--:--\n",
      "     ---------------                          20.5/53.7 kB ? eta -:--:--\n",
      "     --------------------------------------- 53.7/53.7 kB 48.8 kB/s eta 0:00:00\n",
      "Collecting blessed>=1.17.7 (from enlighten)\n",
      "  Downloading blessed-1.20.0-py2.py3-none-any.whl (58 kB)\n",
      "                                              0.0/58.4 kB ? eta -:--:--\n",
      "     ---------------------                    30.7/58.4 kB 1.4 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 58.4/58.4 kB 1.0 MB/s eta 0:00:00\n",
      "Collecting prefixed>=0.3.2 (from enlighten)\n",
      "  Downloading prefixed-0.7.0-py2.py3-none-any.whl (13 kB)\n",
      "Requirement already satisfied: wcwidth>=0.1.4 in c:\\users\\cfont\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from blessed>=1.17.7->enlighten) (0.2.6)\n",
      "Requirement already satisfied: six>=1.9.0 in c:\\users\\cfont\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from blessed>=1.17.7->enlighten) (1.16.0)\n",
      "Collecting jinxed>=1.1.0 (from blessed>=1.17.7->enlighten)\n",
      "  Downloading jinxed-1.2.0-py2.py3-none-any.whl (33 kB)\n",
      "Collecting ansicon (from jinxed>=1.1.0->blessed>=1.17.7->enlighten)\n",
      "  Downloading ansicon-1.89.0-py2.py3-none-any.whl (63 kB)\n",
      "                                              0.0/63.7 kB ? eta -:--:--\n",
      "     ------                                   10.2/63.7 kB ? eta -:--:--\n",
      "     ------                                   10.2/63.7 kB ? eta -:--:--\n",
      "     ------                                   10.2/63.7 kB ? eta -:--:--\n",
      "     ------                                   10.2/63.7 kB ? eta -:--:--\n",
      "     ------                                   10.2/63.7 kB ? eta -:--:--\n",
      "     ------                                   10.2/63.7 kB ? eta -:--:--\n",
      "     ------                                   10.2/63.7 kB ? eta -:--:--\n",
      "     ------                                   10.2/63.7 kB ? eta -:--:--\n",
      "     ------                                   10.2/63.7 kB ? eta -:--:--\n",
      "     ------                                   10.2/63.7 kB ? eta -:--:--\n",
      "     ------                                   10.2/63.7 kB ? eta -:--:--\n",
      "     ------                                   10.2/63.7 kB ? eta -:--:--\n",
      "     ------                                   10.2/63.7 kB ? eta -:--:--\n",
      "     ------                                   10.2/63.7 kB ? eta -:--:--\n",
      "     ------                                   10.2/63.7 kB ? eta -:--:--\n",
      "     ------                                   10.2/63.7 kB ? eta -:--:--\n",
      "     ------                                   10.2/63.7 kB ? eta -:--:--\n",
      "     ------------------                      30.7/63.7 kB 26.2 kB/s eta 0:00:02\n",
      "     ------------------                      30.7/63.7 kB 26.2 kB/s eta 0:00:02\n",
      "     ------------------                      30.7/63.7 kB 26.2 kB/s eta 0:00:02\n",
      "     ------------------                      30.7/63.7 kB 26.2 kB/s eta 0:00:02\n",
      "     ------------------                      30.7/63.7 kB 26.2 kB/s eta 0:00:02\n",
      "     ------------------                      30.7/63.7 kB 26.2 kB/s eta 0:00:02\n",
      "     ------------------                      30.7/63.7 kB 26.2 kB/s eta 0:00:02\n",
      "     ------------------                      30.7/63.7 kB 26.2 kB/s eta 0:00:02\n",
      "     ------------------                      30.7/63.7 kB 26.2 kB/s eta 0:00:02\n",
      "     ------------------                      30.7/63.7 kB 26.2 kB/s eta 0:00:02\n",
      "     ------------------                      30.7/63.7 kB 26.2 kB/s eta 0:00:02\n",
      "     ------------------                      30.7/63.7 kB 26.2 kB/s eta 0:00:02\n",
      "     ------------------                      30.7/63.7 kB 26.2 kB/s eta 0:00:02\n",
      "     ------------------                      30.7/63.7 kB 26.2 kB/s eta 0:00:02\n",
      "     ------------------                      30.7/63.7 kB 26.2 kB/s eta 0:00:02\n",
      "     ------------------                      30.7/63.7 kB 26.2 kB/s eta 0:00:02\n",
      "     -------------------------               41.0/63.7 kB 19.7 kB/s eta 0:00:02\n",
      "     -------------------------               41.0/63.7 kB 19.7 kB/s eta 0:00:02\n",
      "     --------------------------------------- 63.7/63.7 kB 32.0 kB/s eta 0:00:00\n",
      "Installing collected packages: prefixed, ansicon, jinxed, blessed, enlighten\n",
      "Successfully installed ansicon-1.89.0 blessed-1.20.0 enlighten-1.11.2 jinxed-1.2.0 prefixed-0.7.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install enlighten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'packages'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 14\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39menlighten\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[39m# custom functions\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpackages\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcommon\u001b[39;00m \u001b[39mimport\u001b[39;00m requestAndParse\n\u001b[0;32m     15\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpackages\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpage\u001b[39;00m \u001b[39mimport\u001b[39;00m extract_maximums, extract_listings\n\u001b[0;32m     16\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpackages\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlisting\u001b[39;00m \u001b[39mimport\u001b[39;00m extract_listing\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'packages'"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "# standard libraries\n",
    "import argparse\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "from os.path import exists\n",
    "from datetime import datetime\n",
    "from time import time\n",
    "import csv\n",
    "# 3rd-party libraries\n",
    "import enlighten\n",
    "# custom functions\n",
    "from packages.common import requestAndParse\n",
    "from packages.page import extract_maximums, extract_listings\n",
    "from packages.listing import extract_listing\n",
    "\n",
    "\n",
    "class glassdoor_scraper():\n",
    "\n",
    "    def __init__(self, configfile, baseurl, targetnum) -> None:\n",
    "\n",
    "        # load first\n",
    "        base_url, target_num = self.load_configs(path=configfile)\n",
    "        # overwrite those that are not none\n",
    "        if type(baseurl) != type(None):\n",
    "            base_url = baseurl\n",
    "            print(\"Using supplied baseurl\")\n",
    "        if type(targetnum) != type(None):\n",
    "            target_num = targetnum\n",
    "            print(\"Using supplied targetnum\")\n",
    "        print(configfile, baseurl, targetnum)\n",
    "\n",
    "         # initialises output directory and file\n",
    "        if not os.path.exists('output'):\n",
    "            os.makedirs('output')\n",
    "        now = datetime.now() # current date and time\n",
    "        output_fileName = \"./output/output_\" + now.strftime(\"%d-%m-%Y\") + \".csv\"\n",
    "        csv_header = [(\"companyName\", \"company_starRating\", \"company_offeredRole\", \"company_roleLocation\", \"listing_jobDesc\", \"requested_url\")]\n",
    "        self.fileWriter(listOfTuples=csv_header, output_fileName=output_fileName)\n",
    "\n",
    "        maxJobs, maxPages = extract_maximums(base_url)\n",
    "        # print(\"[INFO] Maximum number of jobs in range: {}, number of pages in range: {}\".format(maxJobs, maxPages))\n",
    "        if (target_num >= maxJobs):\n",
    "            print(\"[ERROR] Target number larger than maximum number of jobs. Exiting program...\\n\")\n",
    "            os._exit(0)\n",
    "\n",
    "        # initialises enlighten_manager\n",
    "        enlighten_manager = enlighten.get_manager()\n",
    "        progress_outer = enlighten_manager.counter(total=target_num, desc=\"Total progress\", unit=\"listings\", color=\"green\", leave=False)\n",
    "\n",
    "        # initialise variables\n",
    "        page_index = 1\n",
    "        total_listingCount = 0\n",
    "\n",
    "        # initialises prev_url as base_url\n",
    "        prev_url = base_url\n",
    "\n",
    "        while total_listingCount <= target_num:\n",
    "            # clean up buffer\n",
    "            list_returnedTuple = []\n",
    "\n",
    "            new_url = self.update_url(prev_url, page_index)\n",
    "            page_soup,_ = requestAndParse(new_url)\n",
    "            listings_set, jobCount = extract_listings(page_soup)\n",
    "            progress_inner = enlighten_manager.counter(total=len(listings_set), desc=\"Listings scraped from page\", unit=\"listings\", color=\"blue\", leave=False)\n",
    "\n",
    "            print(\"\\n[INFO] Processing page index {}: {}\".format(page_index, new_url))\n",
    "            print(\"[INFO] Found {} links in page index {}\".format(jobCount, page_index))\n",
    "\n",
    "            for listing_url in listings_set:\n",
    "\n",
    "                # to implement cache here\n",
    "\n",
    "                returned_tuple = extract_listing(listing_url)\n",
    "                list_returnedTuple.append(returned_tuple)\n",
    "                # print(returned_tuple)\n",
    "                progress_inner.update()\n",
    "\n",
    "            progress_inner.close()\n",
    "\n",
    "            self.fileWriter(listOfTuples=list_returnedTuple, output_fileName=output_fileName)\n",
    "\n",
    "            # done with page, moving onto next page\n",
    "            total_listingCount = total_listingCount + jobCount\n",
    "            print(\"[INFO] Finished processing page index {}; Total number of jobs processed: {}\".format(page_index, total_listingCount))\n",
    "            page_index = page_index + 1\n",
    "            prev_url = new_url\n",
    "            progress_outer.update(jobCount)\n",
    "\n",
    "        progress_outer.close()    \n",
    "   \n",
    "            # loads user defined parameters\n",
    "    def load_configs(self, path):\n",
    "        with open(path) as config_file:\n",
    "            configurations = json.load(config_file)\n",
    "\n",
    "        base_url = configurations['base_url']\n",
    "        target_num = int(configurations[\"target_num\"])\n",
    "        return base_url, target_num\n",
    "\n",
    "\n",
    "    # appends list of tuples in specified output csv file\n",
    "    # a tuple is written as a single row in csv file \n",
    "    def fileWriter(self, listOfTuples, output_fileName):\n",
    "        with open(output_fileName,'a', newline='') as out:\n",
    "            csv_out=csv.writer(out)\n",
    "            for row_tuple in listOfTuples:\n",
    "                try:\n",
    "                    csv_out.writerow(row_tuple)\n",
    "                    # can also do csv_out.writerows(data) instead of the for loop\n",
    "                except Exception as e:\n",
    "                    print(\"[WARN] In filewriter: {}\".format(e))\n",
    "\n",
    "\n",
    "    # updates url according to the page_index desired\n",
    "    def update_url(self, prev_url, page_index):\n",
    "        if page_index == 1:\n",
    "            prev_substring = \".htm\"\n",
    "            new_substring = \"_IP\" + str(page_index) + \".htm\"\n",
    "        else:\n",
    "            prev_substring = \"_IP\" + str(page_index - 1) + \".htm\"\n",
    "            new_substring = \"_IP\" + str(page_index) + \".htm\"\n",
    "\n",
    "        new_url = prev_url.replace(prev_substring, new_substring)\n",
    "        return new_url\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('-c', '--configfile', help=\"Specify location of json config file\", type=str, required=False, default=\"config.json\")\n",
    "    parser.add_argument('-b', '--baseurl', help=\"Base_url to use. Overwrites config file\", type=str, required=False, default=None)\n",
    "    parser.add_argument('-tn', '--targetnum', help=\"Target number to scrape. Overwrites config file\", type=int, required=False, default=None)\n",
    "    args = vars(parser.parse_args())\n",
    "\n",
    "    glassdoor_scraper( \n",
    "        configfile=args[\"configfile\"],\n",
    "        baseurl=args[\"baseurl\"],\n",
    "        targetnum=args[\"targetnum\"]\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
